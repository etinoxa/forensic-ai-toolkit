{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-11T22:29:32.075952Z",
     "start_time": "2025-06-11T22:29:32.065009Z"
    }
   },
   "source": [
    "from sympy import false\n",
    "\n",
    "# Installation and Imports\n",
    "\"\"\"\n",
    "DETR + NLP Suspect Gallery Search System\n",
    "==========================================\n",
    "This system combines DETR object detection with multiple NLP models for advanced suspect search.\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment the following lines if running for the first time\n",
    "# !pip install transformers torch torchvision Pillow sentence-transformers openai-clip spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog, messagebox\n",
    "import torch\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:29:32.140463Z",
     "start_time": "2025-06-11T22:29:32.130721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration Settings\n",
    "\"\"\"\n",
    "Configuration Settings\n",
    "=====================\n",
    "Modify these settings to customize the system behavior\n",
    "\"\"\"\n",
    "\n",
    "class Config:\n",
    "    # Paths\n",
    "    SUSPECTS_GALLERY_PATH = \"../datasets/images/objects/raw\"  # Input folder with suspect images\n",
    "    RESULTS_PATH = \"../datasets/images/objects/detr_nlp_results\"             # Output folder for results\n",
    "    TEMP_PATH = \"temp_processing\"               # Temporary processing folder\n",
    "\n",
    "    # Model Configuration\n",
    "    DETR_MODEL = \"facebook/detr-resnet-50\"      # DETR model for object detection\n",
    "\n",
    "    # NLP Models (comment/uncomment to enable/disable)\n",
    "    NLP_MODELS = {\n",
    "        'sentence_transformer': {\n",
    "            'enabled': False,\n",
    "            'model_name': 'all-MiniLM-L6-v2',\n",
    "            'description': 'Sentence-BERT for semantic similarity'\n",
    "        },\n",
    "        'clip': {\n",
    "            'enabled': False,\n",
    "            'model_name': 'ViT-B/32',\n",
    "            'description': 'CLIP for vision-language understanding'\n",
    "        },\n",
    "        'spacy_nlp': {\n",
    "            'enabled': True,\n",
    "            'model_name': 'en_core_web_sm',\n",
    "            'description': 'spaCy for NER and linguistic analysis'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Detection Settings\n",
    "    CONFIDENCE_THRESHOLD = 0.7          # Minimum confidence for object detection\n",
    "    MAX_RESULTS = 10                    # Maximum number of results to return\n",
    "    SIMILARITY_THRESHOLD = 0.8          # Minimum similarity score for matches\n",
    "\n",
    "    # Visual Settings\n",
    "    FIGURE_SIZE = (15, 10)\n",
    "    BBOX_COLOR = 'red'\n",
    "    BBOX_LINEWIDTH = 2\n",
    "\n",
    "config = Config()\n",
    "print(\"‚úÖ Configuration loaded successfully!\")"
   ],
   "id": "290787c0da8da20b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully!\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:29:32.253012Z",
     "start_time": "2025-06-11T22:29:32.237550Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # Utility Functions\n",
    "\"\"\"\n",
    "Utility Functions\n",
    "================\n",
    "Helper functions for file operations and data processing\n",
    "\"\"\"\n",
    "\n",
    "def setup_directories():\n",
    "    \"\"\"Create the necessary directories if they don't exist\"\"\"\n",
    "    directories = [config.SUSPECTS_GALLERY_PATH, config.RESULTS_PATH, config.TEMP_PATH]\n",
    "    for directory in directories:\n",
    "        Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"‚úÖ Directories setup complete: {directories}\")\n",
    "\n",
    "def get_user_query():\n",
    "    \"\"\"Get a search query from user using GUI dialog\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "\n",
    "    query = input(\"Enter your search query (e.g., 'person wearing red jacket', 'man with glasses', 'woman in blue dress'): \")\n",
    "\n",
    "    root.destroy()\n",
    "    return query\n",
    "\n",
    "def load_image_paths(directory: str) -> List[str]:\n",
    "    \"\"\"Load all image paths from directory\"\"\"\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "    image_paths = []\n",
    "\n",
    "    for file_path in Path(directory).rglob('*'):\n",
    "        if file_path.suffix.lower() in image_extensions:\n",
    "            image_paths.append(str(file_path))\n",
    "\n",
    "    return image_paths\n",
    "\n",
    "def copy_results_to_folder(image_paths: List[str], query: str, scores: List[float]):\n",
    "    \"\"\"Copy matching images to results folder\"\"\"\n",
    "    # Create query-specific folder\n",
    "    safe_query = \"\".join(c for c in query if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "    query_folder = Path(config.RESULTS_PATH) / safe_query\n",
    "    query_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    copied_files = []\n",
    "    for i, (image_path, score) in enumerate(zip(image_paths, scores)):\n",
    "        # Create new filename with score\n",
    "        original_name = Path(image_path).stem\n",
    "        extension = Path(image_path).suffix\n",
    "        new_name = f\"{original_name}_score_{score:.3f}{extension}\"\n",
    "\n",
    "        destination = query_folder / new_name\n",
    "        shutil.copy2(image_path, destination)\n",
    "        copied_files.append(str(destination))\n",
    "\n",
    "    print(f\"‚úÖ Copied {len(copied_files)} files to {query_folder}\")\n",
    "    return copied_files\n",
    "\n",
    "setup_directories()\n"
   ],
   "id": "8e07b25b72c433b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Directories setup complete: ['../datasets/images/objects/raw', '../datasets/images/objects/detr_nlp_results', 'temp_processing']\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:29:32.876228Z",
     "start_time": "2025-06-11T22:29:32.336712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DETR Object Detection Module\n",
    "\"\"\"\n",
    "DETR Object Detection Module\n",
    "===========================\n",
    "Handles object detection using DETR model\n",
    "\"\"\"\n",
    "\n",
    "class DETRDetector:\n",
    "    def __init__(self):\n",
    "        self.processor = DetrImageProcessor.from_pretrained(config.DETR_MODEL)\n",
    "        self.model = DetrForObjectDetection.from_pretrained(config.DETR_MODEL)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        print(f\"‚úÖ DETR model loaded on {self.device}\")\n",
    "\n",
    "    def detect_objects(self, image_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Detect objects in image and return results\"\"\"\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "\n",
    "            # Convert outputs to COCO API format\n",
    "            target_sizes = torch.tensor([image.size[::-1]]).to(self.device)\n",
    "            results = self.processor.post_process_object_detection(\n",
    "                outputs, target_sizes=target_sizes, threshold=config.CONFIDENCE_THRESHOLD\n",
    "            )[0]\n",
    "\n",
    "            # Extract object information\n",
    "            objects = []\n",
    "            for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "                objects.append({\n",
    "                    'label': self.model.config.id2label[label.item()],\n",
    "                    'confidence': score.item(),\n",
    "                    'bbox': box.tolist()\n",
    "                })\n",
    "\n",
    "            return {\n",
    "                'image_path': image_path,\n",
    "                'objects': objects,\n",
    "                'image_size': image.size\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {image_path}: {e}\")\n",
    "            return {'image_path': image_path, 'objects': [], 'image_size': None}\n",
    "\n",
    "# Initialize DETR detector\n",
    "detr_detector = DETRDetector()"
   ],
   "id": "907136a3409f0c4c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DETR model loaded on cpu\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:29:33.436792Z",
     "start_time": "2025-06-11T22:29:32.911702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NLP Models Module\n",
    "\"\"\"\n",
    "NLP Models Module\n",
    "================\n",
    "Handles multiple NLP models for text analysis and similarity\n",
    "\"\"\"\n",
    "\n",
    "class NLPModels:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.load_enabled_models()\n",
    "\n",
    "    def load_enabled_models(self):\n",
    "        \"\"\"Load only enabled NLP models\"\"\"\n",
    "\n",
    "        # Sentence Transformer\n",
    "        if config.NLP_MODELS['sentence_transformer']['enabled']:\n",
    "            try:\n",
    "                self.models['sentence_transformer'] = SentenceTransformer(\n",
    "                    config.NLP_MODELS['sentence_transformer']['model_name']\n",
    "                )\n",
    "                print(\"‚úÖ Sentence Transformer loaded\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading Sentence Transformer: {e}\")\n",
    "\n",
    "        # CLIP\n",
    "        if config.NLP_MODELS['clip']['enabled']:\n",
    "            try:\n",
    "                device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "                self.models['clip_model'], self.models['clip_preprocess'] = clip.load(\n",
    "                    config.NLP_MODELS['clip']['model_name'], device=device\n",
    "                )\n",
    "                print(\"‚úÖ CLIP model loaded\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading CLIP: {e}\")\n",
    "\n",
    "        # spaCy NLP\n",
    "        if config.NLP_MODELS['spacy_nlp']['enabled']:\n",
    "            try:\n",
    "                self.models['spacy'] = spacy.load(config.NLP_MODELS['spacy_nlp']['model_name'])\n",
    "                print(\"‚úÖ spaCy model loaded\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading spaCy: {e}\")\n",
    "\n",
    "    def analyze_query_with_spacy(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze query using spaCy for entities and linguistic features\"\"\"\n",
    "        if 'spacy' not in self.models:\n",
    "            return {}\n",
    "\n",
    "        doc = self.models['spacy'](query)\n",
    "\n",
    "        return {\n",
    "            'entities': [(ent.text, ent.label_) for ent in doc.ents],\n",
    "            'tokens': [token.text for token in doc],\n",
    "            'pos_tags': [(token.text, token.pos_) for token in doc],\n",
    "            'noun_phrases': [chunk.text for chunk in doc.noun_chunks]\n",
    "        }\n",
    "\n",
    "    def compute_sentence_similarity(self, query: str, descriptions: List[str]) -> List[float]:\n",
    "        \"\"\"Compute similarity using Sentence Transformer\"\"\"\n",
    "        if 'sentence_transformer' not in self.models:\n",
    "            return [0.0] * len(descriptions)\n",
    "\n",
    "        try:\n",
    "            query_embedding = self.models['sentence_transformer'].encode([query])\n",
    "            desc_embeddings = self.models['sentence_transformer'].encode(descriptions)\n",
    "\n",
    "            # Compute cosine similarity\n",
    "            similarities = []\n",
    "            for desc_emb in desc_embeddings:\n",
    "                similarity = np.dot(query_embedding[0], desc_emb) / (\n",
    "                    np.linalg.norm(query_embedding[0]) * np.linalg.norm(desc_emb)\n",
    "                )\n",
    "                similarities.append(float(similarity))\n",
    "\n",
    "            return similarities\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error computing sentence similarity: {e}\")\n",
    "            return [0.0] * len(descriptions)\n",
    "\n",
    "    def compute_clip_similarity(self, query: str, image_paths: List[str]) -> List[float]:\n",
    "        \"\"\"Compute similarity using CLIP model\"\"\"\n",
    "        if 'clip_model' not in self.models or 'clip_preprocess' not in self.models:\n",
    "            return [0.0] * len(image_paths)\n",
    "\n",
    "        try:\n",
    "            device = next(self.models['clip_model'].parameters()).device\n",
    "\n",
    "            # Tokenize text\n",
    "            text = clip.tokenize([query]).to(device)\n",
    "\n",
    "            similarities = []\n",
    "            for img_path in image_paths:\n",
    "                try:\n",
    "                    # Preprocess image\n",
    "                    image = self.models['clip_preprocess'](\n",
    "                        Image.open(img_path).convert('RGB')\n",
    "                    ).unsqueeze(0).to(device)\n",
    "\n",
    "                    # Compute features\n",
    "                    with torch.no_grad():\n",
    "                        text_features = self.models['clip_model'].encode_text(text)\n",
    "                        image_features = self.models['clip_model'].encode_image(image)\n",
    "\n",
    "                        # Compute similarity\n",
    "                        similarity = torch.cosine_similarity(text_features, image_features)\n",
    "                        similarities.append(float(similarity.cpu().numpy()[0]))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error processing image {img_path}: {e}\")\n",
    "                    similarities.append(0.0)\n",
    "\n",
    "            return similarities\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error computing CLIP similarity: {e}\")\n",
    "            return [0.0] * len(image_paths)\n",
    "\n",
    "# Initialize NLP models\n",
    "nlp_models = NLPModels()"
   ],
   "id": "9f12f29d5b775110",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ spaCy model loaded\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:29:33.540549Z",
     "start_time": "2025-06-11T22:29:33.508304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Search Engine Module\n",
    "\"\"\"\n",
    "Search Engine Module\n",
    "===================\n",
    "Main search engine that combines DETR and NLP models\n",
    "\"\"\"\n",
    "\n",
    "class SuspectGallerySearchEngine:\n",
    "    def __init__(self):\n",
    "        self.detr_detector = detr_detector\n",
    "        self.nlp_models = nlp_models\n",
    "        self.image_cache = {}  # Cache for detected objects\n",
    "\n",
    "    def create_image_description(self, detection_result: Dict[str, Any]) -> str:\n",
    "        \"\"\"Create text description from DETR detection results\"\"\"\n",
    "        objects = detection_result['objects']\n",
    "        if not objects:\n",
    "            return \"image with no detected objects\"\n",
    "\n",
    "        # Group objects by label\n",
    "        object_counts = {}\n",
    "        for obj in objects:\n",
    "            label = obj['label']\n",
    "            object_counts[label] = object_counts.get(label, 0) + 1\n",
    "\n",
    "        # Create description\n",
    "        descriptions = []\n",
    "        for label, count in object_counts.items():\n",
    "            if count == 1:\n",
    "                descriptions.append(f\"a {label}\")\n",
    "            else:\n",
    "                descriptions.append(f\"{count} {label}s\")\n",
    "\n",
    "        return \"image containing \" + \", \".join(descriptions)\n",
    "\n",
    "    def search_gallery(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Main search function\"\"\"\n",
    "        print(f\"üîç Starting search for: '{query}'\")\n",
    "\n",
    "        # Load all images\n",
    "        image_paths = load_image_paths(config.SUSPECTS_GALLERY_PATH)\n",
    "        if not image_paths:\n",
    "            print(\"‚ùå No images found in suspects gallery!\")\n",
    "            return []\n",
    "\n",
    "        print(f\"üìÇ Found {len(image_paths)} images to process\")\n",
    "\n",
    "        # Analyze query with spaCy\n",
    "        query_analysis = self.nlp_models.analyze_query_with_spacy(query)\n",
    "        print(f\"üß† Query analysis: {query_analysis}\")\n",
    "\n",
    "        # Process images with DETR\n",
    "        print(\"üîç Detecting objects in images...\")\n",
    "        detection_results = []\n",
    "        for i, img_path in enumerate(image_paths):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"   Processing image {i+1}/{len(image_paths)}\")\n",
    "\n",
    "            result = self.detr_detector.detect_objects(img_path)\n",
    "            detection_results.append(result)\n",
    "\n",
    "        # Create descriptions for each image\n",
    "        descriptions = [self.create_image_description(result) for result in detection_results]\n",
    "\n",
    "        # Compute similarities using different NLP models\n",
    "        similarities = {}\n",
    "\n",
    "        # Sentence Transformer similarity\n",
    "        if config.NLP_MODELS['sentence_transformer']['enabled']:\n",
    "            similarities['sentence_transformer'] = self.nlp_models.compute_sentence_similarity(\n",
    "                query, descriptions\n",
    "            )\n",
    "\n",
    "        # CLIP similarity\n",
    "        if config.NLP_MODELS['clip']['enabled']:\n",
    "            similarities['clip'] = self.nlp_models.compute_clip_similarity(query, image_paths)\n",
    "\n",
    "        # Combine similarities (weighted average)\n",
    "        combined_similarities = []\n",
    "        for i in range(len(image_paths)):\n",
    "            scores = []\n",
    "\n",
    "            if 'sentence_transformer' in similarities:\n",
    "                scores.append(similarities['sentence_transformer'][i] * 0.4)\n",
    "\n",
    "            if 'clip' in similarities:\n",
    "                scores.append(similarities['clip'][i] * 0.6)\n",
    "\n",
    "            combined_score = sum(scores) / len(scores) if scores else 0.0\n",
    "            combined_similarities.append(combined_score)\n",
    "\n",
    "        # Create results\n",
    "        results = []\n",
    "        for i, (img_path, detection_result, description, combined_score) in enumerate(\n",
    "            zip(image_paths, detection_results, descriptions, combined_similarities)\n",
    "        ):\n",
    "            result = {\n",
    "                'rank': i + 1,\n",
    "                'image_path': img_path,\n",
    "                'combined_score': combined_score,\n",
    "                'description': description,\n",
    "                'detected_objects': detection_result['objects'],\n",
    "                'individual_scores': {\n",
    "                    key: similarities[key][i] for key in similarities.keys()\n",
    "                }\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "        # Sort by combined score and filter by threshold\n",
    "        results = [r for r in results if r['combined_score'] >= config.SIMILARITY_THRESHOLD]\n",
    "        results.sort(key=lambda x: x['combined_score'], reverse=True)\n",
    "\n",
    "        # Limit results\n",
    "        results = results[:config.MAX_RESULTS]\n",
    "\n",
    "        # Update ranks\n",
    "        for i, result in enumerate(results):\n",
    "            result['rank'] = i + 1\n",
    "\n",
    "        print(f\"‚úÖ Search completed! Found {len(results)} matching results.\")\n",
    "        return results\n",
    "\n",
    "# Initialize search engine\n",
    "search_engine = SuspectGallerySearchEngine()"
   ],
   "id": "92af6a4fbb4573bc",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:29:33.587729Z",
     "start_time": "2025-06-11T22:29:33.567754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Visualization Module\n",
    "\"\"\"\n",
    "Visualization Module\n",
    "===================\n",
    "Functions for displaying search results and detected objects\n",
    "\"\"\"\n",
    "\n",
    "def visualize_search_results(results: List[Dict[str, Any]], query: str):\n",
    "    \"\"\"Visualize search results with detected objects\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to display\")\n",
    "        return\n",
    "\n",
    "    # Calculate grid size\n",
    "    n_results = len(results)\n",
    "    cols = min(3, n_results)\n",
    "    rows = (n_results + cols - 1) // cols\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=config.FIGURE_SIZE)\n",
    "    fig.suptitle(f'Search Results for: \"{query}\"', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Handle single result case\n",
    "    if n_results == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = axes if isinstance(axes, list) else [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        try:\n",
    "            # Load and display image\n",
    "            image = Image.open(result['image_path']).convert('RGB')\n",
    "            axes[i].imshow(image)\n",
    "\n",
    "            # Draw bounding boxes for detected objects\n",
    "            for obj in result['detected_objects']:\n",
    "                bbox = obj['bbox']\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                width = x2 - x1\n",
    "                height = y2 - y1\n",
    "\n",
    "                # Create rectangle\n",
    "                rect = plt.Rectangle(\n",
    "                    (x1, y1), width, height,\n",
    "                    linewidth=config.BBOX_LINEWIDTH,\n",
    "                    edgecolor=config.BBOX_COLOR,\n",
    "                    facecolor='none'\n",
    "                )\n",
    "                axes[i].add_patch(rect)\n",
    "\n",
    "                # Add label\n",
    "                axes[i].text(\n",
    "                    x1, y1 - 5,\n",
    "                    f\"{obj['label']} ({obj['confidence']:.2f})\",\n",
    "                    fontsize=8,\n",
    "                    color=config.BBOX_COLOR,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.7)\n",
    "                )\n",
    "\n",
    "            # Set title with ranking and score\n",
    "            title = f\"Rank {result['rank']}: Score {result['combined_score']:.3f}\\n\"\n",
    "            title += f\"File: {Path(result['image_path']).name}\"\n",
    "            axes[i].set_title(title, fontsize=10)\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        except Exception as e:\n",
    "            axes[i].text(0.5, 0.5, f\"Error loading image:\\n{e}\",\n",
    "                        ha='center', va='center', transform=axes[i].transAxes)\n",
    "            axes[i].set_title(f\"Rank {result['rank']}: Error\")\n",
    "            axes[i].axis('off')\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for i in range(n_results, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_results_summary(results: List[Dict[str, Any]], query: str):\n",
    "    \"\"\"Display detailed summary of search results\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SEARCH RESULTS SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Total matches found: {len(results)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for result in results:\n",
    "        print(f\"\\nRank {result['rank']}:\")\n",
    "        print(f\"  üìÑ File: {Path(result['image_path']).name}\")\n",
    "        print(f\"  üéØ Combined Score: {result['combined_score']:.4f}\")\n",
    "        print(f\"  üìù Description: {result['description']}\")\n",
    "\n",
    "        # Show individual model scores\n",
    "        print(f\"  üìä Individual Scores:\")\n",
    "        for model_name, score in result['individual_scores'].items():\n",
    "            print(f\"     - {model_name}: {score:.4f}\")\n",
    "\n",
    "        # Show detected objects\n",
    "        if result['detected_objects']:\n",
    "            print(f\"  üîç Detected Objects:\")\n",
    "            for obj in result['detected_objects'][:3]:  # Show top 3 objects\n",
    "                print(f\"     - {obj['label']} (confidence: {obj['confidence']:.3f})\")\n",
    "\n",
    "        print(f\"  {'-'*50}\")\n"
   ],
   "id": "18a5d0ea04b3f1c1",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:31:37.794725Z",
     "start_time": "2025-06-11T22:29:33.639682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Main Execution Cell\n",
    "\"\"\"\n",
    "Main Execution Cell\n",
    "==================\n",
    "Run the complete suspect gallery search system\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the suspect gallery search\"\"\"\n",
    "    try:\n",
    "        print(\"üöÄ Starting Suspect Gallery Search System\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Check if suspects gallery exists and has images\n",
    "        if not Path(config.SUSPECTS_GALLERY_PATH).exists():\n",
    "            print(f\"‚ùå Suspects gallery not found at: {config.SUSPECTS_GALLERY_PATH}\")\n",
    "            print(\"Please create the folder and add suspect images.\")\n",
    "            return\n",
    "\n",
    "        image_paths = load_image_paths(config.SUSPECTS_GALLERY_PATH)\n",
    "        if not image_paths:\n",
    "            print(f\"‚ùå No images found in: {config.SUSPECTS_GALLERY_PATH}\")\n",
    "            print(\"Please add images to the suspects gallery folder.\")\n",
    "            return\n",
    "\n",
    "        print(f\"‚úÖ Found {len(image_paths)} images in suspects gallery\")\n",
    "\n",
    "        # Get search query from user\n",
    "        query = get_user_query()\n",
    "        if not query:\n",
    "            print(\"‚ùå No query provided. Search cancelled.\")\n",
    "            return\n",
    "\n",
    "        print(f\"üîç Search query: '{query}'\")\n",
    "\n",
    "        # Perform search\n",
    "        results = search_engine.search_gallery(query)\n",
    "\n",
    "        if not results:\n",
    "            print(\"‚ùå No matching results found.\")\n",
    "            print(\"Try adjusting the similarity threshold or using different search terms.\")\n",
    "            return\n",
    "\n",
    "        # Display results\n",
    "        display_results_summary(results, query)\n",
    "        visualize_search_results(results, query)\n",
    "\n",
    "        # Copy results to a folder\n",
    "        result_image_paths = [r['image_path'] for r in results]\n",
    "        result_scores = [r['combined_score'] for r in results]\n",
    "        copied_files = copy_results_to_folder(result_image_paths, query, result_scores)\n",
    "\n",
    "        print(f\"\\n‚úÖ Search completed successfully!\")\n",
    "        print(f\"üìÅ Results saved to: {Path(config.RESULTS_PATH) / query}\")\n",
    "\n",
    "        # Show enabled models\n",
    "        enabled_models = [name for name, settings in config.NLP_MODELS.items() if settings['enabled']]\n",
    "        print(f\"ü§ñ Models used: {', '.join(enabled_models)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during search: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "85373e7aa9719114",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Suspect Gallery Search System\n",
      "============================================================\n",
      "‚úÖ Found 121 images in suspects gallery\n",
      "üîç Search query: 'man with a gun'\n",
      "üîç Starting search for: 'man with a gun'\n",
      "üìÇ Found 121 images to process\n",
      "üß† Query analysis: {'entities': [], 'tokens': ['man', 'with', 'a', 'gun'], 'pos_tags': [('man', 'NOUN'), ('with', 'ADP'), ('a', 'DET'), ('gun', 'NOUN')], 'noun_phrases': ['man', 'a gun']}\n",
      "üîç Detecting objects in images...\n",
      "   Processing image 1/121\n",
      "   Processing image 11/121\n",
      "   Processing image 21/121\n",
      "   Processing image 31/121\n",
      "   Processing image 41/121\n",
      "   Processing image 51/121\n",
      "   Processing image 61/121\n",
      "   Processing image 71/121\n",
      "   Processing image 81/121\n",
      "   Processing image 91/121\n",
      "   Processing image 101/121\n",
      "   Processing image 111/121\n",
      "   Processing image 121/121\n",
      "‚úÖ Search completed! Found 0 matching results.\n",
      "‚ùå No matching results found.\n",
      "Try adjusting the similarity threshold or using different search terms.\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:31:37.842649Z",
     "start_time": "2025-06-11T22:31:37.830005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testing and Demo Cell (Optional)\n",
    "\"\"\"\n",
    "Testing and Demo Cell\n",
    "====================\n",
    "Test the system with sample data (optional)\n",
    "\"\"\"\n",
    "\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample data for testing (optional)\"\"\"\n",
    "    print(\"Creating sample data structure...\")\n",
    "\n",
    "    # Create sample directories\n",
    "    sample_dir = Path(\"sample_suspects\")\n",
    "    sample_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    print(f\"‚úÖ Sample directory created: {sample_dir}\")\n",
    "    print(\"Add some test images to this directory and update config.SUSPECTS_GALLERY_PATH\")\n",
    "    print(\"Then run the main() function again.\")\n",
    "\n",
    "def show_system_info():\n",
    "    \"\"\"Display system information and configuration\"\"\"\n",
    "    print(\"üîß SYSTEM CONFIGURATION\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Suspects Gallery Path: {config.SUSPECTS_GALLERY_PATH}\")\n",
    "    print(f\"Results Path: {config.RESULTS_PATH}\")\n",
    "    print(f\"DETR Model: {config.DETR_MODEL}\")\n",
    "    print(f\"Confidence Threshold: {config.CONFIDENCE_THRESHOLD}\")\n",
    "    print(f\"Similarity Threshold: {config.SIMILARITY_THRESHOLD}\")\n",
    "    print(f\"Max Results: {config.MAX_RESULTS}\")\n",
    "\n",
    "    print(\"\\nü§ñ NLP MODELS STATUS\")\n",
    "    print(\"=\"*50)\n",
    "    for model_name, settings in config.NLP_MODELS.items():\n",
    "        status = \"‚úÖ ENABLED\" if settings['enabled'] else \"‚ùå DISABLED\"\n",
    "        print(f\"{model_name}: {status}\")\n",
    "        print(f\"   Model: {settings['model_name']}\")\n",
    "        print(f\"   Description: {settings['description']}\")\n",
    "\n",
    "    print(f\"\\nüíª DEVICE INFO\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"PyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Show system information\n",
    "show_system_info()\n",
    "\n",
    "# Uncomment the following line to create sample data structure\n",
    "# create_sample_data()"
   ],
   "id": "99100d7fe031fd6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß SYSTEM CONFIGURATION\n",
      "==================================================\n",
      "Suspects Gallery Path: ../datasets/images/objects/raw\n",
      "Results Path: ../datasets/images/objects/detr_nlp_results\n",
      "DETR Model: facebook/detr-resnet-50\n",
      "Confidence Threshold: 0.7\n",
      "Similarity Threshold: 0.8\n",
      "Max Results: 10\n",
      "\n",
      "ü§ñ NLP MODELS STATUS\n",
      "==================================================\n",
      "sentence_transformer: ‚ùå DISABLED\n",
      "   Model: all-MiniLM-L6-v2\n",
      "   Description: Sentence-BERT for semantic similarity\n",
      "clip: ‚ùå DISABLED\n",
      "   Model: ViT-B/32\n",
      "   Description: CLIP for vision-language understanding\n",
      "spacy_nlp: ‚úÖ ENABLED\n",
      "   Model: en_core_web_sm\n",
      "   Description: spaCy for NER and linguistic analysis\n",
      "\n",
      "üíª DEVICE INFO\n",
      "==================================================\n",
      "PyTorch CUDA Available: False\n"
     ]
    }
   ],
   "execution_count": 61
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
