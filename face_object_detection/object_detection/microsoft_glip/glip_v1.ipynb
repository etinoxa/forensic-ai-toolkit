{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-14T04:21:44.310042Z",
     "start_time": "2025-06-14T04:21:30.836987Z"
    }
   },
   "source": [
    "# Cell 1: Configuration and Setup\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Configuration Settings\n",
    "CONFIG = {\n",
    "    # Model Selection (CLIP-based approach for GLIP-like functionality)\n",
    "    'MODEL_NAME': 'openai/clip-vit-base-patch32',  # CLIP models available in transformers\n",
    "    'DEVICE': 'auto',  # 'auto', 'cpu', 'cuda'\n",
    "\n",
    "    # Paths\n",
    "    'SUSPECTS_GALLERY_PATH': '../../datasets/images/objects/raw',  # Input folder with suspect images\n",
    "    'RESULTS_OUTPUT_PATH': '../../datasets/images/objects/detections',      # Output folder for matched images\n",
    "\n",
    "    # Detection parameters (adapted for CLIP)\n",
    "    'CONFIDENCE_THRESHOLD': 0.25,  # Similarity threshold for CLIP\n",
    "    'SIMILARITY_THRESHOLD': 0.2,   # Text-image similarity threshold\n",
    "    'TOP_K_PATCHES': 20,           # Number of top patches to consider\n",
    "\n",
    "    # Processing settings\n",
    "    'BATCH_SIZE': 4,               # Default batch size for processing\n",
    "    'MAX_RESULTS_DISPLAY': 10,     # Maximum results to display at once\n",
    "    'FIGURE_SIZE': (12, 8),        # Size of result visualization\n",
    "    'PATCH_SIZE': 64,              # Size of image patches for analysis\n",
    "    'STRIDE': 32,                  # Stride for sliding window\n",
    "}\n",
    "\n",
    "# Available CLIP model options (GLIP alternative)\n",
    "AVAILABLE_MODELS = {\n",
    "    'clip-vit-base-patch32': {\n",
    "        'name': 'CLIP ViT-Base Patch32',\n",
    "        'model_id': 'openai/clip-vit-base-patch32',\n",
    "        'description': 'Standard CLIP model - good balance of speed and accuracy',\n",
    "        'performance': 'Base performance, good speed'\n",
    "    },\n",
    "    'clip-vit-base-patch16': {\n",
    "        'name': 'CLIP ViT-Base Patch16',\n",
    "        'model_id': 'openai/clip-vit-base-patch16',\n",
    "        'description': 'Higher resolution CLIP - better accuracy, slower',\n",
    "        'performance': 'Better accuracy, moderate speed'\n",
    "    },\n",
    "    'clip-vit-large-patch14': {\n",
    "        'name': 'CLIP ViT-Large Patch14',\n",
    "        'model_id': 'openai/clip-vit-large-patch14',\n",
    "        'description': 'Largest CLIP model - best accuracy, slowest',\n",
    "        'performance': 'Best accuracy, slowest speed'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully\")\n",
    "print(f\"üìÅ Suspects gallery: {CONFIG['SUSPECTS_GALLERY_PATH']}\")\n",
    "print(f\"üìÅ Results output: {CONFIG['RESULTS_OUTPUT_PATH']}\")\n",
    "print(f\"üîç Selected model: {CONFIG['MODEL_NAME']}\")\n",
    "print(\"üìù Note: Using CLIP for GLIP-like functionality (text-image grounding)\")\n",
    "\n",
    "# Cell 2: Install and Import Dependencies\n",
    "# Run this cell first to install required packages\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"‚úÖ Transformers already installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Installing required packages...\")\n",
    "    !pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
    "    !pip install transformers\n",
    "    !pip install ipywidgets\n",
    "    !pip install Pillow\n",
    "    !pip install matplotlib\n",
    "    !pip install opencv-python\n",
    "    print(\"üì¶ Installation complete\")\n",
    "\n",
    "# Import required libraries\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from transformers import CLIPProcessor, CLIPModel\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    print(\"‚úÖ All dependencies imported successfully\")\n",
    "\n",
    "    # Check PyTorch device compatibility\n",
    "    print(f\"üîß PyTorch version: {torch.__version__}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üöÄ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        default_device = \"cuda\"\n",
    "    else:\n",
    "        print(\"üñ•Ô∏è Using CPU mode (CUDA not available)\")\n",
    "        default_device = \"cpu\"\n",
    "\n",
    "    # Update config with detected device\n",
    "    if CONFIG['DEVICE'] == 'auto':\n",
    "        CONFIG['DEVICE'] = default_device\n",
    "        print(f\"üìç Auto-detected device: {CONFIG['DEVICE']}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"1. Restart kernel and run this cell again\")\n",
    "    print(\"2. Check if all packages installed correctly\")\n",
    "\n",
    "# Cell 3: Initialize Model and Directories\n",
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories if they don't exist\"\"\"\n",
    "    os.makedirs(CONFIG['SUSPECTS_GALLERY_PATH'], exist_ok=True)\n",
    "    os.makedirs(CONFIG['RESULTS_OUTPUT_PATH'], exist_ok=True)\n",
    "    print(f\"üìÅ Created directories: {CONFIG['SUSPECTS_GALLERY_PATH']}, {CONFIG['RESULTS_OUTPUT_PATH']}\")\n",
    "\n",
    "def load_clip_model(model_name=None):\n",
    "    \"\"\"Load CLIP model from Hugging Face for GLIP-like functionality\"\"\"\n",
    "    try:\n",
    "        # Use provided model name or default from config\n",
    "        if model_name is None:\n",
    "            model_name = CONFIG['MODEL_NAME']\n",
    "\n",
    "        device = CONFIG['DEVICE']\n",
    "        print(f\"üì• Loading CLIP model: {model_name}\")\n",
    "        print(f\"üñ•Ô∏è Target device: {device}\")\n",
    "        print(\"üìù Using CLIP for grounded language-image understanding\")\n",
    "\n",
    "        # Load processor and model\n",
    "        print(\"‚è≥ Loading processor...\")\n",
    "        processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "        print(\"‚è≥ Loading model weights...\")\n",
    "        model = CLIPModel.from_pretrained(model_name)\n",
    "\n",
    "        # Move to device\n",
    "        print(f\"üìç Moving model to {device}...\")\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        print(f\"‚úÖ Model loaded successfully!\")\n",
    "        print(f\"   üîç Model: {model_name}\")\n",
    "        print(f\"   üñ•Ô∏è Device: {device}\")\n",
    "\n",
    "        return model, processor, device, model_name\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        print(\"\\nüîß Troubleshooting steps:\")\n",
    "        print(\"1. Check internet connection (models download from Hugging Face)\")\n",
    "        print(\"2. Verify model name is correct\")\n",
    "        print(\"3. Try switching to 'openai/clip-vit-base-patch32' for faster loading\")\n",
    "        print(\"4. Restart kernel if memory issues occur\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def switch_model(model_key):\n",
    "    \"\"\"Switch to a different CLIP model variant\"\"\"\n",
    "    if model_key in AVAILABLE_MODELS:\n",
    "        CONFIG['MODEL_NAME'] = AVAILABLE_MODELS[model_key]['model_id']\n",
    "        print(f\"üîÑ Switched to: {AVAILABLE_MODELS[model_key]['name']}\")\n",
    "        return load_clip_model()\n",
    "    else:\n",
    "        print(f\"‚ùå Unknown model: {model_key}\")\n",
    "        print(f\"Available models: {list(AVAILABLE_MODELS.keys())}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Initialize\n",
    "setup_directories()\n",
    "model, processor, device, model_name = load_clip_model()\n",
    "\n",
    "# Cell 4: Core Search Functions (GLIP-like with CLIP)\n",
    "def create_sliding_windows(image, patch_size=64, stride=32):\n",
    "    \"\"\"Create sliding windows across the image for localization\"\"\"\n",
    "    width, height = image.size\n",
    "    windows = []\n",
    "    positions = []\n",
    "\n",
    "    for y in range(0, height - patch_size + 1, stride):\n",
    "        for x in range(0, width - patch_size + 1, stride):\n",
    "            # Extract patch\n",
    "            patch = image.crop((x, y, x + patch_size, y + patch_size))\n",
    "            windows.append(patch)\n",
    "            positions.append((x, y, x + patch_size, y + patch_size))\n",
    "\n",
    "    return windows, positions\n",
    "\n",
    "def compute_text_image_similarity(model, processor, text, images, device):\n",
    "    \"\"\"Compute similarity between text and multiple image patches\"\"\"\n",
    "    # Process text and images\n",
    "    inputs = processor(text=[text], images=images, return_tensors=\"pt\", padding=True)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Get similarity scores\n",
    "        logits_per_image = outputs.logits_per_image  # (num_images, num_texts)\n",
    "        similarities = F.softmax(logits_per_image, dim=1)[:, 0]  # Get scores for our text\n",
    "\n",
    "    return similarities.cpu().numpy()\n",
    "\n",
    "def process_image_batch(image_paths, model, processor, query, device, batch_size=4):\n",
    "    \"\"\"Process a batch of images efficiently using CLIP for grounding\"\"\"\n",
    "    batch_results = []\n",
    "\n",
    "    # Process images one by one to avoid memory issues\n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        try:\n",
    "            # Progress indicator\n",
    "            if i % 5 == 0:\n",
    "                print(f\"Processing {i+1}/{len(image_paths)}: {img_path.name[:30]}...\", end='\\r')\n",
    "\n",
    "            # Load image\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            # Create sliding windows for localization\n",
    "            patch_size = CONFIG['PATCH_SIZE']\n",
    "            stride = CONFIG['STRIDE']\n",
    "\n",
    "            # For very small images, use the whole image\n",
    "            if min(image.size) < patch_size:\n",
    "                patches = [image]\n",
    "                positions = [(0, 0, image.size[0], image.size[1])]\n",
    "            else:\n",
    "                patches, positions = create_sliding_windows(image, patch_size, stride)\n",
    "\n",
    "            # Limit number of patches to avoid memory issues\n",
    "            if len(patches) > CONFIG['TOP_K_PATCHES']:\n",
    "                # Sample patches evenly across the image\n",
    "                step = len(patches) // CONFIG['TOP_K_PATCHES']\n",
    "                indices = list(range(0, len(patches), step))[:CONFIG['TOP_K_PATCHES']]\n",
    "                patches = [patches[idx] for idx in indices]\n",
    "                positions = [positions[idx] for idx in indices]\n",
    "\n",
    "            # Compute similarities for all patches\n",
    "            if patches:\n",
    "                similarities = compute_text_image_similarity(model, processor, query, patches, device)\n",
    "\n",
    "                # Filter by similarity threshold\n",
    "                high_sim_indices = np.where(similarities >= CONFIG['SIMILARITY_THRESHOLD'])[0]\n",
    "\n",
    "                if len(high_sim_indices) > 0:\n",
    "                    # Get high-similarity patches and their positions\n",
    "                    filtered_similarities = similarities[high_sim_indices]\n",
    "                    filtered_positions = [positions[idx] for idx in high_sim_indices]\n",
    "\n",
    "                    # Convert positions to torch tensors for consistency\n",
    "                    boxes_tensor = torch.tensor(filtered_positions, dtype=torch.float32)\n",
    "                    scores_tensor = torch.tensor(filtered_similarities, dtype=torch.float32)\n",
    "\n",
    "                    batch_results.append({\n",
    "                        'image_path': img_path,\n",
    "                        'image': image,\n",
    "                        'boxes': boxes_tensor,\n",
    "                        'confidence_scores': scores_tensor,\n",
    "                        'labels': [query] * len(filtered_similarities),\n",
    "                        'query': query\n",
    "                    })\n",
    "\n",
    "            # Clear memory after each image\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print error but continue processing\n",
    "            print(f\"\\n‚ö†Ô∏è Error processing {img_path.name}: {str(e)[:50]}...\")\n",
    "            continue\n",
    "\n",
    "        # Small break every 10 images to prevent system overload\n",
    "        if i % 10 == 0 and i > 0:\n",
    "            import time\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    return batch_results\n",
    "\n",
    "def search_images_with_query(query, model, processor, device, model_name, gallery_path, batch_size=4):\n",
    "    \"\"\"\n",
    "    Search for objects in images using natural language query with CLIP (GLIP-like)\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    start_datetime = datetime.now()\n",
    "\n",
    "    results = []\n",
    "    gallery_path = Path(gallery_path)\n",
    "\n",
    "    if not gallery_path.exists():\n",
    "        print(f\"‚ùå Gallery path {gallery_path} does not exist\")\n",
    "        return results\n",
    "\n",
    "    if not model or not processor:\n",
    "        print(\"‚ùå Model or processor not loaded. Please check model initialization.\")\n",
    "        return results\n",
    "\n",
    "    # Supported image extensions\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}\n",
    "    image_files = [f for f in gallery_path.iterdir()\n",
    "                  if f.suffix.lower() in image_extensions]\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"‚ö†Ô∏è No images found in {gallery_path}\")\n",
    "        return results\n",
    "\n",
    "    total_files = len(image_files)\n",
    "\n",
    "    print(f\"üîç Processing {total_files} images for: '{query}'\")\n",
    "    print(f\"üñ•Ô∏è Device: {device} | Using CLIP for grounding\")\n",
    "    print(f\"üîç Model: {model_name}\")\n",
    "    print(f\"üìê Patch size: {CONFIG['PATCH_SIZE']}px, Stride: {CONFIG['STRIDE']}px\")\n",
    "    print(f\"‚è∞ Start time: {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    # Process images one by one with progress tracking\n",
    "    try:\n",
    "        print(\"‚è≥ Starting image processing...\")\n",
    "        results = process_image_batch(image_files, model, processor, query, device, batch_size)\n",
    "\n",
    "        # Calculate timing\n",
    "        end_time = time.time()\n",
    "        end_datetime = datetime.now()\n",
    "        total_duration = end_time - start_time\n",
    "\n",
    "        # Format duration\n",
    "        hours = int(total_duration // 3600)\n",
    "        minutes = int((total_duration % 3600) // 60)\n",
    "        seconds = total_duration % 60\n",
    "\n",
    "        if hours > 0:\n",
    "            duration_str = f\"{hours}h {minutes}m {seconds:.1f}s\"\n",
    "        elif minutes > 0:\n",
    "            duration_str = f\"{minutes}m {seconds:.1f}s\"\n",
    "        else:\n",
    "            duration_str = f\"{seconds:.1f}s\"\n",
    "\n",
    "        # Show final progress with timing\n",
    "        matches_found = len(results)\n",
    "        avg_time_per_image = total_duration / total_files if total_files > 0 else 0\n",
    "\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"üìä PROCESSING SUMMARY\")\n",
    "        print(f\"=\"*60)\n",
    "        print(f\"üì∏ Images processed: {total_files}\")\n",
    "        print(f\"‚úÖ Matches found: {matches_found}\")\n",
    "        print(f\"‚è∞ Start time: {start_datetime.strftime('%H:%M:%S')}\")\n",
    "        print(f\"üèÅ End time: {end_datetime.strftime('%H:%M:%S')}\")\n",
    "        print(f\"‚è±Ô∏è Total time: {duration_str}\")\n",
    "        print(f\"üìà Avg per image: {avg_time_per_image:.2f}s\")\n",
    "        print(f\"=\"*60)\n",
    "\n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        end_datetime = datetime.now()\n",
    "        total_duration = end_time - start_time\n",
    "\n",
    "        print(f\"‚ùå Processing error: {e}\")\n",
    "        print(f\"‚è∞ Failed after: {total_duration:.1f}s\")\n",
    "        print(\"üí° Try reducing patch size or similarity threshold\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def copy_results_to_folder(results, output_folder):\n",
    "    \"\"\"Copy matched images to results folder\"\"\"\n",
    "    output_path = Path(output_folder)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Create subfolder with timestamp\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    search_folder = output_path / f\"search_{timestamp}\"\n",
    "    search_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    copied_files = []\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        try:\n",
    "            source_path = result['image_path']\n",
    "            # Create descriptive filename\n",
    "            max_conf = float(result['confidence_scores'].max()) if len(result['confidence_scores']) > 0 else 0.0\n",
    "            filename = f\"{i+1:03d}_{source_path.stem}_sim{max_conf:.2f}{source_path.suffix}\"\n",
    "            dest_path = search_folder / filename\n",
    "\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            copied_files.append(dest_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error copying {source_path}: {e}\")\n",
    "\n",
    "    print(f\"üìã Copied {len(copied_files)} files to {search_folder}\")\n",
    "    return search_folder, copied_files\n",
    "\n",
    "# Cell 5: Interactive Query Interface\n",
    "def create_search_interface():\n",
    "    \"\"\"Create interactive search interface for forensic analysts\"\"\"\n",
    "\n",
    "    # Model selection dropdown\n",
    "    model_options = [(f\"{info['name']} - {info['description']}\", key)\n",
    "                    for key, info in AVAILABLE_MODELS.items()]\n",
    "\n",
    "    model_selector = widgets.Dropdown(\n",
    "        options=model_options,\n",
    "        value='clip-vit-base-patch32',\n",
    "        description='Model:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='500px')\n",
    "    )\n",
    "\n",
    "    # Similarity threshold slider\n",
    "    similarity_threshold_slider = widgets.FloatSlider(\n",
    "        value=CONFIG['SIMILARITY_THRESHOLD'],\n",
    "        min=0.1,\n",
    "        max=0.8,\n",
    "        step=0.05,\n",
    "        description='Similarity Threshold:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    # Patch size slider\n",
    "    patch_size_slider = widgets.IntSlider(\n",
    "        value=CONFIG['PATCH_SIZE'],\n",
    "        min=32,\n",
    "        max=128,\n",
    "        step=16,\n",
    "        description='Patch Size:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    # Input widgets\n",
    "    query_input = widgets.Text(\n",
    "        value='person with weapon',\n",
    "        placeholder='Enter search query (e.g., \"person with weapon\", \"suspicious activity\")',\n",
    "        description='Query:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='500px')\n",
    "    )\n",
    "\n",
    "    confidence_slider = widgets.FloatSlider(\n",
    "        value=CONFIG['CONFIDENCE_THRESHOLD'],\n",
    "        min=0.1,\n",
    "        max=0.9,\n",
    "        step=0.05,\n",
    "        description='Min Confidence:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    search_button = widgets.Button(\n",
    "        description='üîç Search Gallery',\n",
    "        button_style='primary',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "\n",
    "    copy_button = widgets.Button(\n",
    "        description='üìã Copy Results',\n",
    "        button_style='success',\n",
    "        layout=widgets.Layout(width='150px'),\n",
    "        disabled=True\n",
    "    )\n",
    "\n",
    "    clear_button = widgets.Button(\n",
    "        description='üóëÔ∏è Clear Results',\n",
    "        button_style='warning',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "\n",
    "    switch_model_button = widgets.Button(\n",
    "        description='üîÑ Switch Model',\n",
    "        button_style='info',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "\n",
    "    # Output area\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    # Store results and current model\n",
    "    search_results = []\n",
    "    current_model = model\n",
    "    current_processor = processor\n",
    "    current_device = device\n",
    "    current_model_name = model_name\n",
    "\n",
    "    def on_model_switch_clicked(b):\n",
    "        nonlocal current_model, current_processor, current_device, current_model_name\n",
    "        with output_area:\n",
    "            selected_model = model_selector.value\n",
    "            print(f\"üîÑ Switching to: {AVAILABLE_MODELS[selected_model]['name']}\")\n",
    "            new_model, new_processor, new_device, new_model_name = switch_model(selected_model)\n",
    "            if new_model and new_processor:\n",
    "                current_model = new_model\n",
    "                current_processor = new_processor\n",
    "                current_device = new_device\n",
    "                current_model_name = new_model_name\n",
    "                print(\"‚úÖ Model switched successfully!\")\n",
    "            else:\n",
    "                print(\"‚ùå Failed to switch model\")\n",
    "\n",
    "    def on_search_clicked(b):\n",
    "        nonlocal search_results\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            if not current_model or not current_processor:\n",
    "                print(\"‚ùå Model not loaded. Please switch to a valid model first.\")\n",
    "                return\n",
    "\n",
    "            query = query_input.value.strip()\n",
    "            if not query:\n",
    "                print(\"‚ö†Ô∏è Please enter a search query\")\n",
    "                return\n",
    "\n",
    "            # Update configuration\n",
    "            CONFIG['CONFIDENCE_THRESHOLD'] = confidence_slider.value\n",
    "            CONFIG['SIMILARITY_THRESHOLD'] = similarity_threshold_slider.value\n",
    "            CONFIG['PATCH_SIZE'] = patch_size_slider.value\n",
    "\n",
    "            print(f\"üöÄ Starting CLIP-based search: '{query}'\")\n",
    "            print(f\"üìä Confidence: {CONFIG['CONFIDENCE_THRESHOLD']:.2f} | Similarity: {CONFIG['SIMILARITY_THRESHOLD']:.2f}\")\n",
    "            print(f\"üìê Patch size: {CONFIG['PATCH_SIZE']}px\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "            # Perform search\n",
    "            search_results = search_images_with_query(\n",
    "                query, current_model, current_processor, current_device, current_model_name,\n",
    "                CONFIG['SUSPECTS_GALLERY_PATH'], CONFIG['BATCH_SIZE']\n",
    "            )\n",
    "\n",
    "            if search_results:\n",
    "                copy_button.disabled = False\n",
    "                display_results(search_results[:CONFIG['MAX_RESULTS_DISPLAY']])\n",
    "\n",
    "                if len(search_results) > CONFIG['MAX_RESULTS_DISPLAY']:\n",
    "                    print(f\"\\nüìù Showing first {CONFIG['MAX_RESULTS_DISPLAY']} results out of {len(search_results)} total matches\")\n",
    "            else:\n",
    "                print(\"üîç No matches found for your query\")\n",
    "                print(\"üí° Try lowering the similarity threshold or using different search terms\")\n",
    "                copy_button.disabled = True\n",
    "\n",
    "    def on_copy_clicked(b):\n",
    "        with output_area:\n",
    "            if search_results:\n",
    "                print(\"\\nüìã Copying results to output folder...\")\n",
    "                folder, files = copy_results_to_folder(search_results, CONFIG['RESULTS_OUTPUT_PATH'])\n",
    "                print(f\"‚úÖ Results saved to: {folder}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No results to copy\")\n",
    "\n",
    "    def on_clear_clicked(b):\n",
    "        nonlocal search_results\n",
    "        search_results = []\n",
    "        copy_button.disabled = True\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            print(\"üóëÔ∏è Results cleared\")\n",
    "\n",
    "    # Connect button events\n",
    "    switch_model_button.on_click(on_model_switch_clicked)\n",
    "    search_button.on_click(on_search_clicked)\n",
    "    copy_button.on_click(on_copy_clicked)\n",
    "    clear_button.on_click(on_clear_clicked)\n",
    "\n",
    "    # Layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>üîç Forensic Image Search Interface (CLIP-based Grounding)</h3>\"),\n",
    "        model_selector,\n",
    "        query_input,\n",
    "        widgets.HBox([confidence_slider, similarity_threshold_slider]),\n",
    "        widgets.HBox([patch_size_slider]),\n",
    "        widgets.HBox([search_button, copy_button, clear_button, switch_model_button]),\n",
    "        widgets.HTML(\"<hr>\")\n",
    "    ])\n",
    "\n",
    "    return widgets.VBox([controls, output_area])\n",
    "\n",
    "def display_results(results):\n",
    "    \"\"\"Display search results with bounding boxes\"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "\n",
    "    cols = 2\n",
    "    rows = (len(results) + cols - 1) // cols\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=CONFIG['FIGURE_SIZE'])\n",
    "    if rows == 1:\n",
    "        axes = [axes] if cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        ax = axes[i] if len(results) > 1 else axes\n",
    "\n",
    "        # Display image\n",
    "        image = result['image']\n",
    "        ax.imshow(image)\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        w, h = image.size\n",
    "        boxes = result['boxes']\n",
    "        confidences = result['confidence_scores']\n",
    "\n",
    "        for box, conf in zip(boxes, confidences):\n",
    "            # Convert from [x1, y1, x2, y2] to matplotlib rectangle\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "\n",
    "            # Create rectangle\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), x2 - x1, y2 - y1,\n",
    "                linewidth=2, edgecolor='red', facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Add similarity score text\n",
    "            ax.text(x1, y1 - 5, f'{conf:.2f}',\n",
    "                   color='red', fontweight='bold', fontsize=10)\n",
    "\n",
    "        ax.set_title(f\"{result['image_path'].name}\\nMatches: {len(boxes)}\", fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for j in range(len(results), len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display the interface\n",
    "interface = create_search_interface()\n",
    "display(interface)\n",
    "\n",
    "# Cell 6: Batch Processing Functions (Enhanced)\n",
    "def batch_search_multiple_queries(queries_list, model, processor, device, model_name, gallery_path, output_base_path, batch_size=4):\n",
    "    \"\"\"\n",
    "    Process multiple queries in batch for comprehensive analysis using CLIP\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Start timing for entire batch\n",
    "    batch_start_time = time.time()\n",
    "    batch_start_datetime = datetime.now()\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    print(f\"üöÄ Starting batch analysis with {len(queries_list)} queries\")\n",
    "    print(f\"üìÅ Gallery: {gallery_path}\")\n",
    "    print(f\"üîç Model: {model_name}\")\n",
    "    print(f\"‚è∞ Batch start: {batch_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i, query in enumerate(queries_list, 1):\n",
    "        print(f\"\\n[{i}/{len(queries_list)}] Query: '{query}'\")\n",
    "\n",
    "        # Time individual query\n",
    "        query_start_time = time.time()\n",
    "        results = search_images_with_query(query, model, processor, device, model_name, gallery_path, batch_size)\n",
    "        query_end_time = time.time()\n",
    "        query_duration = query_end_time - query_start_time\n",
    "\n",
    "        if results:\n",
    "            # Create query-specific output folder\n",
    "            query_folder = Path(output_base_path) / f\"query_{query.replace(' ', '_').replace('/', '_')}\"\n",
    "            folder, files = copy_results_to_folder(results, query_folder)\n",
    "            all_results[query] = {\n",
    "                'results': results,\n",
    "                'output_folder': folder,\n",
    "                'file_count': len(files),\n",
    "                'match_count': len(results),\n",
    "                'processing_time': query_duration\n",
    "            }\n",
    "            print(f\"üìÅ Saved {len(files)} files to: {folder.name}\")\n",
    "            print(f\"‚è±Ô∏è Query completed in: {query_duration:.1f}s\")\n",
    "        else:\n",
    "            all_results[query] = {\n",
    "                'results': [],\n",
    "                'output_folder': None,\n",
    "                'file_count': 0,\n",
    "                'match_count': 0,\n",
    "                'processing_time': query_duration\n",
    "            }\n",
    "            print(\"‚ö™ No matches found\")\n",
    "            print(f\"‚è±Ô∏è Query completed in: {query_duration:.1f}s\")\n",
    "\n",
    "    # Calculate batch timing\n",
    "    batch_end_time = time.time()\n",
    "    batch_end_datetime = datetime.now()\n",
    "    total_batch_duration = batch_end_time - batch_start_time\n",
    "\n",
    "    # Format duration\n",
    "    hours = int(total_batch_duration // 3600)\n",
    "    minutes = int((total_batch_duration % 3600) // 60)\n",
    "    seconds = total_batch_duration % 60\n",
    "\n",
    "    if hours > 0:\n",
    "        duration_str = f\"{hours}h {minutes}m {seconds:.1f}s\"\n",
    "    elif minutes > 0:\n",
    "        duration_str = f\"{minutes}m {seconds:.1f}s\"\n",
    "    else:\n",
    "        duration_str = f\"{seconds:.1f}s\"\n",
    "\n",
    "    # Summary report with timing\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä BATCH SEARCH SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    total_matches = 0\n",
    "    total_files = 0\n",
    "    total_query_time = 0\n",
    "\n",
    "    for query, data in all_results.items():\n",
    "        matches = data['match_count']\n",
    "        files = data['file_count']\n",
    "        query_time = data['processing_time']\n",
    "        total_matches += matches\n",
    "        total_files += files\n",
    "        total_query_time += query_time\n",
    "\n",
    "        status = \"‚úÖ\" if matches > 0 else \"‚ö™\"\n",
    "        print(f\"{status} '{query}': {matches} images, {files} files saved ({query_time:.1f}s)\")\n",
    "\n",
    "    avg_time_per_query = total_batch_duration / len(queries_list) if queries_list else 0\n",
    "\n",
    "    print(f\"\\nüéØ TOTALS:\")\n",
    "    print(f\"üìä Queries processed: {len(queries_list)}\")\n",
    "    print(f\"üéØ Total matches: {total_matches} images\")\n",
    "    print(f\"üìã Total files copied: {total_files}\")\n",
    "    print(f\"‚è∞ Start time: {batch_start_datetime.strftime('%H:%M:%S')}\")\n",
    "    print(f\"üèÅ End time: {batch_end_datetime.strftime('%H:%M:%S')}\")\n",
    "    print(f\"‚è±Ô∏è Total batch time: {duration_str}\")\n",
    "    print(f\"üìà Avg per query: {avg_time_per_query:.1f}s\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Enhanced batch processing with common forensic queries optimized for CLIP\n",
    "def run_forensic_batch_analysis(custom_queries=None, batch_size=4, model_to_use=None):\n",
    "    \"\"\"Run comprehensive forensic analysis with predefined and custom queries using CLIP\"\"\"\n",
    "\n",
    "    # Use provided model or current global model\n",
    "    if model_to_use:\n",
    "        current_model, current_processor, current_device, current_model_name = model_to_use\n",
    "    else:\n",
    "        current_model, current_processor, current_device, current_model_name = model, processor, device, model_name\n",
    "\n",
    "    # Default forensic queries optimized for CLIP\n",
    "    default_queries = [\n",
    "        \"person with weapon\",\n",
    "        \"weapon\",\n",
    "        \"gun\",\n",
    "        \"knife\",\n",
    "        \"suspicious person\",\n",
    "        \"person running\",\n",
    "        \"vehicle\",\n",
    "        \"mask\",\n",
    "        \"dark clothing\",\n",
    "        \"backpack\",\n",
    "        \"group of people\",\n",
    "        \"mobile phone\",\n",
    "        \"suspicious activity\"\n",
    "    ]\n",
    "\n",
    "    # Combine with custom queries if provided\n",
    "    if custom_queries:\n",
    "        queries = default_queries + custom_queries\n",
    "        print(f\"üìã Using {len(default_queries)} default + {len(custom_queries)} custom queries\")\n",
    "    else:\n",
    "        queries = default_queries\n",
    "        print(f\"üìã Using {len(default_queries)} default forensic queries\")\n",
    "\n",
    "    if current_model and current_processor:\n",
    "        print(\"üîç Starting comprehensive CLIP-based forensic analysis...\")\n",
    "        batch_results = batch_search_multiple_queries(\n",
    "            queries,\n",
    "            current_model,\n",
    "            current_processor,\n",
    "            current_device,\n",
    "            current_model_name,\n",
    "            CONFIG['SUSPECTS_GALLERY_PATH'],\n",
    "            CONFIG['RESULTS_OUTPUT_PATH'],\n",
    "            batch_size\n",
    "        )\n",
    "        return batch_results\n",
    "    else:\n",
    "        print(\"‚ùå Model or processor not loaded. Cannot run batch analysis.\")\n",
    "        return None\n",
    "\n",
    "# Quick test with reduced output\n",
    "def quick_forensic_search(query=\"person with weapon\", batch_size=4, model_to_use=None):\n",
    "    \"\"\"Quick single query search for testing with CLIP\"\"\"\n",
    "    if model_to_use:\n",
    "        current_model, current_processor, current_device, current_model_name = model_to_use\n",
    "    else:\n",
    "        current_model, current_processor, current_device, current_model_name = model, processor, device, model_name\n",
    "\n",
    "    if not current_model or not current_processor:\n",
    "        print(\"‚ùå Model or processor not loaded\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üîç Quick CLIP search: '{query}'\")\n",
    "    print(f\"üîç Using model: {current_model_name}\")\n",
    "\n",
    "    results = search_images_with_query(query, current_model, current_processor, current_device, current_model_name,\n",
    "                                     CONFIG['SUSPECTS_GALLERY_PATH'], batch_size)\n",
    "\n",
    "    if results:\n",
    "        print(f\"üìã Found {len(results)} matches - ready for detailed analysis\")\n",
    "        return results\n",
    "    else:\n",
    "        print(\"‚ö™ No matches found\")\n",
    "        return []\n",
    "\n",
    "# Model comparison utilities\n",
    "def list_available_models():\n",
    "    \"\"\"Display available CLIP models with their descriptions\"\"\"\n",
    "    print(\"üîç Available CLIP Models (GLIP Alternative):\")\n",
    "    print(\"-\" * 50)\n",
    "    for key, info in AVAILABLE_MODELS.items():\n",
    "        print(f\"üîπ {info['name']} ({key})\")\n",
    "        print(f\"   üìä {info['performance']}\")\n",
    "        print(f\"   üìù {info['description']}\")\n",
    "        print()\n",
    "\n",
    "# Display available models\n",
    "list_available_models()\n",
    "\n",
    "# Uncomment to run batch analysis\n",
    "# batch_results = run_forensic_batch_analysis(batch_size=4)\n",
    "\n",
    "# Uncomment for quick test\n",
    "# quick_results = quick_forensic_search(\"weapon\", batch_size=4)\n",
    "\n",
    "# Cell 7: Usage Instructions and Tips\n",
    "print(\"\"\"\n",
    "üéØ FORENSIC IMAGE SEARCH SYSTEM - CLIP-BASED GROUNDING\n",
    "====================================================\n",
    "\n",
    "üìù IMPORTANT NOTE:\n",
    "This implementation uses CLIP (available in transformers) to provide\n",
    "GLIP-like functionality through text-image similarity and sliding window detection.\n",
    "\n",
    "üÜï CLIP-BASED FEATURES:\n",
    "‚Ä¢ üîç Text-image similarity for grounded understanding\n",
    "‚Ä¢ üìê Sliding window approach for object localization\n",
    "‚Ä¢ üéØ Patch-based analysis for detailed detection\n",
    "‚Ä¢ üîó Direct similarity scoring between text and image regions\n",
    "\n",
    "üìã SETUP CHECKLIST:\n",
    "1. ‚úÖ Place suspect images in the './suspects_gallery' folder\n",
    "2. ‚úÖ Run all cells in order (1-6)\n",
    "3. ‚úÖ CLIP models download automatically on first use\n",
    "4. ‚úÖ Works with standard transformers library (no special dependencies)\n",
    "\n",
    "üîç AVAILABLE CLIP MODELS:\n",
    "‚Ä¢ ViT-Base Patch32: Fastest, good balance (recommended for testing)\n",
    "‚Ä¢ ViT-Base Patch16: Higher resolution, better accuracy\n",
    "‚Ä¢ ViT-Large Patch14: Best accuracy, slowest (recommended for critical analysis)\n",
    "\n",
    "üîß DEVICE COMPATIBILITY:\n",
    "‚Ä¢ System automatically detects GPU/CPU availability\n",
    "‚Ä¢ Models work on both CPU and GPU\n",
    "‚Ä¢ CPU mode: Slower but works on all systems\n",
    "‚Ä¢ GPU mode: Significantly faster with CUDA support\n",
    "\n",
    "üîç SEARCH METHODOLOGY:\n",
    "‚Ä¢ Creates sliding windows across each image\n",
    "‚Ä¢ Computes text-image similarity for each patch\n",
    "‚Ä¢ Returns high-similarity regions as detections\n",
    "‚Ä¢ Adjustable patch size and stride for different granularity\n",
    "\n",
    "üîç SEARCH TIPS FOR CLIP-BASED GROUNDING:\n",
    "‚Ä¢ Use clear, descriptive terms: \"weapon\", \"person\", \"vehicle\"\n",
    "‚Ä¢ CLIP works well with object names and simple descriptions\n",
    "‚Ä¢ Try both specific and general terms\n",
    "‚Ä¢ Lower similarity thresholds (0.15-0.3) often work better\n",
    "‚Ä¢ Effective forensic queries:\n",
    "  - \"weapon\" / \"gun\" / \"knife\"\n",
    "  - \"person\" / \"suspicious person\"\n",
    "  - \"vehicle\" / \"car\"\n",
    "  - \"mask\" / \"dark clothing\"\n",
    "  - \"backpack\" / \"bag\"\n",
    "\n",
    "‚öôÔ∏è INTERFACE FEATURES:\n",
    "‚Ä¢ Model selector for switching between CLIP variants\n",
    "‚Ä¢ Similarity threshold: Controls detection sensitivity\n",
    "‚Ä¢ Patch size: Adjusts detection granularity (32-128px)\n",
    "‚Ä¢ Confidence threshold: Final filtering of results\n",
    "‚Ä¢ Real-time search with progress tracking\n",
    "\n",
    "üéõÔ∏è PARAMETER TUNING:\n",
    "‚Ä¢ Similarity Threshold (0.1-0.8): Text-image matching sensitivity\n",
    "  - Lower = more detections, higher = more precise\n",
    "‚Ä¢ Patch Size (32-128px): Detection window size\n",
    "  - Smaller = more detailed, larger = faster processing\n",
    "‚Ä¢ Confidence Threshold (0.1-0.9): Final result filtering\n",
    "‚Ä¢ Start with: Similarity 0.2, Patch 64px, Confidence 0.25\n",
    "\n",
    "üìÅ OUTPUT STRUCTURE:\n",
    "search_results/\n",
    "‚îú‚îÄ‚îÄ search_20240611_143022/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 001_suspect1_sim0.32.jpg\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 002_suspect5_sim0.45.jpg\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "\n",
    "üö® TROUBLESHOOTING:\n",
    "‚Ä¢ \"No matches found\" ‚Üí Lower similarity threshold (try 0.15-0.2)\n",
    "‚Ä¢ \"Too many false positives\" ‚Üí Raise similarity threshold\n",
    "‚Ä¢ \"Missing small objects\" ‚Üí Reduce patch size to 32-48px\n",
    "‚Ä¢ \"Slow processing\" ‚Üí Increase patch size or use faster model\n",
    "‚Ä¢ \"Memory issues\" ‚Üí Restart kernel, reduce patch count\n",
    "\n",
    "üîÑ MODEL COMPARISON:\n",
    "‚Ä¢ ViT-Base Patch32:\n",
    "  - Pros: Fast processing, good general performance\n",
    "  - Cons: Lower resolution features\n",
    "  - Best for: Quick analysis, large image sets\n",
    "‚Ä¢ ViT-Base Patch16:\n",
    "  - Pros: Better detail recognition, good balance\n",
    "  - Cons: Slower than Patch32\n",
    "  - Best for: Standard forensic analysis\n",
    "‚Ä¢ ViT-Large Patch14:\n",
    "  - Pros: Best accuracy, finest details\n",
    "  - Cons: Slowest processing, high memory usage\n",
    "  - Best for: Critical evidence analysis\n",
    "\n",
    "‚≠ê CLIP ADVANTAGES:\n",
    "‚Ä¢ Available in standard transformers library\n",
    "‚Ä¢ No special installation requirements\n",
    "‚Ä¢ Good zero-shot performance on diverse objects\n",
    "‚Ä¢ Robust text-image understanding\n",
    "‚Ä¢ Multiple model sizes for different needs\n",
    "\n",
    "üîç VS GLIP COMPARISON:\n",
    "‚Ä¢ CLIP: Widely available, good performance, simpler setup\n",
    "‚Ä¢ True GLIP: Better language grounding, more complex setup\n",
    "‚Ä¢ This implementation: GLIP-like functionality with CLIP convenience\n",
    "\n",
    "üí° FORENSIC BEST PRACTICES:\n",
    "‚Ä¢ Start with simple object names: \"weapon\", \"person\", \"vehicle\"\n",
    "‚Ä¢ Use multiple patch sizes for comprehensive analysis\n",
    "‚Ä¢ Cross-reference results between different models\n",
    "‚Ä¢ Adjust thresholds based on image quality and lighting\n",
    "‚Ä¢ Document parameter settings for evidence reports\n",
    "\n",
    "üéØ PERFORMANCE EXPECTATIONS:\n",
    "‚Ä¢ CLIP excels at recognizing common objects and people\n",
    "‚Ä¢ Good performance on weapons, vehicles, and clothing\n",
    "‚Ä¢ Works well with clear, well-lit images\n",
    "‚Ä¢ May require parameter tuning for optimal results\n",
    "‚Ä¢ Best results with objects that fill a significant portion of patches\n",
    "\n",
    "üìä OPTIMIZATION TIPS:\n",
    "‚Ä¢ For speed: Use Patch32 model, larger patch sizes (96-128px)\n",
    "‚Ä¢ For accuracy: Use Large model, smaller patches (32-48px)\n",
    "‚Ä¢ For balance: Use Patch16 model, medium patches (64px)\n",
    "‚Ä¢ Memory saving: Reduce TOP_K_PATCHES in config\n",
    "‚Ä¢ Quality images: Lower similarity thresholds work better\n",
    "\n",
    "üîç QUERY OPTIMIZATION:\n",
    "‚Ä¢ Simple terms often work best: \"weapon\" vs \"person holding weapon\"\n",
    "‚Ä¢ Try synonyms: \"car\" vs \"vehicle\", \"gun\" vs \"weapon\"\n",
    "‚Ä¢ Use single concepts per query for clearer results\n",
    "‚Ä¢ Combine results from multiple related queries\n",
    "\n",
    "üéì ADVANCED USAGE:\n",
    "‚Ä¢ Batch processing for systematic investigation\n",
    "‚Ä¢ Multiple model comparison for validation\n",
    "‚Ä¢ Parameter sweeps to find optimal settings\n",
    "‚Ä¢ Integration with other detection models\n",
    "‚Ä¢ Custom patch extraction for specific use cases\n",
    "\n",
    "üìà EXPECTED RESULTS:\n",
    "‚Ä¢ Similarity scores typically range 0.15-0.6\n",
    "‚Ä¢ Higher scores indicate stronger text-image match\n",
    "‚Ä¢ Results depend heavily on image quality and object visibility\n",
    "‚Ä¢ Fine-tune thresholds based on your specific dataset\n",
    "\n",
    "For batch processing of multiple queries, use the functions in Cell 6.\n",
    "For systematic forensic investigations, consider parameter optimization.\n",
    "This CLIP-based approach provides practical GLIP-like functionality without complex setup requirements.\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully\n",
      "üìÅ Suspects gallery: ../../datasets/images/objects/raw\n",
      "üìÅ Results output: ../../datasets/images/objects/detections\n",
      "üîç Selected model: openai/clip-vit-base-patch32\n",
      "üìù Note: Using CLIP for GLIP-like functionality (text-image grounding)\n",
      "‚úÖ Transformers already installed\n",
      "‚úÖ All dependencies imported successfully\n",
      "üîß PyTorch version: 2.7.1+cpu\n",
      "üñ•Ô∏è Using CPU mode (CUDA not available)\n",
      "üìç Auto-detected device: cpu\n",
      "üìÅ Created directories: ../../datasets/images/objects/raw, ../../datasets/images/objects/detections\n",
      "üì• Loading CLIP model: openai/clip-vit-base-patch32\n",
      "üñ•Ô∏è Target device: cpu\n",
      "üìù Using CLIP for grounded language-image understanding\n",
      "‚è≥ Loading processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading model weights...\n",
      "üìç Moving model to cpu...\n",
      "‚úÖ Model loaded successfully!\n",
      "   üîç Model: openai/clip-vit-base-patch32\n",
      "   üñ•Ô∏è Device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h3>üîç Forensic Image Search Interface (CLIP-based Grounding)</h3>'),‚Ä¶"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b60d68548c754ca0a61424d862437cfe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Available CLIP Models (GLIP Alternative):\n",
      "--------------------------------------------------\n",
      "üîπ CLIP ViT-Base Patch32 (clip-vit-base-patch32)\n",
      "   üìä Base performance, good speed\n",
      "   üìù Standard CLIP model - good balance of speed and accuracy\n",
      "\n",
      "üîπ CLIP ViT-Base Patch16 (clip-vit-base-patch16)\n",
      "   üìä Better accuracy, moderate speed\n",
      "   üìù Higher resolution CLIP - better accuracy, slower\n",
      "\n",
      "üîπ CLIP ViT-Large Patch14 (clip-vit-large-patch14)\n",
      "   üìä Best accuracy, slowest speed\n",
      "   üìù Largest CLIP model - best accuracy, slowest\n",
      "\n",
      "\n",
      "üéØ FORENSIC IMAGE SEARCH SYSTEM - CLIP-BASED GROUNDING\n",
      "====================================================\n",
      "\n",
      "üìù IMPORTANT NOTE:\n",
      "This implementation uses CLIP (available in transformers) to provide\n",
      "GLIP-like functionality through text-image similarity and sliding window detection.\n",
      "\n",
      "üÜï CLIP-BASED FEATURES:\n",
      "‚Ä¢ üîç Text-image similarity for grounded understanding\n",
      "‚Ä¢ üìê Sliding window approach for object localization\n",
      "‚Ä¢ üéØ Patch-based analysis for detailed detection\n",
      "‚Ä¢ üîó Direct similarity scoring between text and image regions\n",
      "\n",
      "üìã SETUP CHECKLIST:\n",
      "1. ‚úÖ Place suspect images in the './suspects_gallery' folder\n",
      "2. ‚úÖ Run all cells in order (1-6)\n",
      "3. ‚úÖ CLIP models download automatically on first use\n",
      "4. ‚úÖ Works with standard transformers library (no special dependencies)\n",
      "\n",
      "üîç AVAILABLE CLIP MODELS:\n",
      "‚Ä¢ ViT-Base Patch32: Fastest, good balance (recommended for testing)\n",
      "‚Ä¢ ViT-Base Patch16: Higher resolution, better accuracy\n",
      "‚Ä¢ ViT-Large Patch14: Best accuracy, slowest (recommended for critical analysis)\n",
      "\n",
      "üîß DEVICE COMPATIBILITY:\n",
      "‚Ä¢ System automatically detects GPU/CPU availability\n",
      "‚Ä¢ Models work on both CPU and GPU\n",
      "‚Ä¢ CPU mode: Slower but works on all systems\n",
      "‚Ä¢ GPU mode: Significantly faster with CUDA support\n",
      "\n",
      "üîç SEARCH METHODOLOGY:\n",
      "‚Ä¢ Creates sliding windows across each image\n",
      "‚Ä¢ Computes text-image similarity for each patch\n",
      "‚Ä¢ Returns high-similarity regions as detections\n",
      "‚Ä¢ Adjustable patch size and stride for different granularity\n",
      "\n",
      "üîç SEARCH TIPS FOR CLIP-BASED GROUNDING:\n",
      "‚Ä¢ Use clear, descriptive terms: \"weapon\", \"person\", \"vehicle\"\n",
      "‚Ä¢ CLIP works well with object names and simple descriptions\n",
      "‚Ä¢ Try both specific and general terms\n",
      "‚Ä¢ Lower similarity thresholds (0.15-0.3) often work better\n",
      "‚Ä¢ Effective forensic queries:\n",
      "  - \"weapon\" / \"gun\" / \"knife\"\n",
      "  - \"person\" / \"suspicious person\"\n",
      "  - \"vehicle\" / \"car\"\n",
      "  - \"mask\" / \"dark clothing\"\n",
      "  - \"backpack\" / \"bag\"\n",
      "\n",
      "‚öôÔ∏è INTERFACE FEATURES:\n",
      "‚Ä¢ Model selector for switching between CLIP variants\n",
      "‚Ä¢ Similarity threshold: Controls detection sensitivity\n",
      "‚Ä¢ Patch size: Adjusts detection granularity (32-128px)\n",
      "‚Ä¢ Confidence threshold: Final filtering of results\n",
      "‚Ä¢ Real-time search with progress tracking\n",
      "\n",
      "üéõÔ∏è PARAMETER TUNING:\n",
      "‚Ä¢ Similarity Threshold (0.1-0.8): Text-image matching sensitivity\n",
      "  - Lower = more detections, higher = more precise\n",
      "‚Ä¢ Patch Size (32-128px): Detection window size\n",
      "  - Smaller = more detailed, larger = faster processing\n",
      "‚Ä¢ Confidence Threshold (0.1-0.9): Final result filtering\n",
      "‚Ä¢ Start with: Similarity 0.2, Patch 64px, Confidence 0.25\n",
      "\n",
      "üìÅ OUTPUT STRUCTURE:\n",
      "search_results/\n",
      "‚îú‚îÄ‚îÄ search_20240611_143022/\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ 001_suspect1_sim0.32.jpg\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ 002_suspect5_sim0.45.jpg\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
      "\n",
      "üö® TROUBLESHOOTING:\n",
      "‚Ä¢ \"No matches found\" ‚Üí Lower similarity threshold (try 0.15-0.2)\n",
      "‚Ä¢ \"Too many false positives\" ‚Üí Raise similarity threshold\n",
      "‚Ä¢ \"Missing small objects\" ‚Üí Reduce patch size to 32-48px\n",
      "‚Ä¢ \"Slow processing\" ‚Üí Increase patch size or use faster model\n",
      "‚Ä¢ \"Memory issues\" ‚Üí Restart kernel, reduce patch count\n",
      "\n",
      "üîÑ MODEL COMPARISON:\n",
      "‚Ä¢ ViT-Base Patch32:\n",
      "  - Pros: Fast processing, good general performance\n",
      "  - Cons: Lower resolution features\n",
      "  - Best for: Quick analysis, large image sets\n",
      "‚Ä¢ ViT-Base Patch16:\n",
      "  - Pros: Better detail recognition, good balance\n",
      "  - Cons: Slower than Patch32\n",
      "  - Best for: Standard forensic analysis\n",
      "‚Ä¢ ViT-Large Patch14:\n",
      "  - Pros: Best accuracy, finest details\n",
      "  - Cons: Slowest processing, high memory usage\n",
      "  - Best for: Critical evidence analysis\n",
      "\n",
      "‚≠ê CLIP ADVANTAGES:\n",
      "‚Ä¢ Available in standard transformers library\n",
      "‚Ä¢ No special installation requirements\n",
      "‚Ä¢ Good zero-shot performance on diverse objects\n",
      "‚Ä¢ Robust text-image understanding\n",
      "‚Ä¢ Multiple model sizes for different needs\n",
      "\n",
      "üîç VS GLIP COMPARISON:\n",
      "‚Ä¢ CLIP: Widely available, good performance, simpler setup\n",
      "‚Ä¢ True GLIP: Better language grounding, more complex setup\n",
      "‚Ä¢ This implementation: GLIP-like functionality with CLIP convenience\n",
      "\n",
      "üí° FORENSIC BEST PRACTICES:\n",
      "‚Ä¢ Start with simple object names: \"weapon\", \"person\", \"vehicle\"\n",
      "‚Ä¢ Use multiple patch sizes for comprehensive analysis\n",
      "‚Ä¢ Cross-reference results between different models\n",
      "‚Ä¢ Adjust thresholds based on image quality and lighting\n",
      "‚Ä¢ Document parameter settings for evidence reports\n",
      "\n",
      "üéØ PERFORMANCE EXPECTATIONS:\n",
      "‚Ä¢ CLIP excels at recognizing common objects and people\n",
      "‚Ä¢ Good performance on weapons, vehicles, and clothing\n",
      "‚Ä¢ Works well with clear, well-lit images\n",
      "‚Ä¢ May require parameter tuning for optimal results\n",
      "‚Ä¢ Best results with objects that fill a significant portion of patches\n",
      "\n",
      "üìä OPTIMIZATION TIPS:\n",
      "‚Ä¢ For speed: Use Patch32 model, larger patch sizes (96-128px)\n",
      "‚Ä¢ For accuracy: Use Large model, smaller patches (32-48px)\n",
      "‚Ä¢ For balance: Use Patch16 model, medium patches (64px)\n",
      "‚Ä¢ Memory saving: Reduce TOP_K_PATCHES in config\n",
      "‚Ä¢ Quality images: Lower similarity thresholds work better\n",
      "\n",
      "üîç QUERY OPTIMIZATION:\n",
      "‚Ä¢ Simple terms often work best: \"weapon\" vs \"person holding weapon\"\n",
      "‚Ä¢ Try synonyms: \"car\" vs \"vehicle\", \"gun\" vs \"weapon\"\n",
      "‚Ä¢ Use single concepts per query for clearer results\n",
      "‚Ä¢ Combine results from multiple related queries\n",
      "\n",
      "üéì ADVANCED USAGE:\n",
      "‚Ä¢ Batch processing for systematic investigation\n",
      "‚Ä¢ Multiple model comparison for validation\n",
      "‚Ä¢ Parameter sweeps to find optimal settings\n",
      "‚Ä¢ Integration with other detection models\n",
      "‚Ä¢ Custom patch extraction for specific use cases\n",
      "\n",
      "üìà EXPECTED RESULTS:\n",
      "‚Ä¢ Similarity scores typically range 0.15-0.6\n",
      "‚Ä¢ Higher scores indicate stronger text-image match\n",
      "‚Ä¢ Results depend heavily on image quality and object visibility\n",
      "‚Ä¢ Fine-tune thresholds based on your specific dataset\n",
      "\n",
      "For batch processing of multiple queries, use the functions in Cell 6.\n",
      "For systematic forensic investigations, consider parameter optimization.\n",
      "This CLIP-based approach provides practical GLIP-like functionality without complex setup requirements.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
