{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cell 1: Configuration and Setup\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Configuration Settings\n",
    "CONFIG = {\n",
    "    # Model Selection (OpenAI CLIP)\n",
    "    'MODEL_NAME': 'ViT-B/32',  # Options: RN50, RN101, ViT-B/32, ViT-B/16, ViT-L/14\n",
    "    'DEVICE': 'auto',  # 'auto', 'cpu', 'cuda'\n",
    "\n",
    "    # Paths\n",
    "    'SUSPECTS_GALLERY_PATH': './suspects_gallery',\n",
    "    'RESULTS_OUTPUT_PATH': './search_results',\n",
    "\n",
    "    # Detection parameters\n",
    "    'SIMILARITY_THRESHOLD': 0.25,\n",
    "    'PATCH_SIZE': 224,  # CLIP input size\n",
    "    'STRIDE': 112,      # Half of patch size for overlap\n",
    "    'MAX_PATCHES': 16,  # Limit patches per image\n",
    "\n",
    "    # Processing settings\n",
    "    'BATCH_SIZE': 8,\n",
    "    'MAX_RESULTS_DISPLAY': 10,\n",
    "    'FIGURE_SIZE': (12, 8),\n",
    "}\n",
    "\n",
    "# Available OpenAI CLIP models\n",
    "AVAILABLE_MODELS = {\n",
    "    'RN50': {\n",
    "        'name': 'ResNet-50',\n",
    "        'description': 'Fastest model - good for quick analysis',\n",
    "        'performance': 'Fast speed, good accuracy'\n",
    "    },\n",
    "    'RN101': {\n",
    "        'name': 'ResNet-101',\n",
    "        'description': 'Balanced performance and speed',\n",
    "        'performance': 'Balanced speed and accuracy'\n",
    "    },\n",
    "    'ViT-B/32': {\n",
    "        'name': 'ViT-Base/32',\n",
    "        'description': 'Vision Transformer - good balance (recommended)',\n",
    "        'performance': 'Good performance, moderate speed'\n",
    "    },\n",
    "    'ViT-B/16': {\n",
    "        'name': 'ViT-Base/16',\n",
    "        'description': 'Higher resolution ViT - better accuracy',\n",
    "        'performance': 'Better accuracy, slower speed'\n",
    "    },\n",
    "    'ViT-L/14': {\n",
    "        'name': 'ViT-Large/14',\n",
    "        'description': 'Largest model - best accuracy, slowest',\n",
    "        'performance': 'Best accuracy, slowest speed'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully\")\n",
    "print(f\"üìÅ Suspects gallery: {CONFIG['SUSPECTS_GALLERY_PATH']}\")\n",
    "print(f\"üìÅ Results output: {CONFIG['RESULTS_OUTPUT_PATH']}\")\n",
    "print(f\"üîç Selected model: {CONFIG['MODEL_NAME']}\")\n",
    "\n",
    "# Cell 2: Install and Import Dependencies\n",
    "try:\n",
    "    import clip\n",
    "    print(\"‚úÖ OpenAI CLIP already installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Installing OpenAI CLIP...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    # Install required packages\n",
    "    packages = [\n",
    "        \"torch\",\n",
    "        \"torchvision\",\n",
    "        \"git+https://github.com/openai/CLIP.git\",\n",
    "        \"ipywidgets\",\n",
    "        \"matplotlib\",\n",
    "        \"pillow\",\n",
    "        \"numpy\"\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Failed to install {package}: {e}\")\n",
    "\n",
    "    print(\"üì¶ Installation complete - please restart kernel and run again\")\n",
    "\n",
    "# Import required libraries\n",
    "try:\n",
    "    import clip\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "\n",
    "    print(\"‚úÖ All dependencies imported successfully\")\n",
    "\n",
    "    # Check device compatibility\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üöÄ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        default_device = \"cuda\"\n",
    "    else:\n",
    "        print(\"üñ•Ô∏è Using CPU mode\")\n",
    "        default_device = \"cpu\"\n",
    "\n",
    "    if CONFIG['DEVICE'] == 'auto':\n",
    "        CONFIG['DEVICE'] = default_device\n",
    "        print(f\"üìç Device: {CONFIG['DEVICE']}\")\n",
    "\n",
    "    # Show available models\n",
    "    available_models = clip.available_models()\n",
    "    print(f\"üîç Available CLIP models: {available_models}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Please restart kernel and run Cell 2 again\")\n",
    "\n",
    "# Cell 3: Initialize Model and Setup\n",
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories\"\"\"\n",
    "    os.makedirs(CONFIG['SUSPECTS_GALLERY_PATH'], exist_ok=True)\n",
    "    os.makedirs(CONFIG['RESULTS_OUTPUT_PATH'], exist_ok=True)\n",
    "    print(f\"üìÅ Directories ready\")\n",
    "\n",
    "def load_clip_model(model_name=None):\n",
    "    \"\"\"Load OpenAI CLIP model\"\"\"\n",
    "    try:\n",
    "        if model_name is None:\n",
    "            model_name = CONFIG['MODEL_NAME']\n",
    "\n",
    "        device = CONFIG['DEVICE']\n",
    "        print(f\"üì• Loading {model_name} on {device}...\")\n",
    "\n",
    "        model, preprocess = clip.load(model_name, device=device)\n",
    "        model.eval()\n",
    "\n",
    "        print(f\"‚úÖ Model loaded successfully\")\n",
    "        return model, preprocess, device, model_name\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def switch_model(model_key):\n",
    "    \"\"\"Switch to different model\"\"\"\n",
    "    if model_key in AVAILABLE_MODELS:\n",
    "        CONFIG['MODEL_NAME'] = model_key\n",
    "        print(f\"üîÑ Switching to {AVAILABLE_MODELS[model_key]['name']}\")\n",
    "        return load_clip_model()\n",
    "    else:\n",
    "        print(f\"‚ùå Unknown model: {model_key}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Initialize\n",
    "setup_directories()\n",
    "model, preprocess, device, model_name = load_clip_model()\n",
    "\n",
    "# Cell 4: Core Search Functions\n",
    "def extract_patches(image, patch_size=224, stride=112, max_patches=16):\n",
    "    \"\"\"Extract patches from image\"\"\"\n",
    "    width, height = image.size\n",
    "    patches = []\n",
    "    positions = []\n",
    "\n",
    "    # Calculate patch positions\n",
    "    x_positions = range(0, max(1, width - patch_size + 1), stride)\n",
    "    y_positions = range(0, max(1, height - patch_size + 1), stride)\n",
    "\n",
    "    for y in y_positions:\n",
    "        for x in x_positions:\n",
    "            # Ensure we don't go out of bounds\n",
    "            x_end = min(x + patch_size, width)\n",
    "            y_end = min(y + patch_size, height)\n",
    "\n",
    "            # Extract patch\n",
    "            patch = image.crop((x, y, x_end, y_end))\n",
    "\n",
    "            # Resize to CLIP input size if needed\n",
    "            if patch.size != (patch_size, patch_size):\n",
    "                patch = patch.resize((patch_size, patch_size), Image.LANCZOS)\n",
    "\n",
    "            patches.append(patch)\n",
    "            positions.append((x, y, x_end, y_end))\n",
    "\n",
    "            # Limit number of patches\n",
    "            if len(patches) >= max_patches:\n",
    "                break\n",
    "        if len(patches) >= max_patches:\n",
    "            break\n",
    "\n",
    "    return patches, positions\n",
    "\n",
    "def compute_similarity(model, preprocess, text_query, images, device):\n",
    "    \"\"\"Compute CLIP similarity between text and images\"\"\"\n",
    "    try:\n",
    "        # Preprocess images\n",
    "        image_inputs = torch.stack([preprocess(img) for img in images]).to(device)\n",
    "\n",
    "        # Tokenize text\n",
    "        text_inputs = clip.tokenize([text_query]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get embeddings\n",
    "            image_features = model.encode_image(image_inputs)\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "\n",
    "            # Normalize features\n",
    "            image_features = F.normalize(image_features, dim=-1)\n",
    "            text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "            # Calculate similarities\n",
    "            similarities = torch.matmul(image_features, text_features.T).squeeze(-1)\n",
    "\n",
    "        return similarities.cpu().numpy()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in similarity computation: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "def search_single_image(image_path, model, preprocess, device, query):\n",
    "    \"\"\"Search single image for query\"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Extract patches\n",
    "        patches, positions = extract_patches(\n",
    "            image,\n",
    "            CONFIG['PATCH_SIZE'],\n",
    "            CONFIG['STRIDE'],\n",
    "            CONFIG['MAX_PATCHES']\n",
    "        )\n",
    "\n",
    "        if not patches:\n",
    "            return None\n",
    "\n",
    "        # Compute similarities\n",
    "        similarities = compute_similarity(model, preprocess, query, patches, device)\n",
    "\n",
    "        if len(similarities) == 0:\n",
    "            return None\n",
    "\n",
    "        # Filter by threshold\n",
    "        high_sim_indices = similarities >= CONFIG['SIMILARITY_THRESHOLD']\n",
    "\n",
    "        if not high_sim_indices.any():\n",
    "            return None\n",
    "\n",
    "        # Get results above threshold\n",
    "        filtered_similarities = similarities[high_sim_indices]\n",
    "        filtered_positions = [positions[i] for i in range(len(positions)) if high_sim_indices[i]]\n",
    "\n",
    "        return {\n",
    "            'image_path': image_path,\n",
    "            'image': image,\n",
    "            'boxes': torch.tensor(filtered_positions, dtype=torch.float32),\n",
    "            'scores': torch.tensor(filtered_similarities, dtype=torch.float32),\n",
    "            'query': query\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def search_images_with_query(query, model, preprocess, device, model_name, gallery_path):\n",
    "    \"\"\"Search all images for query with timing\"\"\"\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    start_time = time.time()\n",
    "    start_datetime = datetime.now()\n",
    "\n",
    "    print(f\"üîç Starting search for: '{query}'\")\n",
    "    print(f\"üñ•Ô∏è Model: {model_name} on {device}\")\n",
    "    print(f\"‚è∞ Start: {start_datetime.strftime('%H:%M:%S')}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Get image files\n",
    "    gallery_path = Path(gallery_path)\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}\n",
    "    image_files = [f for f in gallery_path.iterdir() if f.suffix.lower() in image_extensions]\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"‚ö†Ô∏è No images found in {gallery_path}\")\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    total_files = len(image_files)\n",
    "\n",
    "    print(f\"üì∏ Processing {total_files} images...\")\n",
    "\n",
    "    for i, img_path in enumerate(image_files):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Progress: {i+1}/{total_files}\", end='\\r')\n",
    "\n",
    "        result = search_single_image(img_path, model, preprocess, device, query)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "\n",
    "        # Memory cleanup\n",
    "        if torch.cuda.is_available() and i % 20 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Calculate timing\n",
    "    end_time = time.time()\n",
    "    end_datetime = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    # Format duration\n",
    "    if duration >= 60:\n",
    "        duration_str = f\"{int(duration//60)}m {duration%60:.1f}s\"\n",
    "    else:\n",
    "        duration_str = f\"{duration:.1f}s\"\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"üìä SEARCH SUMMARY\")\n",
    "    print(f\"=\"*50)\n",
    "    print(f\"üì∏ Images processed: {total_files}\")\n",
    "    print(f\"‚úÖ Matches found: {len(results)}\")\n",
    "    print(f\"‚è∞ Start: {start_datetime.strftime('%H:%M:%S')}\")\n",
    "    print(f\"üèÅ End: {end_datetime.strftime('%H:%M:%S')}\")\n",
    "    print(f\"‚è±Ô∏è Duration: {duration_str}\")\n",
    "    print(f\"üìà Avg per image: {duration/total_files:.2f}s\")\n",
    "    print(f\"=\"*50)\n",
    "\n",
    "    return results\n",
    "\n",
    "def copy_results_to_folder(results, output_folder):\n",
    "    \"\"\"Copy matched images to results folder\"\"\"\n",
    "    if not results:\n",
    "        return None, []\n",
    "\n",
    "    output_path = Path(output_folder)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Create timestamped folder\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    search_folder = output_path / f\"search_{timestamp}\"\n",
    "    search_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    copied_files = []\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        try:\n",
    "            source_path = result['image_path']\n",
    "            max_score = float(result['scores'].max()) if len(result['scores']) > 0 else 0.0\n",
    "            filename = f\"{i+1:03d}_{source_path.stem}_sim{max_score:.2f}{source_path.suffix}\"\n",
    "            dest_path = search_folder / filename\n",
    "\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            copied_files.append(dest_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying {source_path}: {e}\")\n",
    "\n",
    "    print(f\"üìã Copied {len(copied_files)} files to {search_folder}\")\n",
    "    return search_folder, copied_files\n",
    "\n",
    "# Cell 5: Interactive Interface\n",
    "def create_search_interface():\n",
    "    \"\"\"Create search interface\"\"\"\n",
    "\n",
    "    # Widgets\n",
    "    model_selector = widgets.Dropdown(\n",
    "        options=[(f\"{info['name']} - {info['description']}\", key)\n",
    "                for key, info in AVAILABLE_MODELS.items()],\n",
    "        value='ViT-B/32',\n",
    "        description='Model:',\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "\n",
    "    query_input = widgets.Text(\n",
    "        value='person with weapon',\n",
    "        placeholder='Enter search query...',\n",
    "        description='Query:',\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "\n",
    "    threshold_slider = widgets.FloatSlider(\n",
    "        value=CONFIG['SIMILARITY_THRESHOLD'],\n",
    "        min=0.1,\n",
    "        max=0.8,\n",
    "        step=0.05,\n",
    "        description='Threshold:',\n",
    "        readout_format='.2f'\n",
    "    )\n",
    "\n",
    "    search_button = widgets.Button(\n",
    "        description='üîç Search',\n",
    "        button_style='primary',\n",
    "        layout=widgets.Layout(width='120px')\n",
    "    )\n",
    "\n",
    "    copy_button = widgets.Button(\n",
    "        description='üìã Copy Results',\n",
    "        button_style='success',\n",
    "        layout=widgets.Layout(width='120px'),\n",
    "        disabled=True\n",
    "    )\n",
    "\n",
    "    switch_button = widgets.Button(\n",
    "        description='üîÑ Switch Model',\n",
    "        button_style='info',\n",
    "        layout=widgets.Layout(width='120px')\n",
    "    )\n",
    "\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    # State\n",
    "    search_results = []\n",
    "    current_model = model\n",
    "    current_preprocess = preprocess\n",
    "    current_device = device\n",
    "    current_model_name = model_name\n",
    "\n",
    "    def on_switch_model(b):\n",
    "        nonlocal current_model, current_preprocess, current_device, current_model_name\n",
    "        with output_area:\n",
    "            selected = model_selector.value\n",
    "            new_model, new_preprocess, new_device, new_name = switch_model(selected)\n",
    "            if new_model:\n",
    "                current_model = new_model\n",
    "                current_preprocess = new_preprocess\n",
    "                current_device = new_device\n",
    "                current_model_name = new_name\n",
    "                print(\"‚úÖ Model switched!\")\n",
    "            else:\n",
    "                print(\"‚ùå Model switch failed\")\n",
    "\n",
    "    def on_search(b):\n",
    "        nonlocal search_results\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            if not current_model:\n",
    "                print(\"‚ùå No model loaded\")\n",
    "                return\n",
    "\n",
    "            query = query_input.value.strip()\n",
    "            if not query:\n",
    "                print(\"‚ö†Ô∏è Enter a query\")\n",
    "                return\n",
    "\n",
    "            CONFIG['SIMILARITY_THRESHOLD'] = threshold_slider.value\n",
    "\n",
    "            search_results = search_images_with_query(\n",
    "                query, current_model, current_preprocess, current_device,\n",
    "                current_model_name, CONFIG['SUSPECTS_GALLERY_PATH']\n",
    "            )\n",
    "\n",
    "            if search_results:\n",
    "                copy_button.disabled = False\n",
    "                display_search_results(search_results[:CONFIG['MAX_RESULTS_DISPLAY']])\n",
    "                if len(search_results) > CONFIG['MAX_RESULTS_DISPLAY']:\n",
    "                    print(f\"\\nShowing {CONFIG['MAX_RESULTS_DISPLAY']} of {len(search_results)} results\")\n",
    "            else:\n",
    "                print(\"üîç No matches found\")\n",
    "                copy_button.disabled = True\n",
    "\n",
    "    def on_copy(b):\n",
    "        with output_area:\n",
    "            if search_results:\n",
    "                folder, files = copy_results_to_folder(search_results, CONFIG['RESULTS_OUTPUT_PATH'])\n",
    "                print(f\"‚úÖ Results saved to: {folder}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No results to copy\")\n",
    "\n",
    "    # Connect events\n",
    "    switch_button.on_click(on_switch_model)\n",
    "    search_button.on_click(on_search)\n",
    "    copy_button.on_click(on_copy)\n",
    "\n",
    "    # Layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>üîç Forensic Image Search (OpenAI CLIP)</h3>\"),\n",
    "        model_selector,\n",
    "        query_input,\n",
    "        threshold_slider,\n",
    "        widgets.HBox([search_button, copy_button, switch_button]),\n",
    "        widgets.HTML(\"<hr>\")\n",
    "    ])\n",
    "\n",
    "    return widgets.VBox([controls, output_area])\n",
    "\n",
    "def display_search_results(results):\n",
    "    \"\"\"Display results with bounding boxes\"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "\n",
    "    cols = 2\n",
    "    rows = (len(results) + cols - 1) // cols\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=CONFIG['FIGURE_SIZE'])\n",
    "    if rows == 1:\n",
    "        axes = [axes] if cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        ax = axes[i] if len(results) > 1 else axes\n",
    "\n",
    "        # Display image\n",
    "        ax.imshow(result['image'])\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        boxes = result['boxes']\n",
    "        scores = result['scores']\n",
    "\n",
    "        for box, score in zip(boxes, scores):\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), x2 - x1, y2 - y1,\n",
    "                linewidth=2, edgecolor='red', facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Add score\n",
    "            ax.text(x1, y1 - 5, f'{score:.2f}',\n",
    "                   color='red', fontweight='bold', fontsize=9)\n",
    "\n",
    "        ax.set_title(f\"{result['image_path'].name}\\nMatches: {len(boxes)}\", fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for j in range(len(results), len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display interface\n",
    "interface = create_search_interface()\n",
    "display(interface)\n",
    "\n",
    "# Cell 6: Batch Processing\n",
    "def run_batch_analysis(custom_queries=None):\n",
    "    \"\"\"Run batch analysis with multiple queries\"\"\"\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Default queries\n",
    "    default_queries = [\n",
    "        \"person with weapon\",\n",
    "        \"weapon\",\n",
    "        \"gun\",\n",
    "        \"knife\",\n",
    "        \"suspicious person\",\n",
    "        \"vehicle\",\n",
    "        \"mask\",\n",
    "        \"backpack\"\n",
    "    ]\n",
    "\n",
    "    queries = default_queries + (custom_queries or [])\n",
    "\n",
    "    print(f\"üöÄ Starting batch analysis with {len(queries)} queries\")\n",
    "    batch_start = time.time()\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\n[{i}/{len(queries)}] Processing: '{query}'\")\n",
    "\n",
    "        query_start = time.time()\n",
    "        results = search_images_with_query(\n",
    "            query, model, preprocess, device, model_name,\n",
    "            CONFIG['SUSPECTS_GALLERY_PATH']\n",
    "        )\n",
    "        query_time = time.time() - query_start\n",
    "\n",
    "        all_results[query] = {\n",
    "            'results': results,\n",
    "            'count': len(results),\n",
    "            'time': query_time\n",
    "        }\n",
    "\n",
    "        if results:\n",
    "            folder, files = copy_results_to_folder(results, CONFIG['RESULTS_OUTPUT_PATH'])\n",
    "            print(f\"üìÅ Saved to: {folder.name}\")\n",
    "\n",
    "    # Summary\n",
    "    batch_time = time.time() - batch_start\n",
    "    total_matches = sum(data['count'] for data in all_results.values())\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üìä BATCH SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    for query, data in all_results.items():\n",
    "        status = \"‚úÖ\" if data['count'] > 0 else \"‚ö™\"\n",
    "        print(f\"{status} '{query}': {data['count']} matches ({data['time']:.1f}s)\")\n",
    "\n",
    "    print(f\"\\nüéØ Total: {total_matches} matches in {batch_time/60:.1f} minutes\")\n",
    "    return all_results\n",
    "\n",
    "# Cell 7: Usage Guide\n",
    "print(\"\"\"\n",
    "üéØ FORENSIC IMAGE SEARCH - OPENAI CLIP\n",
    "=====================================\n",
    "\n",
    "üìã SETUP:\n",
    "1. Place images in './suspects_gallery' folder\n",
    "2. Run cells 1-5 in order\n",
    "3. Use the interface above to search\n",
    "\n",
    "üîç AVAILABLE MODELS:\n",
    "‚Ä¢ RN50: Fastest\n",
    "‚Ä¢ ViT-B/32: Recommended balance\n",
    "‚Ä¢ ViT-B/16: Higher accuracy\n",
    "‚Ä¢ ViT-L/14: Best accuracy, slowest\n",
    "\n",
    "üéõÔ∏è PARAMETERS:\n",
    "‚Ä¢ Similarity Threshold: 0.1-0.8 (start with 0.25)\n",
    "‚Ä¢ Lower = more results, higher = more precise\n",
    "\n",
    "üîç GOOD QUERIES:\n",
    "‚Ä¢ \"weapon\", \"gun\", \"knife\"\n",
    "‚Ä¢ \"person\", \"suspicious person\"\n",
    "‚Ä¢ \"vehicle\", \"car\"\n",
    "‚Ä¢ \"mask\", \"backpack\"\n",
    "\n",
    "üí° TIPS:\n",
    "‚Ä¢ Use simple, clear terms\n",
    "‚Ä¢ Try different thresholds\n",
    "‚Ä¢ Check timing info for optimization\n",
    "‚Ä¢ Use batch processing for multiple queries\n",
    "\n",
    "üö® TROUBLESHOOTING:\n",
    "‚Ä¢ Restart kernel if installation fails\n",
    "‚Ä¢ Lower threshold if no matches\n",
    "‚Ä¢ Use RN50 model for speed\n",
    "‚Ä¢ Check image folder path\n",
    "\n",
    "# Uncomment to run batch analysis:\n",
    "# batch_results = run_batch_analysis()\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
