{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T21:15:26.352237Z",
     "start_time": "2025-06-11T21:11:36.090464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# JUPYTER NOTEBOOK DETR + NLP GALLERY SEARCH\n",
    "# Run each cell sequentially\n",
    "# =============================================================================\n",
    "\n",
    "# CELL 1: Install Requirements (run once)\n",
    "!pip install torch transformers sentence-transformers pillow\n",
    "\n",
    "# CELL 2: Imports and Configuration\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration - Edit these as needed\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'suspect_gallery': '../datasets/images/objects/raw',\n",
    "    'results_folder': '../datasets/images/objects/detr_nlp_results',\n",
    "\n",
    "    # Model Selection\n",
    "    'approach': 'detr_sentence_similarity',  # 'detr_sentence_similarity', 'detr_clip', 'owlvit_direct'\n",
    "    'detection_model': 'facebook/detr-resnet-50',\n",
    "    'nlp_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    'clip_model': 'openai/clip-vit-base-patch32',\n",
    "\n",
    "    # Settings\n",
    "    'detection_confidence': 0.6,\n",
    "    'similarity_threshold': 0.3,\n",
    "    'max_results': 20,\n",
    "    'show_progress': True,\n",
    "    'image_formats': ['.jpg', '.jpeg', '.png', '.bmp', '.tiff'],\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Approach: {CONFIG['approach']}\")\n",
    "print(f\"   Gallery: {CONFIG['suspect_gallery']}\")\n",
    "print(f\"   Results: {CONFIG['results_folder']}\")\n",
    "\n",
    "# CELL 3: Setup Folders\n",
    "def setup_folders():\n",
    "    \"\"\"Create necessary folders\"\"\"\n",
    "    folders = [CONFIG['suspect_gallery'], CONFIG['results_folder'], './temp_detections']\n",
    "\n",
    "    for folder in folders:\n",
    "        Path(folder).mkdir(exist_ok=True)\n",
    "        print(f\"üìÅ Created: {folder}\")\n",
    "\n",
    "    print(f\"\\nüí° Next steps:\")\n",
    "    print(f\"1. Add suspect images to: {CONFIG['suspect_gallery']}\")\n",
    "    print(f\"2. Run the model loading cell\")\n",
    "    print(f\"3. Start searching!\")\n",
    "\n",
    "# Run setup\n",
    "setup_folders()\n",
    "\n",
    "# CELL 4: Model Loading Class\n",
    "class JupyterGallerySearcher:\n",
    "    \"\"\"Jupyter-friendly gallery searcher\"\"\"\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or CONFIG\n",
    "        self.models_loaded = False\n",
    "\n",
    "    def load_models(self):\n",
    "        \"\"\"Load models based on selected approach\"\"\"\n",
    "        print(f\"ü§ñ Loading models for: {self.config['approach']}\")\n",
    "\n",
    "        try:\n",
    "            if self.config['approach'] == 'detr_sentence_similarity':\n",
    "                self._load_detr_sentence()\n",
    "            elif self.config['approach'] == 'detr_clip':\n",
    "                self._load_detr_clip()\n",
    "            elif self.config['approach'] == 'owlvit_direct':\n",
    "                self._load_owlvit()\n",
    "\n",
    "            self.models_loaded = True\n",
    "            print(\"‚úÖ Models loaded successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Model loading failed: {e}\")\n",
    "            print(\"üí° Make sure you ran the install cell first\")\n",
    "            raise\n",
    "\n",
    "    def _load_detr_sentence(self):\n",
    "        \"\"\"Load DETR + Sentence Transformer\"\"\"\n",
    "        from transformers import AutoProcessor, AutoModelForObjectDetection\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "\n",
    "        print(\"   Loading DETR...\")\n",
    "        self.detr_processor = AutoProcessor.from_pretrained(self.config['detection_model'])\n",
    "        self.detr_model = AutoModelForObjectDetection.from_pretrained(self.config['detection_model'])\n",
    "\n",
    "        print(\"   Loading Sentence Transformer...\")\n",
    "        self.nlp_model = SentenceTransformer(self.config['nlp_model'])\n",
    "\n",
    "        print(\"   ‚úÖ DETR + Sentence Transformer ready\")\n",
    "\n",
    "    def _load_detr_clip(self):\n",
    "        \"\"\"Load DETR + CLIP\"\"\"\n",
    "        from transformers import AutoProcessor, AutoModelForObjectDetection, CLIPProcessor, CLIPModel\n",
    "\n",
    "        print(\"   Loading DETR...\")\n",
    "        self.detr_processor = AutoProcessor.from_pretrained(self.config['detection_model'])\n",
    "        self.detr_model = AutoModelForObjectDetection.from_pretrained(self.config['detection_model'])\n",
    "\n",
    "        print(\"   Loading CLIP...\")\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(self.config['clip_model'])\n",
    "        self.clip_model = CLIPModel.from_pretrained(self.config['clip_model'])\n",
    "\n",
    "        print(\"   ‚úÖ DETR + CLIP ready\")\n",
    "\n",
    "    def _load_owlvit(self):\n",
    "        \"\"\"Load OWL-ViT\"\"\"\n",
    "        from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "\n",
    "        print(\"   Loading OWL-ViT...\")\n",
    "        self.owlvit_processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "        self.owlvit_model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "\n",
    "        print(\"   ‚úÖ OWL-ViT ready\")\n",
    "\n",
    "    def get_gallery_images(self):\n",
    "        \"\"\"Get all images from gallery\"\"\"\n",
    "        gallery_path = Path(self.config['suspect_gallery'])\n",
    "\n",
    "        if not gallery_path.exists():\n",
    "            print(f\"‚ùå Gallery folder not found: {gallery_path}\")\n",
    "            return []\n",
    "\n",
    "        images = []\n",
    "        for ext in self.config['image_formats']:\n",
    "            images.extend(gallery_path.glob(f\"*{ext}\"))\n",
    "            images.extend(gallery_path.glob(f\"*{ext.upper()}\"))\n",
    "\n",
    "        return sorted(images)\n",
    "\n",
    "    def search_gallery(self, query, session_name=None):\n",
    "        \"\"\"Search gallery with natural language query\"\"\"\n",
    "        if not self.models_loaded:\n",
    "            print(\"‚ùå Models not loaded! Run the model loading cell first.\")\n",
    "            return []\n",
    "\n",
    "        if session_name is None:\n",
    "            session_name = f\"search_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "        print(f\"üîç Searching gallery for: '{query}'\")\n",
    "\n",
    "        # Get images\n",
    "        gallery_images = self.get_gallery_images()\n",
    "\n",
    "        if not gallery_images:\n",
    "            print(f\"‚ùå No images found in {self.config['suspect_gallery']}\")\n",
    "            print(f\"üí° Add images to the gallery folder first\")\n",
    "            return []\n",
    "\n",
    "        print(f\"   üìÅ Processing {len(gallery_images)} images...\")\n",
    "\n",
    "        # Search based on approach\n",
    "        if self.config['approach'] == 'detr_sentence_similarity':\n",
    "            matches = self._search_detr_sentence(gallery_images, query)\n",
    "        elif self.config['approach'] == 'detr_clip':\n",
    "            matches = self._search_detr_clip(gallery_images, query)\n",
    "        elif self.config['approach'] == 'owlvit_direct':\n",
    "            matches = self._search_owlvit(gallery_images, query)\n",
    "\n",
    "        # Copy results\n",
    "        if matches:\n",
    "            self._copy_results(matches, query, session_name)\n",
    "            print(f\"‚úÖ Found {len(matches)} matches!\")\n",
    "            print(f\"üì∏ Results copied to: {self.config['results_folder']}/{session_name}\")\n",
    "\n",
    "            # Show top matches\n",
    "            print(f\"\\nüèÜ Top matches:\")\n",
    "            for i, match in enumerate(matches[:3], 1):\n",
    "                score = match['similarity_score']\n",
    "                path = Path(match['image_path']).name\n",
    "                print(f\"   {i}. {path} (score: {score:.3f})\")\n",
    "        else:\n",
    "            print(f\"‚ùå No matches found for: '{query}'\")\n",
    "            print(f\"üí° Try lowering similarity_threshold or different query\")\n",
    "\n",
    "        return matches\n",
    "\n",
    "    def _search_detr_sentence(self, gallery_images, query):\n",
    "        \"\"\"DETR + Sentence similarity search\"\"\"\n",
    "        import torch\n",
    "        from PIL import Image\n",
    "\n",
    "        matches = []\n",
    "        query_embedding = self.nlp_model.encode([query])\n",
    "\n",
    "        for i, image_path in enumerate(gallery_images):\n",
    "            if i % 5 == 0:  # Progress every 5 images\n",
    "                print(f\"      üì∏ {i+1}/{len(gallery_images)}\")\n",
    "\n",
    "            try:\n",
    "                # Load and process image\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                inputs = self.detr_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "                # DETR detection\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.detr_model(**inputs)\n",
    "\n",
    "                # Post-process\n",
    "                target_sizes = torch.tensor([image.size[::-1]])\n",
    "                results = self.detr_processor.post_process_object_detection(\n",
    "                    outputs, target_sizes=target_sizes,\n",
    "                    threshold=self.config['detection_confidence']\n",
    "                )[0]\n",
    "\n",
    "                # Get detected objects\n",
    "                detected_objects = []\n",
    "                for label_id in results[\"labels\"]:\n",
    "                    label_name = self.detr_model.config.id2label[label_id.item()]\n",
    "                    detected_objects.append(label_name)\n",
    "\n",
    "                if detected_objects:\n",
    "                    # Create scene description\n",
    "                    scene_description = f\"Image contains: {', '.join(detected_objects)}\"\n",
    "\n",
    "                    # Calculate similarity\n",
    "                    scene_embedding = self.nlp_model.encode([scene_description])\n",
    "                    similarity = self.nlp_model.similarity(query_embedding, scene_embedding)[0][0].item()\n",
    "\n",
    "                    if similarity >= self.config['similarity_threshold']:\n",
    "                        matches.append({\n",
    "                            'image_path': str(image_path),\n",
    "                            'similarity_score': similarity,\n",
    "                            'detected_objects': detected_objects,\n",
    "                            'scene_description': scene_description\n",
    "                        })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ö†Ô∏è Error with {image_path.name}: {e}\")\n",
    "\n",
    "        # Sort by similarity\n",
    "        matches.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "        return matches[:self.config['max_results']]\n",
    "\n",
    "    def _search_detr_clip(self, gallery_images, query):\n",
    "        \"\"\"DETR + CLIP search\"\"\"\n",
    "        import torch\n",
    "        from PIL import Image\n",
    "\n",
    "        matches = []\n",
    "\n",
    "        for i, image_path in enumerate(gallery_images):\n",
    "            if i % 5 == 0:\n",
    "                print(f\"      üì∏ {i+1}/{len(gallery_images)}\")\n",
    "\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "                # DETR detection\n",
    "                inputs = self.detr_processor(images=image, return_tensors=\"pt\")\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.detr_model(**inputs)\n",
    "\n",
    "                target_sizes = torch.tensor([image.size[::-1]])\n",
    "                results = self.detr_processor.post_process_object_detection(\n",
    "                    outputs, target_sizes=target_sizes,\n",
    "                    threshold=self.config['detection_confidence']\n",
    "                )[0]\n",
    "\n",
    "                if len(results[\"boxes\"]) > 0:\n",
    "                    # CLIP similarity\n",
    "                    clip_inputs = self.clip_processor(text=[query], images=image, return_tensors=\"pt\", padding=True)\n",
    "                    with torch.no_grad():\n",
    "                        clip_outputs = self.clip_model(**clip_inputs)\n",
    "                        logits_per_image = clip_outputs.logits_per_image\n",
    "                        similarity = torch.nn.functional.softmax(logits_per_image, dim=1)[0][0].item()\n",
    "\n",
    "                    if similarity >= self.config['similarity_threshold']:\n",
    "                        detected_objects = []\n",
    "                        for label_id in results[\"labels\"]:\n",
    "                            label_name = self.detr_model.config.id2label[label_id.item()]\n",
    "                            detected_objects.append(label_name)\n",
    "\n",
    "                        matches.append({\n",
    "                            'image_path': str(image_path),\n",
    "                            'similarity_score': similarity,\n",
    "                            'detected_objects': detected_objects\n",
    "                        })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ö†Ô∏è Error with {image_path.name}: {e}\")\n",
    "\n",
    "        matches.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "        return matches[:self.config['max_results']]\n",
    "\n",
    "    def _search_owlvit(self, gallery_images, query):\n",
    "        \"\"\"OWL-ViT direct search\"\"\"\n",
    "        import torch\n",
    "        from PIL import Image\n",
    "\n",
    "        matches = []\n",
    "\n",
    "        for i, image_path in enumerate(gallery_images):\n",
    "            if i % 5 == 0:\n",
    "                print(f\"      üì∏ {i+1}/{len(gallery_images)}\")\n",
    "\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                inputs = self.owlvit_processor(text=[query], images=image, return_tensors=\"pt\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.owlvit_model(**inputs)\n",
    "\n",
    "                target_sizes = torch.Tensor([image.size[::-1]])\n",
    "                results = self.owlvit_processor.post_process_object_detection(\n",
    "                    outputs=outputs,\n",
    "                    target_sizes=target_sizes,\n",
    "                    threshold=self.config['similarity_threshold']\n",
    "                )\n",
    "\n",
    "                if results and len(results) > 0 and len(results[0][\"boxes\"]) > 0:\n",
    "                    result = results[0]\n",
    "                    scores = result[\"scores\"]\n",
    "                    avg_confidence = scores.mean().item()\n",
    "\n",
    "                    matches.append({\n",
    "                        'image_path': str(image_path),\n",
    "                        'similarity_score': avg_confidence,\n",
    "                        'detections': len(result[\"boxes\"])\n",
    "                    })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ö†Ô∏è Error with {image_path.name}: {e}\")\n",
    "\n",
    "        matches.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "        return matches[:self.config['max_results']]\n",
    "\n",
    "    def _copy_results(self, matches, query, session_name):\n",
    "        \"\"\"Copy results to results folder\"\"\"\n",
    "        session_folder = Path(self.config['results_folder']) / session_name\n",
    "        session_folder.mkdir(exist_ok=True)\n",
    "\n",
    "        # Save metadata\n",
    "        search_metadata = {\n",
    "            'query': query,\n",
    "            'session_name': session_name,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'approach': self.config['approach'],\n",
    "            'total_matches': len(matches),\n",
    "            'matches': matches\n",
    "        }\n",
    "\n",
    "        with open(session_folder / 'search_metadata.json', 'w') as f:\n",
    "            json.dump(search_metadata, f, indent=2)\n",
    "\n",
    "        # Copy images\n",
    "        for i, match in enumerate(matches):\n",
    "            try:\n",
    "                source_path = Path(match['image_path'])\n",
    "                similarity_score = match['similarity_score']\n",
    "                new_name = f\"{i+1:03d}_score_{similarity_score:.3f}_{source_path.name}\"\n",
    "                dest_path = session_folder / new_name\n",
    "                shutil.copy2(source_path, dest_path)\n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ùå Copy error: {e}\")\n",
    "\n",
    "# Initialize the searcher\n",
    "searcher = JupyterGallerySearcher(CONFIG)\n",
    "print(\"‚úÖ Gallery searcher ready\")\n",
    "\n",
    "# CELL 5: Load Models (run this after adding images to gallery)\n",
    "print(\"ü§ñ Loading models...\")\n",
    "searcher.load_models()\n",
    "\n",
    "# CELL 6: Quick Gallery Check\n",
    "gallery_images = searcher.get_gallery_images()\n",
    "print(f\"üìÅ Gallery status:\")\n",
    "print(f\"   Location: {CONFIG['suspect_gallery']}\")\n",
    "print(f\"   Images found: {len(gallery_images)}\")\n",
    "\n",
    "if gallery_images:\n",
    "    print(f\"   Sample images:\")\n",
    "    for i, img in enumerate(gallery_images[:5], 1):\n",
    "        print(f\"     {i}. {img.name}\")\n",
    "    if len(gallery_images) > 5:\n",
    "        print(f\"     ... and {len(gallery_images) - 5} more\")\n",
    "else:\n",
    "    print(f\"   ‚ùå No images found!\")\n",
    "    print(f\"   üí° Add images to: {CONFIG['suspect_gallery']}\")\n",
    "\n",
    "# CELL 7: Search Examples - Run these to test\n",
    "def run_search_examples():\n",
    "    \"\"\"Run example searches\"\"\"\n",
    "\n",
    "    if not searcher.models_loaded:\n",
    "        print(\"‚ùå Load models first!\")\n",
    "        return\n",
    "\n",
    "    example_queries = [\n",
    "        # \"person\",\n",
    "        # \"person with phone\",\n",
    "        # \"person holding object\",\n",
    "        # \"car\",\n",
    "        # \"person near car\"\n",
    "    ]\n",
    "\n",
    "    print(\"üîç Running example searches...\")\n",
    "\n",
    "    for query in example_queries:\n",
    "        print(f\"\\n\" + \"=\"*50)\n",
    "        matches = searcher.search_gallery(query, f\"example_{query.replace(' ', '_')}\")\n",
    "\n",
    "        if matches:\n",
    "            print(f\"‚úÖ '{query}': {len(matches)} matches\")\n",
    "        else:\n",
    "            print(f\"‚ùå '{query}': No matches\")\n",
    "\n",
    "    print(f\"\\nüéØ Example searches complete!\")\n",
    "    print(f\"üìÇ Check results in: {CONFIG['results_folder']}\")\n",
    "\n",
    "# Uncomment to run examples:\n",
    "# run_search_examples()\n",
    "\n",
    "# CELL 8: Custom Search Function\n",
    "def search_custom(query, approach=None, threshold=None):\n",
    "    \"\"\"\n",
    "    Custom search function for easy testing\n",
    "\n",
    "    Args:\n",
    "        query: Search query string\n",
    "        approach: 'detr_sentence_similarity', 'detr_clip', 'owlvit_direct'\n",
    "        threshold: Similarity threshold (0.1-0.9)\n",
    "    \"\"\"\n",
    "\n",
    "    # Update config if parameters provided\n",
    "    if approach:\n",
    "        CONFIG['approach'] = approach\n",
    "        print(f\"üîÑ Switched to approach: {approach}\")\n",
    "        searcher.config['approach'] = approach\n",
    "        # Note: You'll need to reload models if switching approaches\n",
    "\n",
    "    if threshold:\n",
    "        CONFIG['similarity_threshold'] = threshold\n",
    "        searcher.config['similarity_threshold'] = threshold\n",
    "        print(f\"üéØ Set threshold to: {threshold}\")\n",
    "\n",
    "    # Run search\n",
    "    matches = searcher.search_gallery(query)\n",
    "\n",
    "    # Display results\n",
    "    if matches:\n",
    "        print(f\"\\nüìä Search Results for '{query}':\")\n",
    "        print(f\"   Total matches: {len(matches)}\")\n",
    "        print(f\"   Approach: {CONFIG['approach']}\")\n",
    "        print(f\"   Threshold: {CONFIG['similarity_threshold']}\")\n",
    "\n",
    "        print(f\"\\nüèÜ Top 5 matches:\")\n",
    "        for i, match in enumerate(matches[:5], 1):\n",
    "            score = match['similarity_score']\n",
    "            path = Path(match['image_path']).name\n",
    "            print(f\"   {i}. {path} - Score: {score:.3f}\")\n",
    "\n",
    "    return matches\n",
    "\n",
    "# CELL 9: Interactive Search Cell - Edit and run as needed\n",
    "\n",
    "# Example searches - edit these and run:\n",
    "\n",
    "# Basic search\n",
    "# matches1 = search_custom(\"person\")\n",
    "\n",
    "# More specific search\n",
    "matches2 = search_custom(\"person holding gum\")\n",
    "\n",
    "# With custom threshold (more sensitive)\n",
    "# matches3 = search_custom(\"person with phone\", threshold=0.2)\n",
    "\n",
    "# Different approach\n",
    "# Note: Need to reload models when switching approaches\n",
    "# matches4 = search_custom(\"weapon\", approach='owlvit_direct')\n",
    "\n",
    "print(\"üéØ Search complete! Check the results folders.\")\n",
    "\n",
    "# CELL 10: Experiment with Different Settings\n",
    "def experiment_settings():\n",
    "    \"\"\"Experiment with different thresholds and approaches\"\"\"\n",
    "\n",
    "    test_query = \"person\"  # Change this to your test query\n",
    "    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "    print(f\"üß™ Experimenting with query: '{test_query}'\")\n",
    "    print(f\"üìä Testing different thresholds...\")\n",
    "\n",
    "    results_summary = {}\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        matches = search_custom(test_query, threshold=threshold)\n",
    "        results_summary[threshold] = len(matches)\n",
    "        print(f\"   Threshold {threshold}: {len(matches)} matches\")\n",
    "\n",
    "    print(f\"\\nüìà Results Summary:\")\n",
    "    for threshold, count in results_summary.items():\n",
    "        print(f\"   {threshold}: {count} matches\")\n",
    "\n",
    "    return results_summary\n",
    "\n",
    "# Uncomment to run experiment:\n",
    "# experiment_results = experiment_settings()\n",
    "\n",
    "# CELL 11: Analyze Results\n",
    "def analyze_results():\n",
    "    \"\"\"Analyze search results\"\"\"\n",
    "    results_path = Path(CONFIG['results_folder'])\n",
    "\n",
    "    print(f\"üìä Results Analysis\")\n",
    "    print(f\"üìÇ Results folder: {results_path}\")\n",
    "\n",
    "    if not results_path.exists():\n",
    "        print(\"‚ùå No results folder found\")\n",
    "        return\n",
    "\n",
    "    sessions = list(results_path.iterdir())\n",
    "\n",
    "    if not sessions:\n",
    "        print(\"‚ùå No search sessions found\")\n",
    "        return\n",
    "\n",
    "    print(f\"üìã Found {len(sessions)} search sessions:\")\n",
    "\n",
    "    for session in sessions:\n",
    "        if session.is_dir():\n",
    "            metadata_file = session / 'search_metadata.json'\n",
    "\n",
    "            if metadata_file.exists():\n",
    "                try:\n",
    "                    with open(metadata_file, 'r') as f:\n",
    "                        metadata = json.load(f)\n",
    "\n",
    "                    print(f\"\\nüìÅ {session.name}\")\n",
    "                    print(f\"   Query: '{metadata['query']}'\")\n",
    "                    print(f\"   Matches: {metadata['total_matches']}\")\n",
    "                    print(f\"   Approach: {metadata['approach']}\")\n",
    "\n",
    "                    if metadata['matches']:\n",
    "                        best_score = metadata['matches'][0]['similarity_score']\n",
    "                        print(f\"   Best score: {best_score:.3f}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Error reading metadata: {e}\")\n",
    "\n",
    "# Run analysis\n",
    "analyze_results()\n",
    "\n",
    "print(\"\\nüéâ Jupyter DETR+NLP System Ready!\")\n",
    "print(\"üí° Edit the search queries in the cells above and run them to test different approaches\")"
   ],
   "id": "f853d0326f297a84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: transformers in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: sentence-transformers in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: pillow in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (11.2.1)\n",
      "Requirement already satisfied: filelock in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from transformers) (0.32.4)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\interpreter\\python\\projects\\ai_cyberforensics\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "‚úÖ Configuration loaded\n",
      "   Approach: detr_sentence_similarity\n",
      "   Gallery: ../datasets/images/objects/raw\n",
      "   Results: ../datasets/images/objects/detr_nlp_results\n",
      "üìÅ Created: ../datasets/images/objects/raw\n",
      "üìÅ Created: ../datasets/images/objects/detr_nlp_results\n",
      "üìÅ Created: ./temp_detections\n",
      "\n",
      "üí° Next steps:\n",
      "1. Add suspect images to: ../datasets/images/objects/raw\n",
      "2. Run the model loading cell\n",
      "3. Start searching!\n",
      "‚úÖ Gallery searcher ready\n",
      "ü§ñ Loading models...\n",
      "ü§ñ Loading models for: detr_sentence_similarity\n",
      "   Loading DETR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loading Sentence Transformer...\n",
      "   ‚úÖ DETR + Sentence Transformer ready\n",
      "‚úÖ Models loaded successfully!\n",
      "üìÅ Gallery status:\n",
      "   Location: ../datasets/images/objects/raw\n",
      "   Images found: 242\n",
      "   Sample images:\n",
      "     1. 445_jpg.rf.2e04379013684f454abbc00564910fcc.jpg\n",
      "     2. 445_jpg.rf.2e04379013684f454abbc00564910fcc.jpg\n",
      "     3. ABbframe00154_jpg.rf.d56723a3de6d775966551d0d0e8add67.jpg\n",
      "     4. ABbframe00154_jpg.rf.d56723a3de6d775966551d0d0e8add67.jpg\n",
      "     5. ABbframe00160_jpg.rf.23c17f2cfeacd7873e38995e4832e0c1.jpg\n",
      "     ... and 237 more\n",
      "üîç Searching gallery for: 'person holding gum'\n",
      "   üìÅ Processing 242 images...\n",
      "      üì∏ 1/242\n",
      "      üì∏ 6/242\n",
      "      üì∏ 11/242\n",
      "      üì∏ 16/242\n",
      "      üì∏ 21/242\n",
      "      üì∏ 26/242\n",
      "      üì∏ 31/242\n",
      "      üì∏ 36/242\n",
      "      üì∏ 41/242\n",
      "      üì∏ 46/242\n",
      "      üì∏ 51/242\n",
      "      üì∏ 56/242\n",
      "      üì∏ 61/242\n",
      "      üì∏ 66/242\n",
      "      üì∏ 71/242\n",
      "      üì∏ 76/242\n",
      "      üì∏ 81/242\n",
      "      üì∏ 86/242\n",
      "      üì∏ 91/242\n",
      "      üì∏ 96/242\n",
      "      üì∏ 101/242\n",
      "      üì∏ 106/242\n",
      "      üì∏ 111/242\n",
      "      üì∏ 116/242\n",
      "      üì∏ 121/242\n",
      "      üì∏ 126/242\n",
      "      üì∏ 131/242\n",
      "      üì∏ 136/242\n",
      "      üì∏ 141/242\n",
      "      üì∏ 146/242\n",
      "      üì∏ 151/242\n",
      "      üì∏ 156/242\n",
      "      üì∏ 161/242\n",
      "      üì∏ 166/242\n",
      "      üì∏ 171/242\n",
      "      üì∏ 176/242\n",
      "      üì∏ 181/242\n",
      "      üì∏ 186/242\n",
      "      üì∏ 191/242\n",
      "      üì∏ 196/242\n",
      "      üì∏ 201/242\n",
      "      üì∏ 206/242\n",
      "      üì∏ 211/242\n",
      "      üì∏ 216/242\n",
      "      üì∏ 221/242\n",
      "      üì∏ 226/242\n",
      "      üì∏ 231/242\n",
      "      üì∏ 236/242\n",
      "      üì∏ 241/242\n",
      "‚úÖ Found 14 matches!\n",
      "üì∏ Results copied to: ../datasets/images/objects/detr_nlp_results/search_20250611_171140\n",
      "\n",
      "üèÜ Top matches:\n",
      "   1. knife_163_jpg.rf.fe4ca068df4cdc83927e29e9184bb66b.jpg (score: 0.324)\n",
      "   2. knife_163_jpg.rf.fe4ca068df4cdc83927e29e9184bb66b.jpg (score: 0.324)\n",
      "   3. knife_227_jpg.rf.de1f020e98d188e9abe1ae49accba966.jpg (score: 0.324)\n",
      "\n",
      "üìä Search Results for 'person holding gum':\n",
      "   Total matches: 14\n",
      "   Approach: detr_sentence_similarity\n",
      "   Threshold: 0.3\n",
      "\n",
      "üèÜ Top 5 matches:\n",
      "   1. knife_163_jpg.rf.fe4ca068df4cdc83927e29e9184bb66b.jpg - Score: 0.324\n",
      "   2. knife_163_jpg.rf.fe4ca068df4cdc83927e29e9184bb66b.jpg - Score: 0.324\n",
      "   3. knife_227_jpg.rf.de1f020e98d188e9abe1ae49accba966.jpg - Score: 0.324\n",
      "   4. knife_227_jpg.rf.de1f020e98d188e9abe1ae49accba966.jpg - Score: 0.324\n",
      "   5. knife_195.jpg - Score: 0.318\n",
      "üéØ Search complete! Check the results folders.\n",
      "üìä Results Analysis\n",
      "üìÇ Results folder: ..\\datasets\\images\\objects\\detr_nlp_results\n",
      "üìã Found 4 search sessions:\n",
      "\n",
      "üìÅ search_20250611_171140\n",
      "   Query: 'person holding gum'\n",
      "   Matches: 14\n",
      "   Approach: detr_sentence_similarity\n",
      "   Best score: 0.324\n",
      "\n",
      "üéâ Jupyter DETR+NLP System Ready!\n",
      "üí° Edit the search queries in the cells above and run them to test different approaches\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
