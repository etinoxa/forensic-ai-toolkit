{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Worked with Python_3.9",
   "id": "1170857b95ac103b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T23:11:02.344221Z",
     "start_time": "2025-07-09T23:11:02.330389Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install ipywidgets paddlepaddle paddleocr",
   "id": "abbda9d04f0079e1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text_ocr_000.jpg',\n",
       " 'Text_ocr_001.jpg',\n",
       " 'Text_ocr_002.jpg',\n",
       " 'Text_ocr_003.jpg',\n",
       " 'Text_ocr_004.jpg',\n",
       " 'Text_ocr_005.jpg',\n",
       " 'Text_ocr_006.jpg',\n",
       " 'Text_ocr_007.jpg',\n",
       " 'Text_ocr_008.jpg',\n",
       " 'Text_ocr_009.jpg',\n",
       " 'Text_ocr_010.jpg',\n",
       " 'Text_ocr_011.jpg',\n",
       " 'Text_ocr_012.jpg',\n",
       " 'Text_ocr_013.jpg',\n",
       " 'Text_ocr_014.jpg',\n",
       " 'Text_ocr_015.jpg',\n",
       " 'Text_ocr_016.jpg',\n",
       " 'Text_ocr_017.jpg',\n",
       " 'Text_ocr_018.jpg',\n",
       " 'Text_ocr_019.jpg',\n",
       " 'Text_ocr_020.jpg',\n",
       " 'Text_ocr_021.jpg',\n",
       " 'Text_ocr_022.jpg',\n",
       " 'Text_ocr_023.jpg',\n",
       " 'Text_ocr_024.jpg',\n",
       " 'Text_ocr_025.jpg',\n",
       " 'Text_ocr_026.jpg',\n",
       " 'Text_ocr_027.jpg',\n",
       " 'Text_ocr_028.jpg',\n",
       " 'Text_ocr_029.jpg',\n",
       " 'Text_ocr_030.jpg',\n",
       " 'Text_ocr_031.jpg',\n",
       " 'Text_ocr_032.jpg',\n",
       " 'Text_ocr_033.jpg',\n",
       " 'Text_ocr_034.jpg',\n",
       " 'Text_ocr_035.jpg',\n",
       " 'Text_ocr_036.jpg',\n",
       " 'Text_ocr_037.jpg',\n",
       " 'Text_ocr_038.jpg',\n",
       " 'Text_ocr_039.jpg',\n",
       " 'Text_ocr_040.jpg',\n",
       " 'Text_ocr_041.jpg',\n",
       " 'Text_ocr_042.jpg',\n",
       " 'Text_ocr_043.jpg',\n",
       " 'Text_ocr_044.jpg',\n",
       " 'Text_ocr_045.jpg',\n",
       " 'Text_ocr_046.jpg',\n",
       " 'Text_ocr_047.jpg',\n",
       " 'Text_ocr_048.jpg',\n",
       " 'Text_ocr_049.jpg',\n",
       " 'Text_ocr_050.jpg',\n",
       " 'Text_ocr_051.jpg',\n",
       " 'Text_ocr_052.jpg',\n",
       " 'Text_ocr_053.jpg',\n",
       " 'Text_ocr_054.jpg',\n",
       " 'Text_ocr_055.jpg',\n",
       " 'Text_ocr_056.jpg',\n",
       " 'Text_ocr_057.jpg',\n",
       " 'Text_ocr_058.jpg',\n",
       " 'Text_ocr_059.jpg',\n",
       " 'Text_ocr_060.jpg',\n",
       " 'Text_ocr_061.jpg',\n",
       " 'Text_ocr_062.jpg',\n",
       " 'Text_ocr_063.jpg',\n",
       " 'Text_ocr_064.jpg',\n",
       " 'Text_ocr_065.jpg',\n",
       " 'Text_ocr_066.jpg',\n",
       " 'Text_ocr_067.jpg',\n",
       " 'Text_ocr_068.jpg',\n",
       " 'Text_ocr_069.jpg',\n",
       " 'Text_ocr_070.jpg',\n",
       " 'Text_ocr_071.jpg',\n",
       " 'Text_ocr_072.jpg',\n",
       " 'Text_ocr_073.jpg',\n",
       " 'Text_ocr_074.jpg',\n",
       " 'Text_ocr_075.jpg',\n",
       " 'Text_ocr_076.jpg',\n",
       " 'Text_ocr_077.jpg',\n",
       " 'Text_ocr_078.jpg',\n",
       " 'Text_ocr_079.jpg',\n",
       " 'Text_ocr_080.jpg',\n",
       " 'Text_ocr_081.jpg',\n",
       " 'Text_ocr_082.jpg',\n",
       " 'Text_ocr_083.jpg',\n",
       " 'Text_ocr_084.jpg',\n",
       " 'Text_ocr_085.jpg',\n",
       " 'Text_ocr_086.jpg',\n",
       " 'Text_ocr_087.jpg',\n",
       " 'Text_ocr_088.jpg',\n",
       " 'Text_ocr_089.jpg',\n",
       " 'Text_ocr_090.jpg',\n",
       " 'Text_ocr_091.jpg',\n",
       " 'Text_ocr_092.jpg',\n",
       " 'Text_ocr_093.jpg',\n",
       " 'Text_ocr_094.jpg',\n",
       " 'Text_ocr_095.jpg',\n",
       " 'Text_ocr_096.jpg',\n",
       " 'Text_ocr_097.jpg',\n",
       " 'Text_ocr_098.jpg',\n",
       " 'Text_ocr_099.jpg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-10T00:05:45.093901Z",
     "start_time": "2025-07-10T00:05:14.024068Z"
    }
   },
   "source": [
    "# Cell 1: Configuration and Setup\n",
    "import os\n",
    "import shutil\n",
    "import sqlite3\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Configuration Settings\n",
    "CONFIG = {\n",
    "    # OCR Model Selection\n",
    "    'OCR_MODEL': 'paddleocr',  # 'paddleocr', 'easyocr', 'tesseract'\n",
    "    'OCR_LANGUAGES': ['en'],   # Languages for OCR\n",
    "\n",
    "    # Search and Embeddings\n",
    "    'EMBEDDING_MODEL': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    'DEVICE': 'auto',  # 'auto', 'cpu', 'cuda'\n",
    "\n",
    "    # Paths\n",
    "    'SUSPECTS_GALLERY_PATH': '../../datasets/images/text_ocr',     # Input folder with suspect images\n",
    "    'DATABASE_PATH': 'forensic_analysis_db',       # Database folder\n",
    "    'RESULTS_OUTPUT_PATH': '../../datasets/images/forensic_results',     # Output folder for matched images\n",
    "\n",
    "    # OCR Processing parameters\n",
    "    'OCR_CONFIDENCE_THRESHOLD': 0.6,  # Minimum OCR confidence\n",
    "    'TEXT_MIN_LENGTH': 3,              # Minimum text length to consider\n",
    "    'BATCH_SIZE': 4,                   # Batch size for processing\n",
    "\n",
    "    # Search parameters\n",
    "    'SIMILARITY_THRESHOLD': 0.3,       # Semantic similarity threshold\n",
    "    'MAX_RESULTS_DISPLAY': 10,         # Maximum results to display\n",
    "    'TOP_K_RESULTS': 20,               # Top K results for queries\n",
    "\n",
    "    # Processing settings\n",
    "    'SAVE_OCR_METADATA': True,         # Save detailed OCR metadata\n",
    "    'FIGURE_SIZE': (15, 10),           # Size of result visualization\n",
    "}\n",
    "\n",
    "# Available OCR models\n",
    "AVAILABLE_OCR_MODELS = {\n",
    "    'paddleocr': {\n",
    "        'name': 'PaddleOCR',\n",
    "        'description': 'High accuracy, supports 80+ languages',\n",
    "        'performance': 'Best overall performance',\n",
    "        'install_cmd': 'pip install paddlepaddle paddleocr'\n",
    "    },\n",
    "    'easyocr': {\n",
    "        'name': 'EasyOCR',\n",
    "        'description': 'Simple to use, good accuracy',\n",
    "        'performance': 'Good performance, easy setup',\n",
    "        'install_cmd': 'pip install easyocr'\n",
    "    },\n",
    "    'tesseract': {\n",
    "        'name': 'Tesseract',\n",
    "        'description': 'Classic OCR, widely supported',\n",
    "        'performance': 'Baseline performance',\n",
    "        'install_cmd': 'pip install pytesseract'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully\")\n",
    "print(f\"üìÅ Suspects gallery: {CONFIG['SUSPECTS_GALLERY_PATH']}\")\n",
    "print(f\"üìÅ Database path: {CONFIG['DATABASE_PATH']}\")\n",
    "print(f\"üìÅ Results output: {CONFIG['RESULTS_OUTPUT_PATH']}\")\n",
    "print(f\"üîç OCR model: {CONFIG['OCR_MODEL']}\")\n",
    "print(\"üìù Forensic OCR + Search System Ready\")\n",
    "\n",
    "# Cell 2: Install and Import Dependencies\n",
    "def install_dependencies():\n",
    "    \"\"\"Install required packages based on selected OCR model\"\"\"\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    try:\n",
    "        # Check core dependencies\n",
    "        import numpy\n",
    "        import PIL\n",
    "        import cv2\n",
    "        print(\"‚úÖ Core dependencies available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è Installing core dependencies...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy\", \"Pillow\", \"opencv-python\"])\n",
    "\n",
    "    # Install OCR model\n",
    "    ocr_model = CONFIG['OCR_MODEL']\n",
    "    try:\n",
    "        if ocr_model == 'paddleocr':\n",
    "            import paddleocr\n",
    "            print(\"‚úÖ PaddleOCR already available\")\n",
    "        elif ocr_model == 'easyocr':\n",
    "            import easyocr\n",
    "            print(\"‚úÖ EasyOCR already available\")\n",
    "        elif ocr_model == 'tesseract':\n",
    "            import pytesseract\n",
    "            print(\"‚úÖ Tesseract already available\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ö†Ô∏è Installing {ocr_model}...\")\n",
    "        install_cmd = AVAILABLE_OCR_MODELS[ocr_model]['install_cmd']\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + install_cmd.split()[2:])\n",
    "\n",
    "    # Install search dependencies\n",
    "    try:\n",
    "        import chromadb\n",
    "        import sentence_transformers\n",
    "        print(\"‚úÖ Search dependencies available\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è Installing search dependencies...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"chromadb\", \"sentence-transformers\"])\n",
    "\n",
    "# Run installation\n",
    "install_dependencies()\n",
    "\n",
    "# Import required libraries\n",
    "try:\n",
    "    import chromadb\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    # Import OCR model\n",
    "    if CONFIG['OCR_MODEL'] == 'paddleocr':\n",
    "        from paddleocr import PaddleOCR\n",
    "    elif CONFIG['OCR_MODEL'] == 'easyocr':\n",
    "        import easyocr\n",
    "    elif CONFIG['OCR_MODEL'] == 'tesseract':\n",
    "        import pytesseract\n",
    "\n",
    "    print(\"‚úÖ All dependencies imported successfully\")\n",
    "\n",
    "    # Check device compatibility\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üöÄ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        default_device = \"cuda\"\n",
    "    else:\n",
    "        print(\"üñ•Ô∏è Using CPU mode\")\n",
    "        default_device = \"cpu\"\n",
    "\n",
    "    if CONFIG['DEVICE'] == 'auto':\n",
    "        CONFIG['DEVICE'] = default_device\n",
    "        print(f\"üìç Auto-detected device: {CONFIG['DEVICE']}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"üîß Please run the installation cell and restart kernel\")\n",
    "\n",
    "# Cell 3: Initialize OCR and Search Models\n",
    "class ForensicOCRSearchSystem:\n",
    "    \"\"\"Comprehensive forensic OCR and search system\"\"\"\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        \"\"\"Initialize the forensic analysis system\"\"\"\n",
    "        self.config = config or CONFIG\n",
    "        self.setup_directories()\n",
    "        self.init_ocr_model()\n",
    "        self.init_search_components()\n",
    "        self.init_database()\n",
    "\n",
    "    def setup_directories(self):\n",
    "        \"\"\"Create necessary directories\"\"\"\n",
    "        for path_key in ['SUSPECTS_GALLERY_PATH', 'DATABASE_PATH', 'RESULTS_OUTPUT_PATH']:\n",
    "            path = self.config[path_key]\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            print(f\"üìÅ Directory ready: {path}\")\n",
    "\n",
    "    def init_ocr_model(self):\n",
    "        \"\"\"Initialize OCR model based on configuration\"\"\"\n",
    "        ocr_model = self.config['OCR_MODEL']\n",
    "        languages = self.config['OCR_LANGUAGES']\n",
    "\n",
    "        print(f\"üì• Loading OCR model: {ocr_model}\")\n",
    "\n",
    "        try:\n",
    "            if ocr_model == 'paddleocr':\n",
    "                self.ocr = PaddleOCR(\n",
    "                    use_textline_orientation=True,\n",
    "                    lang=languages[0] if languages else 'en',\n",
    "                    # use_gpu=True if self.config['DEVICE'] == 'cuda' else False\n",
    "                )\n",
    "                print(\"‚úÖ PaddleOCR initialized\")\n",
    "\n",
    "            elif ocr_model == 'easyocr':\n",
    "                self.ocr = easyocr.Reader(\n",
    "                    languages,\n",
    "                    gpu=True if self.config['DEVICE'] == 'cuda' else False\n",
    "                )\n",
    "                print(\"‚úÖ EasyOCR initialized\")\n",
    "\n",
    "            elif ocr_model == 'tesseract':\n",
    "                # Tesseract doesn't need initialization, just check availability\n",
    "                try:\n",
    "                    import pytesseract\n",
    "                    self.ocr = pytesseract\n",
    "                    print(\"‚úÖ Tesseract initialized\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Tesseract error: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå OCR initialization error: {e}\")\n",
    "            self.ocr = None\n",
    "\n",
    "    def init_search_components(self):\n",
    "        \"\"\"Initialize search and embedding components\"\"\"\n",
    "        try:\n",
    "            print(f\"üì• Loading embedding model: {self.config['EMBEDDING_MODEL']}\")\n",
    "            self.embedding_model = SentenceTransformer(self.config['EMBEDDING_MODEL'])\n",
    "\n",
    "            print(\"üì• Initializing vector database...\")\n",
    "            self.chroma_client = chromadb.PersistentClient(path=self.config['DATABASE_PATH'])\n",
    "            self.collection = self.chroma_client.get_or_create_collection(\n",
    "                name=\"forensic_text_search\",\n",
    "                metadata={\"hnsw:space\": \"cosine\"}\n",
    "            )\n",
    "            print(\"‚úÖ Search components initialized\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Search initialization error: {e}\")\n",
    "            self.embedding_model = None\n",
    "            self.chroma_client = None\n",
    "            self.collection = None\n",
    "\n",
    "    def init_database(self):\n",
    "        \"\"\"Initialize SQLite database for metadata\"\"\"\n",
    "        try:\n",
    "            db_path = os.path.join(self.config['DATABASE_PATH'], 'forensic_metadata.db')\n",
    "            self.conn = sqlite3.connect(db_path)\n",
    "\n",
    "            # Create tables\n",
    "            self.conn.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS forensic_images (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    filename TEXT UNIQUE NOT NULL,\n",
    "                    full_path TEXT NOT NULL,\n",
    "                    extracted_text TEXT,\n",
    "                    ocr_confidence REAL,\n",
    "                    text_blocks_count INTEGER,\n",
    "                    processed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    file_size INTEGER,\n",
    "                    image_dimensions TEXT,\n",
    "                    ocr_metadata TEXT\n",
    "                )\n",
    "            ''')\n",
    "\n",
    "            self.conn.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS search_queries (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    query_text TEXT NOT NULL,\n",
    "                    query_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    results_count INTEGER,\n",
    "                    processing_time REAL\n",
    "                )\n",
    "            ''')\n",
    "\n",
    "            self.conn.commit()\n",
    "            print(\"‚úÖ Database initialized\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Database initialization error: {e}\")\n",
    "            self.conn = None\n",
    "\n",
    "    def extract_text_from_image(self, image_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract text from image using configured OCR model\"\"\"\n",
    "        try:\n",
    "            image_path = str(image_path)\n",
    "            filename = os.path.basename(image_path)\n",
    "\n",
    "            # Load image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image_array = np.array(image)\n",
    "\n",
    "            result = {\n",
    "                'filename': filename,\n",
    "                'full_path': image_path,\n",
    "                'extracted_text': '',\n",
    "                'ocr_confidence': 0.0,\n",
    "                'text_blocks': [],\n",
    "                'text_blocks_count': 0,\n",
    "                'error': None\n",
    "            }\n",
    "\n",
    "            # Process with selected OCR model\n",
    "            if self.config['OCR_MODEL'] == 'paddleocr' and self.ocr:\n",
    "                ocr_result = self.ocr.ocr(image_array, cls=True)\n",
    "\n",
    "                if ocr_result and ocr_result[0]:\n",
    "                    text_blocks = []\n",
    "                    all_text = []\n",
    "                    confidences = []\n",
    "\n",
    "                    for line in ocr_result:\n",
    "                        for detection in line:\n",
    "                            bbox, (text, confidence) = detection\n",
    "\n",
    "                            if confidence >= self.config['OCR_CONFIDENCE_THRESHOLD']:\n",
    "                                text_blocks.append({\n",
    "                                    'text': text,\n",
    "                                    'confidence': confidence,\n",
    "                                    'bbox': bbox\n",
    "                                })\n",
    "                                all_text.append(text)\n",
    "                                confidences.append(confidence)\n",
    "\n",
    "                    result.update({\n",
    "                        'extracted_text': ' '.join(all_text),\n",
    "                        'ocr_confidence': np.mean(confidences) if confidences else 0.0,\n",
    "                        'text_blocks': text_blocks,\n",
    "                        'text_blocks_count': len(text_blocks)\n",
    "                    })\n",
    "\n",
    "            elif self.config['OCR_MODEL'] == 'easyocr' and self.ocr:\n",
    "                ocr_result = self.ocr.readtext(image_array)\n",
    "\n",
    "                text_blocks = []\n",
    "                all_text = []\n",
    "                confidences = []\n",
    "\n",
    "                for detection in ocr_result:\n",
    "                    bbox, text, confidence = detection\n",
    "\n",
    "                    if confidence >= self.config['OCR_CONFIDENCE_THRESHOLD']:\n",
    "                        text_blocks.append({\n",
    "                            'text': text,\n",
    "                            'confidence': confidence,\n",
    "                            'bbox': bbox\n",
    "                        })\n",
    "                        all_text.append(text)\n",
    "                        confidences.append(confidence)\n",
    "\n",
    "                result.update({\n",
    "                    'extracted_text': ' '.join(all_text),\n",
    "                    'ocr_confidence': np.mean(confidences) if confidences else 0.0,\n",
    "                    'text_blocks': text_blocks,\n",
    "                    'text_blocks_count': len(text_blocks)\n",
    "                })\n",
    "\n",
    "            elif self.config['OCR_MODEL'] == 'tesseract' and self.ocr:\n",
    "                # Simple Tesseract extraction\n",
    "                text = self.ocr.image_to_string(image, config='--psm 6')\n",
    "                confidence = 0.8  # Tesseract doesn't provide confidence easily\n",
    "\n",
    "                if text.strip():\n",
    "                    result.update({\n",
    "                        'extracted_text': text.strip(),\n",
    "                        'ocr_confidence': confidence,\n",
    "                        'text_blocks': [{'text': text.strip(), 'confidence': confidence}],\n",
    "                        'text_blocks_count': 1\n",
    "                    })\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'filename': os.path.basename(image_path),\n",
    "                'full_path': image_path,\n",
    "                'extracted_text': '',\n",
    "                'ocr_confidence': 0.0,\n",
    "                'text_blocks': [],\n",
    "                'text_blocks_count': 0,\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "    def process_image_gallery(self, image_extensions=['.jpg', '.jpeg', '.png', '.bmp', '.tiff']):\n",
    "        \"\"\"Process all images in the suspect gallery\"\"\"\n",
    "        start_time = time.time()\n",
    "        gallery_path = Path(self.config['SUSPECTS_GALLERY_PATH'])\n",
    "\n",
    "        print(f\"üîç Processing images from: {gallery_path}\")\n",
    "        print(f\"üîç OCR Model: {self.config['OCR_MODEL']}\")\n",
    "        print(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "        if not gallery_path.exists():\n",
    "            print(f\"‚ùå Gallery path {gallery_path} does not exist!\")\n",
    "            return\n",
    "\n",
    "        # Get all image files\n",
    "        image_files = []\n",
    "        for ext in image_extensions:\n",
    "            image_files.extend(gallery_path.glob(f\"**/*{ext}\"))\n",
    "            image_files.extend(gallery_path.glob(f\"**/*{ext.upper()}\"))\n",
    "\n",
    "        total_files = len(image_files)\n",
    "        print(f\"üì∏ Found {total_files} images to process\")\n",
    "\n",
    "        if total_files == 0:\n",
    "            print(\"‚ö†Ô∏è No images found!\")\n",
    "            return\n",
    "\n",
    "        processed_count = 0\n",
    "        text_found_count = 0\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üîç PROCESSING IMAGES\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        for i, image_path in enumerate(image_files, 1):\n",
    "            filename = image_path.name\n",
    "\n",
    "            # Progress indicator\n",
    "            progress = (i / total_files) * 100\n",
    "            print(f\"[{i:3d}/{total_files}] ({progress:5.1f}%) Processing: {filename[:40]}...\", end='')\n",
    "\n",
    "            # Check if already processed\n",
    "            cursor = self.conn.execute(\"SELECT filename FROM forensic_images WHERE filename = ?\", (filename,))\n",
    "            if cursor.fetchone():\n",
    "                print(\" ‚è≠Ô∏è SKIP (already processed)\")\n",
    "                continue\n",
    "\n",
    "            # Extract text\n",
    "            ocr_result = self.extract_text_from_image(image_path)\n",
    "\n",
    "            if ocr_result.get('error'):\n",
    "                print(f\" ‚ùå ERROR: {ocr_result['error'][:30]}\")\n",
    "                continue\n",
    "\n",
    "            # Store in database\n",
    "            file_size = image_path.stat().st_size if image_path.exists() else 0\n",
    "            image_dims = f\"{ocr_result.get('width', 0)}x{ocr_result.get('height', 0)}\"\n",
    "\n",
    "            self.conn.execute('''\n",
    "                INSERT OR REPLACE INTO forensic_images\n",
    "                (filename, full_path, extracted_text, ocr_confidence, text_blocks_count,\n",
    "                 file_size, image_dimensions, ocr_metadata)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                filename,\n",
    "                str(image_path),\n",
    "                ocr_result['extracted_text'],\n",
    "                ocr_result['ocr_confidence'],\n",
    "                ocr_result['text_blocks_count'],\n",
    "                file_size,\n",
    "                image_dims,\n",
    "                json.dumps(ocr_result['text_blocks']) if self.config['SAVE_OCR_METADATA'] else None\n",
    "            ))\n",
    "\n",
    "            # Add to vector database if text found\n",
    "            if ocr_result['extracted_text'].strip():\n",
    "                try:\n",
    "                    doc_id = f\"img_{processed_count}_{filename}\"\n",
    "                    self.collection.add(\n",
    "                        documents=[ocr_result['extracted_text']],\n",
    "                        metadatas=[{\n",
    "                            'filename': filename,\n",
    "                            'full_path': str(image_path),\n",
    "                            'ocr_confidence': ocr_result['ocr_confidence'],\n",
    "                            'text_blocks_count': ocr_result['text_blocks_count']\n",
    "                        }],\n",
    "                        ids=[doc_id]\n",
    "                    )\n",
    "                    text_found_count += 1\n",
    "                    print(f\" ‚úÖ TEXT ({ocr_result['text_blocks_count']} blocks, conf: {ocr_result['ocr_confidence']:.2f})\")\n",
    "                except Exception as e:\n",
    "                    print(f\" ‚ö†Ô∏è DB ERROR: {str(e)[:20]}\")\n",
    "            else:\n",
    "                print(\" ‚ö™ NO TEXT\")\n",
    "\n",
    "            processed_count += 1\n",
    "\n",
    "            # Commit every 10 images\n",
    "            if i % 10 == 0:\n",
    "                self.conn.commit()\n",
    "\n",
    "        # Final commit\n",
    "        self.conn.commit()\n",
    "\n",
    "        # Calculate timing\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìä PROCESSING SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"üì∏ Total images: {total_files}\")\n",
    "        print(f\"‚úÖ Successfully processed: {processed_count}\")\n",
    "        print(f\"üìù Images with text: {text_found_count}\")\n",
    "        print(f\"‚ö™ Images without text: {processed_count - text_found_count}\")\n",
    "        print(f\"‚è±Ô∏è Total time: {duration:.1f}s ({duration/total_files:.2f}s per image)\")\n",
    "        print(f\"üìä Text detection rate: {(text_found_count/processed_count*100):.1f}%\" if processed_count > 0 else \"\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "    def search_by_text_query(self, query: str, max_results: int = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search images by semantic text similarity\"\"\"\n",
    "        if max_results is None:\n",
    "            max_results = self.config['TOP_K_RESULTS']\n",
    "\n",
    "        if not self.collection or not self.embedding_model:\n",
    "            print(\"‚ùå Search components not available\")\n",
    "            return []\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Perform semantic search\n",
    "            results = self.collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=max_results\n",
    "            )\n",
    "\n",
    "            # Format results\n",
    "            matches = []\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                for i, doc in enumerate(results['documents'][0]):\n",
    "                    metadata = results['metadatas'][0][i]\n",
    "                    distance = results['distances'][0][i] if 'distances' in results else 0\n",
    "                    similarity = 1 - distance\n",
    "\n",
    "                    # Filter by similarity threshold\n",
    "                    if similarity >= self.config['SIMILARITY_THRESHOLD']:\n",
    "                        matches.append({\n",
    "                            'filename': metadata['filename'],\n",
    "                            'full_path': metadata['full_path'],\n",
    "                            'extracted_text': doc,\n",
    "                            'similarity_score': similarity,\n",
    "                            'ocr_confidence': metadata.get('ocr_confidence', 0),\n",
    "                            'text_blocks_count': metadata.get('text_blocks_count', 0),\n",
    "                            'text_preview': doc[:200] + \"...\" if len(doc) > 200 else doc\n",
    "                        })\n",
    "\n",
    "            # Log query\n",
    "            processing_time = time.time() - start_time\n",
    "            self.conn.execute('''\n",
    "                INSERT INTO search_queries (query_text, results_count, processing_time)\n",
    "                VALUES (?, ?, ?)\n",
    "            ''', (query, len(matches), processing_time))\n",
    "            self.conn.commit()\n",
    "\n",
    "            return matches\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Search error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def search_by_keywords(self, keywords: List[str], exact_match: bool = False) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search images by specific keywords using SQL\"\"\"\n",
    "        matches = []\n",
    "\n",
    "        try:\n",
    "            for keyword in keywords:\n",
    "                if exact_match:\n",
    "                    sql_query = \"\"\"\n",
    "                        SELECT filename, full_path, extracted_text, ocr_confidence, text_blocks_count\n",
    "                        FROM forensic_images\n",
    "                        WHERE extracted_text LIKE ? AND extracted_text != ''\n",
    "                    \"\"\"\n",
    "                    params = (f\"%{keyword}%\",)\n",
    "                else:\n",
    "                    sql_query = \"\"\"\n",
    "                        SELECT filename, full_path, extracted_text, ocr_confidence, text_blocks_count\n",
    "                        FROM forensic_images\n",
    "                        WHERE LOWER(extracted_text) LIKE LOWER(?) AND extracted_text != ''\n",
    "                    \"\"\"\n",
    "                    params = (f\"%{keyword}%\",)\n",
    "\n",
    "                cursor = self.conn.execute(sql_query, params)\n",
    "                results = cursor.fetchall()\n",
    "\n",
    "                for row in results:\n",
    "                    match = {\n",
    "                        'filename': row[0],\n",
    "                        'full_path': row[1],\n",
    "                        'extracted_text': row[2],\n",
    "                        'ocr_confidence': row[3],\n",
    "                        'text_blocks_count': row[4],\n",
    "                        'matched_keyword': keyword,\n",
    "                        'search_type': 'keyword',\n",
    "                        'text_preview': row[2][:200] + \"...\" if len(row[2]) > 200 else row[2]\n",
    "                    }\n",
    "\n",
    "                    # Avoid duplicates\n",
    "                    if not any(m['filename'] == match['filename'] for m in matches):\n",
    "                        matches.append(match)\n",
    "\n",
    "            return matches\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Keyword search error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def forensic_query_analysis(self, query: str, include_keywords: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive forensic analysis for a query\"\"\"\n",
    "        print(f\"üîç Forensic Analysis: '{query}'\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Semantic search\n",
    "        print(\"üìù Performing semantic search...\")\n",
    "        semantic_results = self.search_by_text_query(query)\n",
    "\n",
    "        # Keyword search\n",
    "        keyword_results = []\n",
    "        if include_keywords:\n",
    "            print(\"üîç Performing keyword search...\")\n",
    "            query_words = query.lower().split()\n",
    "            keyword_results = self.search_by_keywords(query_words)\n",
    "\n",
    "        # Combine and deduplicate results\n",
    "        all_results = semantic_results.copy()\n",
    "        for kw_result in keyword_results:\n",
    "            if not any(r['filename'] == kw_result['filename'] for r in all_results):\n",
    "                kw_result['search_type'] = 'keyword'\n",
    "                all_results.append(kw_result)\n",
    "\n",
    "        # Sort by relevance (similarity score or confidence)\n",
    "        all_results.sort(\n",
    "            key=lambda x: x.get('similarity_score', x.get('ocr_confidence', 0)),\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        processing_time = time.time() - start_time\n",
    "\n",
    "        analysis = {\n",
    "            'query': query,\n",
    "            'semantic_matches': len(semantic_results),\n",
    "            'keyword_matches': len(keyword_results),\n",
    "            'total_unique_matches': len(all_results),\n",
    "            'processing_time': processing_time,\n",
    "            'results': all_results[:self.config['MAX_RESULTS_DISPLAY']],\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        print(f\"‚úÖ Analysis complete:\")\n",
    "        print(f\"   üìä Semantic matches: {len(semantic_results)}\")\n",
    "        print(f\"   üîç Keyword matches: {len(keyword_results)}\")\n",
    "        print(f\"   üìù Total unique: {len(all_results)}\")\n",
    "        print(f\"   ‚è±Ô∏è Time: {processing_time:.2f}s\")\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive database statistics\"\"\"\n",
    "        try:\n",
    "            stats = {}\n",
    "\n",
    "            # Image statistics\n",
    "            cursor = self.conn.execute(\"SELECT COUNT(*) FROM forensic_images\")\n",
    "            stats['total_images'] = cursor.fetchone()[0]\n",
    "\n",
    "            cursor = self.conn.execute(\"SELECT COUNT(*) FROM forensic_images WHERE extracted_text != ''\")\n",
    "            stats['images_with_text'] = cursor.fetchone()[0]\n",
    "\n",
    "            cursor = self.conn.execute(\"SELECT AVG(ocr_confidence) FROM forensic_images WHERE extracted_text != ''\")\n",
    "            result = cursor.fetchone()[0]\n",
    "            stats['avg_ocr_confidence'] = round(result, 3) if result else 0\n",
    "\n",
    "            cursor = self.conn.execute(\"SELECT SUM(text_blocks_count) FROM forensic_images\")\n",
    "            result = cursor.fetchone()[0]\n",
    "            stats['total_text_blocks'] = result if result else 0\n",
    "\n",
    "            # Query statistics\n",
    "            cursor = self.conn.execute(\"SELECT COUNT(*) FROM search_queries\")\n",
    "            stats['total_queries'] = cursor.fetchone()[0]\n",
    "\n",
    "            cursor = self.conn.execute(\"SELECT AVG(processing_time) FROM search_queries\")\n",
    "            result = cursor.fetchone()[0]\n",
    "            stats['avg_query_time'] = round(result, 3) if result else 0\n",
    "\n",
    "            # File size statistics\n",
    "            cursor = self.conn.execute(\"SELECT SUM(file_size), AVG(file_size) FROM forensic_images\")\n",
    "            total_size, avg_size = cursor.fetchone()\n",
    "            stats['total_file_size_mb'] = round(total_size / (1024*1024), 2) if total_size else 0\n",
    "            stats['avg_file_size_kb'] = round(avg_size / 1024, 2) if avg_size else 0\n",
    "\n",
    "            return stats\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Statistics error: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def export_results(self, results: List[Dict], output_folder: str, copy_images: bool = True) -> str:\n",
    "        \"\"\"Export search results to folder with metadata\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = Path(output_folder) / f\"forensic_search_{timestamp}\"\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        copied_files = []\n",
    "        metadata = {\n",
    "            'export_timestamp': timestamp,\n",
    "            'total_results': len(results),\n",
    "            'export_path': str(output_path),\n",
    "            'results': []\n",
    "        }\n",
    "\n",
    "        for i, result in enumerate(results, 1):\n",
    "            try:\n",
    "                result_meta = {\n",
    "                    'index': i,\n",
    "                    'filename': result['filename'],\n",
    "                    'original_path': result['full_path'],\n",
    "                    'extracted_text': result['extracted_text'],\n",
    "                    'text_preview': result.get('text_preview', ''),\n",
    "                    'ocr_confidence': result.get('ocr_confidence', 0),\n",
    "                    'similarity_score': result.get('similarity_score', 0),\n",
    "                    'search_type': result.get('search_type', 'semantic')\n",
    "                }\n",
    "\n",
    "                if copy_images:\n",
    "                    # Copy image file\n",
    "                    source_path = Path(result['full_path'])\n",
    "                    if source_path.exists():\n",
    "                        # Create descriptive filename\n",
    "                        conf_or_sim = result.get('similarity_score', result.get('ocr_confidence', 0))\n",
    "                        new_filename = f\"{i:03d}_{source_path.stem}_score{conf_or_sim:.2f}{source_path.suffix}\"\n",
    "                        dest_path = output_path / new_filename\n",
    "\n",
    "                        shutil.copy2(source_path, dest_path)\n",
    "                        copied_files.append(str(dest_path))\n",
    "                        result_meta['exported_filename'] = new_filename\n",
    "                        result_meta['exported_path'] = str(dest_path)\n",
    "\n",
    "                metadata['results'].append(result_meta)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error exporting {result['filename']}: {e}\")\n",
    "\n",
    "        # Save metadata JSON\n",
    "        metadata_file = output_path / \"search_metadata.json\"\n",
    "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        # Save text summary\n",
    "        summary_file = output_path / \"search_summary.txt\"\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Forensic Search Results\\n\")\n",
    "            f.write(f\"=\"*50 + \"\\n\")\n",
    "            f.write(f\"Export Date: {timestamp}\\n\")\n",
    "            f.write(f\"Total Results: {len(results)}\\n\")\n",
    "            f.write(f\"Files Copied: {len(copied_files)}\\n\\n\")\n",
    "\n",
    "            for i, result in enumerate(results, 1):\n",
    "                f.write(f\"{i}. {result['filename']}\\n\")\n",
    "                f.write(f\"   Text: {result.get('text_preview', '')}\\n\")\n",
    "                f.write(f\"   Score: {result.get('similarity_score', result.get('ocr_confidence', 0)):.3f}\\n\\n\")\n",
    "\n",
    "        print(f\"üìÅ Exported {len(results)} results to: {output_path}\")\n",
    "        print(f\"üìã Images copied: {len(copied_files)}\")\n",
    "        print(f\"üìÑ Metadata saved: {metadata_file}\")\n",
    "\n",
    "        return str(output_path)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close database connections and cleanup\"\"\"\n",
    "        if hasattr(self, 'conn') and self.conn:\n",
    "            self.conn.close()\n",
    "            print(\"üìä Database connections closed\")\n",
    "\n",
    "# Cell 4: Batch Processing and Forensic Analysis Functions\n",
    "def run_comprehensive_forensic_analysis(system, custom_queries=None):\n",
    "    \"\"\"Run comprehensive forensic analysis with predefined and custom queries\"\"\"\n",
    "\n",
    "    # Default forensic text queries\n",
    "    default_queries = [\n",
    "        \"bank account number\",\n",
    "        \"credit card\",\n",
    "        \"social security number\",\n",
    "        \"phone number\",\n",
    "        \"email address\",\n",
    "        \"password\",\n",
    "        \"address location\",\n",
    "        \"threat violence\",\n",
    "        \"weapon gun knife\",\n",
    "        \"drugs illegal substances\",\n",
    "        \"money cash payment\",\n",
    "        \"meeting appointment\",\n",
    "        \"suspicious activity\",\n",
    "        \"personal identification\",\n",
    "        \"financial transaction\"\n",
    "    ]\n",
    "\n",
    "    # Combine with custom queries\n",
    "    queries = default_queries + (custom_queries or [])\n",
    "\n",
    "    print(f\"üöÄ Starting comprehensive forensic analysis\")\n",
    "    print(f\"üìã Total queries: {len(queries)} ({len(default_queries)} default + {len(custom_queries or [])} custom)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    all_results = {}\n",
    "    high_priority_findings = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\n[{i:2d}/{len(queries)}] Analyzing: '{query}'\")\n",
    "\n",
    "        # Perform analysis\n",
    "        analysis = system.forensic_query_analysis(query, include_keywords=True)\n",
    "        all_results[query] = analysis\n",
    "\n",
    "        # Identify high-priority findings\n",
    "        for result in analysis['results'][:3]:  # Top 3 per query\n",
    "            score = result.get('similarity_score', result.get('ocr_confidence', 0))\n",
    "            if score > 0.5:  # High confidence threshold\n",
    "                high_priority_findings.append({\n",
    "                    'query': query,\n",
    "                    'filename': result['filename'],\n",
    "                    'score': score,\n",
    "                    'text_preview': result['text_preview'],\n",
    "                    'search_type': result.get('search_type', 'semantic')\n",
    "                })\n",
    "\n",
    "        # Brief status\n",
    "        matches = analysis['total_unique_matches']\n",
    "        status = \"üî¥\" if matches > 5 else \"üü°\" if matches > 0 else \"‚ö™\"\n",
    "        print(f\"   {status} {matches} matches found\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    # Generate summary report\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä COMPREHENSIVE ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    total_matches = sum(r['total_unique_matches'] for r in all_results.values())\n",
    "    queries_with_results = sum(1 for r in all_results.values() if r['total_unique_matches'] > 0)\n",
    "\n",
    "    print(f\"üéØ Queries processed: {len(queries)}\")\n",
    "    print(f\"‚úÖ Queries with results: {queries_with_results}\")\n",
    "    print(f\"üìä Total matches found: {total_matches}\")\n",
    "    print(f\"üî¥ High priority findings: {len(high_priority_findings)}\")\n",
    "    print(f\"‚è±Ô∏è Total analysis time: {total_time:.1f}s\")\n",
    "    print(f\"üìà Average per query: {total_time/len(queries):.1f}s\")\n",
    "\n",
    "    # Show top findings by query\n",
    "    print(f\"\\nüìã TOP FINDINGS BY QUERY:\")\n",
    "    for query, analysis in all_results.items():\n",
    "        matches = analysis['total_unique_matches']\n",
    "        if matches > 0:\n",
    "            status = \"üî¥\" if matches > 5 else \"üü°\"\n",
    "            print(f\"{status} '{query}': {matches} matches\")\n",
    "\n",
    "    # Show high priority findings\n",
    "    if high_priority_findings:\n",
    "        print(f\"\\nüö® HIGH PRIORITY EVIDENCE:\")\n",
    "        high_priority_findings.sort(key=lambda x: x['score'], reverse=True)\n",
    "        for finding in high_priority_findings[:10]:  # Top 10\n",
    "            print(f\"üî¥ {finding['filename']} (Query: '{finding['query']}')\")\n",
    "            print(f\"   Score: {finding['score']:.3f} | Type: {finding['search_type']}\")\n",
    "            print(f\"   Text: {finding['text_preview'][:80]}...\")\n",
    "            print()\n",
    "\n",
    "    return {\n",
    "        'all_results': all_results,\n",
    "        'high_priority_findings': high_priority_findings,\n",
    "        'summary': {\n",
    "            'total_queries': len(queries),\n",
    "            'queries_with_results': queries_with_results,\n",
    "            'total_matches': total_matches,\n",
    "            'high_priority_count': len(high_priority_findings),\n",
    "            'processing_time': total_time\n",
    "        }\n",
    "    }\n",
    "\n",
    "def batch_export_all_results(system, analysis_results, output_base_path):\n",
    "    \"\"\"Export all analysis results in organized folders\"\"\"\n",
    "\n",
    "    print(\"üìÅ Exporting all forensic analysis results...\")\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_export_path = Path(output_base_path) / f\"forensic_analysis_{timestamp}\"\n",
    "    base_export_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Export high priority findings\n",
    "    high_priority = analysis_results['high_priority_findings']\n",
    "    if high_priority:\n",
    "        print(f\"üî¥ Exporting {len(high_priority)} high priority findings...\")\n",
    "        hp_results = []\n",
    "        for finding in high_priority:\n",
    "            # Convert finding to result format\n",
    "            result = {\n",
    "                'filename': finding['filename'],\n",
    "                'full_path': '',  # Will be filled from database\n",
    "                'extracted_text': '',\n",
    "                'text_preview': finding['text_preview'],\n",
    "                'similarity_score': finding['score'],\n",
    "                'search_type': finding['search_type']\n",
    "            }\n",
    "\n",
    "            # Get full details from database\n",
    "            cursor = system.conn.execute(\n",
    "                \"SELECT full_path, extracted_text FROM forensic_images WHERE filename = ?\",\n",
    "                (finding['filename'],)\n",
    "            )\n",
    "            row = cursor.fetchone()\n",
    "            if row:\n",
    "                result['full_path'] = row[0]\n",
    "                result['extracted_text'] = row[1]\n",
    "                hp_results.append(result)\n",
    "\n",
    "        if hp_results:\n",
    "            hp_folder = system.export_results(hp_results, base_export_path / \"high_priority\", copy_images=True)\n",
    "\n",
    "    # Export results by query category\n",
    "    categories = {\n",
    "        'financial': ['bank account', 'credit card', 'money', 'payment', 'transaction'],\n",
    "        'personal_info': ['phone number', 'email', 'address', 'social security', 'identification'],\n",
    "        'security': ['password', 'threat', 'weapon', 'suspicious'],\n",
    "        'illegal': ['drugs', 'illegal']\n",
    "    }\n",
    "\n",
    "    for category, keywords in categories.items():\n",
    "        category_results = []\n",
    "        for query, analysis in analysis_results['all_results'].items():\n",
    "            if any(keyword in query.lower() for keyword in keywords):\n",
    "                category_results.extend(analysis['results'])\n",
    "\n",
    "        if category_results:\n",
    "            # Remove duplicates\n",
    "            unique_results = []\n",
    "            seen_files = set()\n",
    "            for result in category_results:\n",
    "                if result['filename'] not in seen_files:\n",
    "                    unique_results.append(result)\n",
    "                    seen_files.add(result['filename'])\n",
    "\n",
    "            if unique_results:\n",
    "                print(f\"üìÇ Exporting {len(unique_results)} {category} findings...\")\n",
    "                system.export_results(unique_results, base_export_path / category, copy_images=True)\n",
    "\n",
    "    # Create master summary\n",
    "    summary_file = base_export_path / \"analysis_summary.json\"\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            'export_timestamp': timestamp,\n",
    "            'summary': analysis_results['summary'],\n",
    "            'high_priority_count': len(high_priority),\n",
    "            'categories_exported': list(categories.keys()),\n",
    "            'export_path': str(base_export_path)\n",
    "        }, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ All results exported to: {base_export_path}\")\n",
    "    return str(base_export_path)\n",
    "\n",
    "# Cell 5: Interactive Interface Functions\n",
    "def create_forensic_interface(system):\n",
    "    \"\"\"Create interactive interface for forensic analysts\"\"\"\n",
    "\n",
    "    def display_statistics():\n",
    "        \"\"\"Display system statistics\"\"\"\n",
    "        stats = system.get_statistics()\n",
    "        print(\"\\nüìä FORENSIC SYSTEM STATISTICS\")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"üì∏ Total images processed: {stats.get('total_images', 0)}\")\n",
    "        print(f\"üìù Images with text: {stats.get('images_with_text', 0)}\")\n",
    "        print(f\"üìä Text detection rate: {(stats.get('images_with_text', 0) / max(stats.get('total_images', 1), 1) * 100):.1f}%\")\n",
    "        print(f\"üéØ Average OCR confidence: {stats.get('avg_ocr_confidence', 0):.3f}\")\n",
    "        print(f\"üìã Total text blocks found: {stats.get('total_text_blocks', 0)}\")\n",
    "        print(f\"üîç Total queries executed: {stats.get('total_queries', 0)}\")\n",
    "        print(f\"‚è±Ô∏è Average query time: {stats.get('avg_query_time', 0):.3f}s\")\n",
    "        print(f\"üíæ Total data size: {stats.get('total_file_size_mb', 0)} MB\")\n",
    "        print(f\"üìè Average file size: {stats.get('avg_file_size_kb', 0)} KB\")\n",
    "        print(\"=\"*40)\n",
    "\n",
    "    def interactive_search():\n",
    "        \"\"\"Interactive search interface\"\"\"\n",
    "        print(\"\\nüîç INTERACTIVE FORENSIC SEARCH\")\n",
    "        print(\"=\"*40)\n",
    "        print(\"Enter 'help' for commands, 'quit' to exit\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nüîç Query> \").strip()\n",
    "\n",
    "                if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                    break\n",
    "                elif user_input.lower() == 'help':\n",
    "                    print(\"\\nüìã Available commands:\")\n",
    "                    print(\"  search <query>     - Semantic text search\")\n",
    "                    print(\"  keyword <words>    - Keyword search\")\n",
    "                    print(\"  stats             - Show system statistics\")\n",
    "                    print(\"  recent            - Show recent queries\")\n",
    "                    print(\"  export <query>    - Export last search results\")\n",
    "                    print(\"  help              - Show this help\")\n",
    "                    print(\"  quit              - Exit interface\")\n",
    "                    continue\n",
    "                elif user_input.lower() == 'stats':\n",
    "                    display_statistics()\n",
    "                    continue\n",
    "                elif user_input.lower() == 'recent':\n",
    "                    cursor = system.conn.execute(\n",
    "                        \"SELECT query_text, results_count, query_date FROM search_queries ORDER BY query_date DESC LIMIT 10\"\n",
    "                    )\n",
    "                    recent = cursor.fetchall()\n",
    "                    print(\"\\nüïê Recent queries:\")\n",
    "                    for query, count, date in recent:\n",
    "                        print(f\"  {date}: '{query}' ({count} results)\")\n",
    "                    continue\n",
    "                elif not user_input:\n",
    "                    continue\n",
    "\n",
    "                # Parse command\n",
    "                parts = user_input.split(' ', 1)\n",
    "                command = parts[0].lower()\n",
    "                query = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "                if command == 'search' and query:\n",
    "                    print(f\"\\nüîç Searching: '{query}'\")\n",
    "                    results = system.search_by_text_query(query)\n",
    "\n",
    "                    if results:\n",
    "                        print(f\"‚úÖ Found {len(results)} matches:\")\n",
    "                        for i, result in enumerate(results[:5], 1):  # Show top 5\n",
    "                            score = result.get('similarity_score', 0)\n",
    "                            print(f\"{i}. {result['filename']} (score: {score:.3f})\")\n",
    "                            print(f\"   {result['text_preview']}\")\n",
    "                            print()\n",
    "                    else:\n",
    "                        print(\"‚ö™ No matches found\")\n",
    "\n",
    "                elif command == 'keyword' and query:\n",
    "                    keywords = query.split()\n",
    "                    print(f\"\\nüîç Keyword search: {keywords}\")\n",
    "                    results = system.search_by_keywords(keywords)\n",
    "\n",
    "                    if results:\n",
    "                        print(f\"‚úÖ Found {len(results)} matches:\")\n",
    "                        for i, result in enumerate(results[:5], 1):\n",
    "                            keyword = result.get('matched_keyword', '')\n",
    "                            print(f\"{i}. {result['filename']} (keyword: '{keyword}')\")\n",
    "                            print(f\"   {result['text_preview']}\")\n",
    "                            print()\n",
    "                    else:\n",
    "                        print(\"‚ö™ No matches found\")\n",
    "\n",
    "                elif command == 'export' and query:\n",
    "                    # Search and export\n",
    "                    results = system.search_by_text_query(query)\n",
    "                    if results:\n",
    "                        output_path = system.export_results(results, system.config['RESULTS_OUTPUT_PATH'])\n",
    "                        print(f\"üìÅ Results exported to: {output_path}\")\n",
    "                    else:\n",
    "                        print(\"‚ö™ No results to export\")\n",
    "\n",
    "                else:\n",
    "                    print(\"‚ùå Unknown command. Type 'help' for available commands.\")\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nüëã Goodbye!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "    return interactive_search, display_statistics\n",
    "\n",
    "# Cell 6: Main Execution and Examples\n",
    "def main_forensic_analysis():\n",
    "    \"\"\"Main function to run the complete forensic analysis system\"\"\"\n",
    "\n",
    "    print(\"üöÄ INITIALIZING FORENSIC OCR + SEARCH SYSTEM\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Initialize system\n",
    "    system = ForensicOCRSearchSystem(CONFIG)\n",
    "\n",
    "    # Check if system is ready\n",
    "    if not system.ocr:\n",
    "        print(\"‚ùå OCR model not loaded. Please check installation.\")\n",
    "        return None\n",
    "\n",
    "    if not system.collection:\n",
    "        print(\"‚ùå Search components not loaded. Please check installation.\")\n",
    "        return None\n",
    "\n",
    "    print(\"‚úÖ System initialized successfully!\")\n",
    "\n",
    "    # Process images if needed\n",
    "    stats = system.get_statistics()\n",
    "    if stats.get('total_images', 0) == 0:\n",
    "        print(\"\\nüì∏ No processed images found. Processing gallery...\")\n",
    "        system.process_image_gallery()\n",
    "    else:\n",
    "        print(f\"\\nüìä Found {stats['total_images']} processed images, {stats['images_with_text']} with text\")\n",
    "\n",
    "    return system\n",
    "\n",
    "# Example usage functions\n",
    "def run_quick_demo(system):\n",
    "    \"\"\"Run a quick demonstration of the system\"\"\"\n",
    "    if not system:\n",
    "        print(\"‚ùå System not available\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nüéØ QUICK DEMONSTRATION\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    # Example queries\n",
    "    demo_queries = [\n",
    "        \"phone number\",\n",
    "        \"bank account\",\n",
    "        \"password\",\n",
    "        \"address\"\n",
    "    ]\n",
    "\n",
    "    for query in demo_queries:\n",
    "        print(f\"\\nüîç Demo query: '{query}'\")\n",
    "        results = system.search_by_text_query(query, max_results=3)\n",
    "\n",
    "        if results:\n",
    "            print(f\"‚úÖ Found {len(results)} matches\")\n",
    "            for result in results[:2]:  # Show top 2\n",
    "                print(f\"  üìÑ {result['filename']} (score: {result['similarity_score']:.3f})\")\n",
    "        else:\n",
    "            print(\"‚ö™ No matches\")\n",
    "\n",
    "def list_available_ocr_models():\n",
    "    \"\"\"Display available OCR models\"\"\"\n",
    "    print(\"\\nüîç AVAILABLE OCR MODELS\")\n",
    "    print(\"=\"*30)\n",
    "    for key, info in AVAILABLE_OCR_MODELS.items():\n",
    "        status = \"‚úÖ\" if key == CONFIG['OCR_MODEL'] else \"‚ö™\"\n",
    "        print(f\"{status} {info['name']} ({key})\")\n",
    "        print(f\"   üìä {info['performance']}\")\n",
    "        print(f\"   üìù {info['description']}\")\n",
    "        print(f\"   üíª Install: {info['install_cmd']}\")\n",
    "        print()\n",
    "\n",
    "# Display available models\n",
    "list_available_ocr_models()\n",
    "\n",
    "# Initialize system\n",
    "print(\"\\nüöÄ STARTING FORENSIC SYSTEM...\")\n",
    "forensic_system = main_forensic_analysis()\n",
    "\n",
    "if forensic_system:\n",
    "    # Create interactive interface\n",
    "    interactive_search, display_stats = create_forensic_interface(forensic_system)\n",
    "\n",
    "    print(f\"\\n‚úÖ FORENSIC OCR + SEARCH SYSTEM READY!\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"üìã AVAILABLE FUNCTIONS:\")\n",
    "    print(\"  ‚Ä¢ forensic_system.process_image_gallery() - Process new images\")\n",
    "    print(\"  ‚Ä¢ forensic_system.search_by_text_query(query) - Semantic search\")\n",
    "    print(\"  ‚Ä¢ forensic_system.search_by_keywords([keywords]) - Keyword search\")\n",
    "    print(\"  ‚Ä¢ forensic_system.forensic_query_analysis(query) - Full analysis\")\n",
    "    print(\"  ‚Ä¢ run_comprehensive_forensic_analysis(forensic_system) - Batch analysis\")\n",
    "    print(\"  ‚Ä¢ interactive_search() - Interactive search interface\")\n",
    "    print(\"  ‚Ä¢ display_stats() - Show system statistics\")\n",
    "    print(\"  ‚Ä¢ run_quick_demo(forensic_system) - Quick demonstration\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Show initial statistics\n",
    "    display_stats()\n",
    "\n",
    "    # Run quick demo\n",
    "    run_quick_demo(forensic_system)\n",
    "\n",
    "    print(f\"\\nüí° TIP: Use interactive_search() for hands-on searching!\")\n",
    "    print(f\"üí° TIP: Run run_comprehensive_forensic_analysis(forensic_system) for full analysis!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Failed to initialize forensic system\")\n",
    "\n",
    "# Cell 7: Usage Instructions and Documentation\n",
    "print(\"\"\"\n",
    "üéØ FORENSIC OCR + SEARCH SYSTEM - COMPLETE GUIDE\n",
    "===============================================\n",
    "\n",
    "üÜï SYSTEM FEATURES:\n",
    "‚Ä¢ üîç Advanced OCR with PaddleOCR/EasyOCR/Tesseract\n",
    "‚Ä¢ üìù Semantic text search using embeddings\n",
    "‚Ä¢ üîç Keyword-based search with SQL\n",
    "‚Ä¢ üìä Comprehensive forensic analysis\n",
    "‚Ä¢ üìÅ Automated result export with metadata\n",
    "‚Ä¢ üíæ SQLite database for persistence\n",
    "‚Ä¢ üéØ Interactive search interface\n",
    "\n",
    "üìã SETUP CHECKLIST:\n",
    "1. ‚úÖ Place suspect images in './suspect_images' folder\n",
    "2. ‚úÖ Run all cells in order (1-7)\n",
    "3. ‚úÖ System automatically processes images with OCR\n",
    "4. ‚úÖ Vector database created for semantic search\n",
    "5. ‚úÖ Ready for forensic analysis\n",
    "\n",
    "üîß OCR MODEL OPTIONS:\n",
    "‚Ä¢ PaddleOCR: Best accuracy, 80+ languages, GPU support\n",
    "‚Ä¢ EasyOCR: Good balance, easy setup, 80+ languages\n",
    "‚Ä¢ Tesseract: Classic, reliable, widely supported\n",
    "\n",
    "üîç SEARCH CAPABILITIES:\n",
    "‚Ä¢ Semantic Search: \"bank account information\" finds related concepts\n",
    "‚Ä¢ Keyword Search: Exact/partial word matching in extracted text\n",
    "‚Ä¢ Combined Analysis: Both semantic + keyword for comprehensive results\n",
    "‚Ä¢ Batch Processing: Multiple queries for systematic investigation\n",
    "\n",
    "üéõÔ∏è KEY FUNCTIONS:\n",
    "forensic_system.search_by_text_query(\"bank account\")\n",
    "forensic_system.search_by_keywords([\"phone\", \"number\"])\n",
    "forensic_system.forensic_query_analysis(\"suspicious activity\")\n",
    "run_comprehensive_forensic_analysis(forensic_system)\n",
    "\n",
    "üìä ANALYSIS FEATURES:\n",
    "‚Ä¢ High-priority evidence detection (score > 0.5)\n",
    "‚Ä¢ Automatic categorization (financial, personal, security, illegal)\n",
    "‚Ä¢ Export results with images and metadata\n",
    "‚Ä¢ Query performance tracking\n",
    "‚Ä¢ Statistical reporting\n",
    "\n",
    "üîç FORENSIC QUERY EXAMPLES:\n",
    "‚Ä¢ \"bank account number\" - Financial information\n",
    "‚Ä¢ \"phone number\" - Contact information\n",
    "‚Ä¢ \"threat violence\" - Security concerns\n",
    "‚Ä¢ \"password\" - Security credentials\n",
    "‚Ä¢ \"suspicious activity\" - General investigation\n",
    "‚Ä¢ \"weapon gun knife\" - Dangerous items\n",
    "‚Ä¢ \"drugs illegal substances\" - Illegal content\n",
    "\n",
    "üìÅ OUTPUT STRUCTURE:\n",
    "forensic_results/\n",
    "‚îú‚îÄ‚îÄ forensic_analysis_20240614_143022/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ high_priority/          # Critical findings\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ financial/              # Financial evidence\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ personal_info/          # Personal information\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ security/               # Security-related\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ illegal/                # Illegal content\n",
    "‚îî‚îÄ‚îÄ search_metadata.json       # Analysis metadata\n",
    "\n",
    "‚öôÔ∏è CONFIGURATION OPTIONS:\n",
    "‚Ä¢ OCR_CONFIDENCE_THRESHOLD: Minimum OCR confidence (0.6)\n",
    "‚Ä¢ SIMILARITY_THRESHOLD: Semantic search threshold (0.3)\n",
    "‚Ä¢ TEXT_MIN_LENGTH: Minimum text length to process (3)\n",
    "‚Ä¢ MAX_RESULTS_DISPLAY: Results shown per query (10)\n",
    "‚Ä¢ BATCH_SIZE: Processing batch size (4)\n",
    "\n",
    "üö® FORENSIC BEST PRACTICES:\n",
    "‚Ä¢ Start with comprehensive batch analysis\n",
    "‚Ä¢ Review high-priority findings first\n",
    "‚Ä¢ Cross-reference semantic + keyword results\n",
    "‚Ä¢ Export evidence with metadata for reports\n",
    "‚Ä¢ Document analysis parameters for court\n",
    "‚Ä¢ Use interactive search for targeted investigation\n",
    "\n",
    "üìà PERFORMANCE OPTIMIZATION:\n",
    "‚Ä¢ PaddleOCR + GPU: Best performance for large datasets\n",
    "‚Ä¢ Adjust confidence thresholds based on image quality\n",
    "‚Ä¢ Use batch processing for efficiency\n",
    "‚Ä¢ Monitor similarity scores for relevance tuning\n",
    "‚Ä¢ Regular database maintenance for speed\n",
    "\n",
    "üîç TROUBLESHOOTING:\n",
    "‚Ä¢ \"No text found\" ‚Üí Check OCR confidence threshold\n",
    "‚Ä¢ \"No search results\" ‚Üí Lower similarity threshold\n",
    "‚Ä¢ \"Slow processing\" ‚Üí Reduce batch size or use CPU\n",
    "‚Ä¢ \"Memory issues\" ‚Üí Process smaller batches\n",
    "‚Ä¢ \"Poor OCR accuracy\" ‚Üí Try different OCR model\n",
    "\n",
    "üìä EXPECTED PERFORMANCE:\n",
    "‚Ä¢ OCR Processing: 2-5 seconds per image (CPU)\n",
    "‚Ä¢ Semantic Search: 0.1-0.5 seconds per query\n",
    "‚Ä¢ Text Detection Rate: 60-90% depending on image quality\n",
    "‚Ä¢ Search Accuracy: High for clear, well-extracted text\n",
    "\n",
    "üéØ INVESTIGATION WORKFLOW:\n",
    "1. Process all suspect images with OCR\n",
    "2. Run comprehensive forensic analysis\n",
    "3. Review high-priority findings\n",
    "4. Conduct targeted searches based on case needs\n",
    "5. Export evidence with metadata\n",
    "6. Generate reports for legal proceedings\n",
    "\n",
    "üí° ADVANCED FEATURES:\n",
    "‚Ä¢ Custom query development for specific cases\n",
    "‚Ä¢ Integration with existing forensic workflows\n",
    "‚Ä¢ Batch export for evidence management\n",
    "‚Ä¢ Statistical analysis for case patterns\n",
    "‚Ä¢ Interactive search for real-time investigation\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully\n",
      "üìÅ Suspects gallery: ../../datasets/images/text_ocr\n",
      "üìÅ Database path: forensic_analysis_db\n",
      "üìÅ Results output: ../../datasets/images/forensic_results\n",
      "üîç OCR model: paddleocr\n",
      "üìù Forensic OCR + Search System Ready\n",
      "‚úÖ Core dependencies available\n",
      "‚úÖ PaddleOCR already available\n",
      "‚úÖ Search dependencies available\n",
      "‚úÖ All dependencies imported successfully\n",
      "üñ•Ô∏è Using CPU mode\n",
      "üìç Auto-detected device: cpu\n",
      "\n",
      "üîç AVAILABLE OCR MODELS\n",
      "==============================\n",
      "‚úÖ PaddleOCR (paddleocr)\n",
      "   üìä Best overall performance\n",
      "   üìù High accuracy, supports 80+ languages\n",
      "   üíª Install: pip install paddlepaddle paddleocr\n",
      "\n",
      "‚ö™ EasyOCR (easyocr)\n",
      "   üìä Good performance, easy setup\n",
      "   üìù Simple to use, good accuracy\n",
      "   üíª Install: pip install easyocr\n",
      "\n",
      "‚ö™ Tesseract (tesseract)\n",
      "   üìä Baseline performance\n",
      "   üìù Classic OCR, widely supported\n",
      "   üíª Install: pip install pytesseract\n",
      "\n",
      "\n",
      "üöÄ STARTING FORENSIC SYSTEM...\n",
      "üöÄ INITIALIZING FORENSIC OCR + SEARCH SYSTEM\n",
      "============================================================\n",
      "üìÅ Directory ready: ../../datasets/images/text_ocr\n",
      "üìÅ Directory ready: forensic_analysis_db\n",
      "üìÅ Directory ready: ../../datasets/images/forensic_results\n",
      "üì• Loading OCR model: paddleocr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Interpreter\\Python\\Projects\\ai_cyberforensics_ocr\\.venv\\lib\\site-packages\\paddle\\utils\\cpp_extension\\extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "\u001B[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001B[0m\n",
      "\u001B[32mUsing official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in C:\\Users\\scott\\.paddlex\\official_models.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d0ae62d5daf412c982c9acf4415c6fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mCreating model: ('UVDoc', None)\u001B[0m\n",
      "\u001B[33mThe model(UVDoc) is not supported to run in MKLDNN mode! Using `paddle` instead!\u001B[0m\n",
      "\u001B[32mUsing official model (UVDoc), the model files will be automatically downloaded and saved in C:\\Users\\scott\\.paddlex\\official_models.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44482d6f65d24f138663a2f642375703"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001B[0m\n",
      "\u001B[32mUsing official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in C:\\Users\\scott\\.paddlex\\official_models.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00c81ed383b3459aa4b3cbaf0d4f86a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mCreating model: ('PP-OCRv5_server_det', None)\u001B[0m\n",
      "\u001B[32mUsing official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in C:\\Users\\scott\\.paddlex\\official_models.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1df6ba0fa5dc46428f35b4868d1d6d3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mCreating model: ('PP-OCRv5_server_rec', None)\u001B[0m\n",
      "\u001B[32mUsing official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in C:\\Users\\scott\\.paddlex\\official_models.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "949299be723b4b0993416f0a750ad423"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PaddleOCR initialized\n",
      "üì• Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "üì• Initializing vector database...\n",
      "‚úÖ Search components initialized\n",
      "‚úÖ Database initialized\n",
      "‚úÖ System initialized successfully!\n",
      "\n",
      "üì∏ No processed images found. Processing gallery...\n",
      "üîç Processing images from: ..\\..\\datasets\\images\\text_ocr\n",
      "üîç OCR Model: paddleocr\n",
      "‚è∞ Start time: 2025-07-09 20:05:41\n",
      "üì∏ Found 200 images to process\n",
      "\n",
      "============================================================\n",
      "üîç PROCESSING IMAGES\n",
      "============================================================\n",
      "[  1/200] (  0.5%) Processing: Text_ocr_000.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[  2/200] (  1.0%) Processing: Text_ocr_001.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[  3/200] (  1.5%) Processing: Text_ocr_002.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[  4/200] (  2.0%) Processing: Text_ocr_003.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[  5/200] (  2.5%) Processing: Text_ocr_004.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[  6/200] (  3.0%) Processing: Text_ocr_005.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[  7/200] (  3.5%) Processing: Text_ocr_006.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[  8/200] (  4.0%) Processing: Text_ocr_007.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[  9/200] (  4.5%) Processing: Text_ocr_008.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 10/200] (  5.0%) Processing: Text_ocr_009.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 11/200] (  5.5%) Processing: Text_ocr_010.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 12/200] (  6.0%) Processing: Text_ocr_011.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 13/200] (  6.5%) Processing: Text_ocr_012.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 14/200] (  7.0%) Processing: Text_ocr_013.jpg..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\AppData\\Local\\Temp\\ipykernel_9260\\1044633368.py:286: DeprecationWarning: Please use `predict` instead.\n",
      "  ocr_result = self.ocr.ocr(image_array, cls=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 15/200] (  7.5%) Processing: Text_ocr_014.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 16/200] (  8.0%) Processing: Text_ocr_015.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 17/200] (  8.5%) Processing: Text_ocr_016.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 18/200] (  9.0%) Processing: Text_ocr_017.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 19/200] (  9.5%) Processing: Text_ocr_018.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 20/200] ( 10.0%) Processing: Text_ocr_019.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 21/200] ( 10.5%) Processing: Text_ocr_020.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 22/200] ( 11.0%) Processing: Text_ocr_021.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 23/200] ( 11.5%) Processing: Text_ocr_022.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 24/200] ( 12.0%) Processing: Text_ocr_023.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 25/200] ( 12.5%) Processing: Text_ocr_024.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 26/200] ( 13.0%) Processing: Text_ocr_025.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 27/200] ( 13.5%) Processing: Text_ocr_026.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 28/200] ( 14.0%) Processing: Text_ocr_027.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 29/200] ( 14.5%) Processing: Text_ocr_028.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 30/200] ( 15.0%) Processing: Text_ocr_029.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 31/200] ( 15.5%) Processing: Text_ocr_030.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 32/200] ( 16.0%) Processing: Text_ocr_031.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 33/200] ( 16.5%) Processing: Text_ocr_032.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 34/200] ( 17.0%) Processing: Text_ocr_033.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 35/200] ( 17.5%) Processing: Text_ocr_034.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 36/200] ( 18.0%) Processing: Text_ocr_035.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 37/200] ( 18.5%) Processing: Text_ocr_036.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 38/200] ( 19.0%) Processing: Text_ocr_037.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 39/200] ( 19.5%) Processing: Text_ocr_038.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 40/200] ( 20.0%) Processing: Text_ocr_039.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 41/200] ( 20.5%) Processing: Text_ocr_040.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 42/200] ( 21.0%) Processing: Text_ocr_041.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 43/200] ( 21.5%) Processing: Text_ocr_042.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 44/200] ( 22.0%) Processing: Text_ocr_043.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 45/200] ( 22.5%) Processing: Text_ocr_044.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 46/200] ( 23.0%) Processing: Text_ocr_045.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 47/200] ( 23.5%) Processing: Text_ocr_046.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 48/200] ( 24.0%) Processing: Text_ocr_047.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 49/200] ( 24.5%) Processing: Text_ocr_048.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 50/200] ( 25.0%) Processing: Text_ocr_049.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 51/200] ( 25.5%) Processing: Text_ocr_050.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 52/200] ( 26.0%) Processing: Text_ocr_051.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 53/200] ( 26.5%) Processing: Text_ocr_052.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 54/200] ( 27.0%) Processing: Text_ocr_053.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 55/200] ( 27.5%) Processing: Text_ocr_054.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 56/200] ( 28.0%) Processing: Text_ocr_055.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 57/200] ( 28.5%) Processing: Text_ocr_056.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 58/200] ( 29.0%) Processing: Text_ocr_057.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 59/200] ( 29.5%) Processing: Text_ocr_058.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 60/200] ( 30.0%) Processing: Text_ocr_059.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 61/200] ( 30.5%) Processing: Text_ocr_060.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 62/200] ( 31.0%) Processing: Text_ocr_061.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 63/200] ( 31.5%) Processing: Text_ocr_062.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 64/200] ( 32.0%) Processing: Text_ocr_063.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 65/200] ( 32.5%) Processing: Text_ocr_064.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 66/200] ( 33.0%) Processing: Text_ocr_065.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 67/200] ( 33.5%) Processing: Text_ocr_066.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 68/200] ( 34.0%) Processing: Text_ocr_067.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 69/200] ( 34.5%) Processing: Text_ocr_068.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 70/200] ( 35.0%) Processing: Text_ocr_069.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 71/200] ( 35.5%) Processing: Text_ocr_070.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 72/200] ( 36.0%) Processing: Text_ocr_071.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 73/200] ( 36.5%) Processing: Text_ocr_072.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 74/200] ( 37.0%) Processing: Text_ocr_073.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 75/200] ( 37.5%) Processing: Text_ocr_074.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 76/200] ( 38.0%) Processing: Text_ocr_075.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 77/200] ( 38.5%) Processing: Text_ocr_076.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 78/200] ( 39.0%) Processing: Text_ocr_077.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 79/200] ( 39.5%) Processing: Text_ocr_078.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 80/200] ( 40.0%) Processing: Text_ocr_079.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 81/200] ( 40.5%) Processing: Text_ocr_080.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 82/200] ( 41.0%) Processing: Text_ocr_081.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 83/200] ( 41.5%) Processing: Text_ocr_082.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 84/200] ( 42.0%) Processing: Text_ocr_083.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 85/200] ( 42.5%) Processing: Text_ocr_084.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 86/200] ( 43.0%) Processing: Text_ocr_085.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 87/200] ( 43.5%) Processing: Text_ocr_086.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 88/200] ( 44.0%) Processing: Text_ocr_087.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 89/200] ( 44.5%) Processing: Text_ocr_088.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 90/200] ( 45.0%) Processing: Text_ocr_089.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 91/200] ( 45.5%) Processing: Text_ocr_090.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 92/200] ( 46.0%) Processing: Text_ocr_091.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 93/200] ( 46.5%) Processing: Text_ocr_092.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 94/200] ( 47.0%) Processing: Text_ocr_093.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 95/200] ( 47.5%) Processing: Text_ocr_094.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 96/200] ( 48.0%) Processing: Text_ocr_095.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 97/200] ( 48.5%) Processing: Text_ocr_096.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 98/200] ( 49.0%) Processing: Text_ocr_097.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[ 99/200] ( 49.5%) Processing: Text_ocr_098.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[100/200] ( 50.0%) Processing: Text_ocr_099.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[101/200] ( 50.5%) Processing: Text_ocr_000.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[102/200] ( 51.0%) Processing: Text_ocr_001.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[103/200] ( 51.5%) Processing: Text_ocr_002.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[104/200] ( 52.0%) Processing: Text_ocr_003.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[105/200] ( 52.5%) Processing: Text_ocr_004.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[106/200] ( 53.0%) Processing: Text_ocr_005.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[107/200] ( 53.5%) Processing: Text_ocr_006.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[108/200] ( 54.0%) Processing: Text_ocr_007.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[109/200] ( 54.5%) Processing: Text_ocr_008.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[110/200] ( 55.0%) Processing: Text_ocr_009.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[111/200] ( 55.5%) Processing: Text_ocr_010.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[112/200] ( 56.0%) Processing: Text_ocr_011.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[113/200] ( 56.5%) Processing: Text_ocr_012.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[114/200] ( 57.0%) Processing: Text_ocr_013.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[115/200] ( 57.5%) Processing: Text_ocr_014.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[116/200] ( 58.0%) Processing: Text_ocr_015.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[117/200] ( 58.5%) Processing: Text_ocr_016.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[118/200] ( 59.0%) Processing: Text_ocr_017.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[119/200] ( 59.5%) Processing: Text_ocr_018.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[120/200] ( 60.0%) Processing: Text_ocr_019.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[121/200] ( 60.5%) Processing: Text_ocr_020.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[122/200] ( 61.0%) Processing: Text_ocr_021.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[123/200] ( 61.5%) Processing: Text_ocr_022.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[124/200] ( 62.0%) Processing: Text_ocr_023.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[125/200] ( 62.5%) Processing: Text_ocr_024.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[126/200] ( 63.0%) Processing: Text_ocr_025.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[127/200] ( 63.5%) Processing: Text_ocr_026.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[128/200] ( 64.0%) Processing: Text_ocr_027.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[129/200] ( 64.5%) Processing: Text_ocr_028.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[130/200] ( 65.0%) Processing: Text_ocr_029.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[131/200] ( 65.5%) Processing: Text_ocr_030.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[132/200] ( 66.0%) Processing: Text_ocr_031.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[133/200] ( 66.5%) Processing: Text_ocr_032.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[134/200] ( 67.0%) Processing: Text_ocr_033.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[135/200] ( 67.5%) Processing: Text_ocr_034.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[136/200] ( 68.0%) Processing: Text_ocr_035.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[137/200] ( 68.5%) Processing: Text_ocr_036.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[138/200] ( 69.0%) Processing: Text_ocr_037.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[139/200] ( 69.5%) Processing: Text_ocr_038.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[140/200] ( 70.0%) Processing: Text_ocr_039.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[141/200] ( 70.5%) Processing: Text_ocr_040.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[142/200] ( 71.0%) Processing: Text_ocr_041.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[143/200] ( 71.5%) Processing: Text_ocr_042.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[144/200] ( 72.0%) Processing: Text_ocr_043.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[145/200] ( 72.5%) Processing: Text_ocr_044.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[146/200] ( 73.0%) Processing: Text_ocr_045.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[147/200] ( 73.5%) Processing: Text_ocr_046.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[148/200] ( 74.0%) Processing: Text_ocr_047.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[149/200] ( 74.5%) Processing: Text_ocr_048.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[150/200] ( 75.0%) Processing: Text_ocr_049.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[151/200] ( 75.5%) Processing: Text_ocr_050.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[152/200] ( 76.0%) Processing: Text_ocr_051.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[153/200] ( 76.5%) Processing: Text_ocr_052.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[154/200] ( 77.0%) Processing: Text_ocr_053.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[155/200] ( 77.5%) Processing: Text_ocr_054.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[156/200] ( 78.0%) Processing: Text_ocr_055.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[157/200] ( 78.5%) Processing: Text_ocr_056.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[158/200] ( 79.0%) Processing: Text_ocr_057.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[159/200] ( 79.5%) Processing: Text_ocr_058.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[160/200] ( 80.0%) Processing: Text_ocr_059.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[161/200] ( 80.5%) Processing: Text_ocr_060.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[162/200] ( 81.0%) Processing: Text_ocr_061.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[163/200] ( 81.5%) Processing: Text_ocr_062.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[164/200] ( 82.0%) Processing: Text_ocr_063.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[165/200] ( 82.5%) Processing: Text_ocr_064.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[166/200] ( 83.0%) Processing: Text_ocr_065.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[167/200] ( 83.5%) Processing: Text_ocr_066.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[168/200] ( 84.0%) Processing: Text_ocr_067.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[169/200] ( 84.5%) Processing: Text_ocr_068.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[170/200] ( 85.0%) Processing: Text_ocr_069.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[171/200] ( 85.5%) Processing: Text_ocr_070.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[172/200] ( 86.0%) Processing: Text_ocr_071.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[173/200] ( 86.5%) Processing: Text_ocr_072.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[174/200] ( 87.0%) Processing: Text_ocr_073.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[175/200] ( 87.5%) Processing: Text_ocr_074.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[176/200] ( 88.0%) Processing: Text_ocr_075.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[177/200] ( 88.5%) Processing: Text_ocr_076.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[178/200] ( 89.0%) Processing: Text_ocr_077.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[179/200] ( 89.5%) Processing: Text_ocr_078.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[180/200] ( 90.0%) Processing: Text_ocr_079.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[181/200] ( 90.5%) Processing: Text_ocr_080.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[182/200] ( 91.0%) Processing: Text_ocr_081.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[183/200] ( 91.5%) Processing: Text_ocr_082.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[184/200] ( 92.0%) Processing: Text_ocr_083.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[185/200] ( 92.5%) Processing: Text_ocr_084.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[186/200] ( 93.0%) Processing: Text_ocr_085.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[187/200] ( 93.5%) Processing: Text_ocr_086.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[188/200] ( 94.0%) Processing: Text_ocr_087.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[189/200] ( 94.5%) Processing: Text_ocr_088.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[190/200] ( 95.0%) Processing: Text_ocr_089.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[191/200] ( 95.5%) Processing: Text_ocr_090.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[192/200] ( 96.0%) Processing: Text_ocr_091.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[193/200] ( 96.5%) Processing: Text_ocr_092.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[194/200] ( 97.0%) Processing: Text_ocr_093.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[195/200] ( 97.5%) Processing: Text_ocr_094.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[196/200] ( 98.0%) Processing: Text_ocr_095.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[197/200] ( 98.5%) Processing: Text_ocr_096.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[198/200] ( 99.0%) Processing: Text_ocr_097.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[199/200] ( 99.5%) Processing: Text_ocr_098.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "[200/200] (100.0%) Processing: Text_ocr_099.jpg... ‚ùå ERROR: predict() got an unexpected ke\n",
      "\n",
      "============================================================\n",
      "üìä PROCESSING SUMMARY\n",
      "============================================================\n",
      "üì∏ Total images: 200\n",
      "‚úÖ Successfully processed: 0\n",
      "üìù Images with text: 0\n",
      "‚ö™ Images without text: 0\n",
      "‚è±Ô∏è Total time: 2.6s (0.01s per image)\n",
      "\n",
      "============================================================\n",
      "\n",
      "‚úÖ FORENSIC OCR + SEARCH SYSTEM READY!\n",
      "==================================================\n",
      "üìã AVAILABLE FUNCTIONS:\n",
      "  ‚Ä¢ forensic_system.process_image_gallery() - Process new images\n",
      "  ‚Ä¢ forensic_system.search_by_text_query(query) - Semantic search\n",
      "  ‚Ä¢ forensic_system.search_by_keywords([keywords]) - Keyword search\n",
      "  ‚Ä¢ forensic_system.forensic_query_analysis(query) - Full analysis\n",
      "  ‚Ä¢ run_comprehensive_forensic_analysis(forensic_system) - Batch analysis\n",
      "  ‚Ä¢ interactive_search() - Interactive search interface\n",
      "  ‚Ä¢ display_stats() - Show system statistics\n",
      "  ‚Ä¢ run_quick_demo(forensic_system) - Quick demonstration\n",
      "==================================================\n",
      "\n",
      "üìä FORENSIC SYSTEM STATISTICS\n",
      "========================================\n",
      "üì∏ Total images processed: 0\n",
      "üìù Images with text: 0\n",
      "üìä Text detection rate: 0.0%\n",
      "üéØ Average OCR confidence: 0.000\n",
      "üìã Total text blocks found: 0\n",
      "üîç Total queries executed: 8\n",
      "‚è±Ô∏è Average query time: 1.107s\n",
      "üíæ Total data size: 0 MB\n",
      "üìè Average file size: 0 KB\n",
      "========================================\n",
      "\n",
      "üéØ QUICK DEMONSTRATION\n",
      "==============================\n",
      "\n",
      "üîç Demo query: 'phone number'\n",
      "‚ö™ No matches\n",
      "\n",
      "üîç Demo query: 'bank account'\n",
      "‚ö™ No matches\n",
      "\n",
      "üîç Demo query: 'password'\n",
      "‚ö™ No matches\n",
      "\n",
      "üîç Demo query: 'address'\n",
      "‚ö™ No matches\n",
      "\n",
      "üí° TIP: Use interactive_search() for hands-on searching!\n",
      "üí° TIP: Run run_comprehensive_forensic_analysis(forensic_system) for full analysis!\n",
      "\n",
      "üéØ FORENSIC OCR + SEARCH SYSTEM - COMPLETE GUIDE\n",
      "===============================================\n",
      "\n",
      "üÜï SYSTEM FEATURES:\n",
      "‚Ä¢ üîç Advanced OCR with PaddleOCR/EasyOCR/Tesseract\n",
      "‚Ä¢ üìù Semantic text search using embeddings\n",
      "‚Ä¢ üîç Keyword-based search with SQL\n",
      "‚Ä¢ üìä Comprehensive forensic analysis\n",
      "‚Ä¢ üìÅ Automated result export with metadata\n",
      "‚Ä¢ üíæ SQLite database for persistence\n",
      "‚Ä¢ üéØ Interactive search interface\n",
      "\n",
      "üìã SETUP CHECKLIST:\n",
      "1. ‚úÖ Place suspect images in './suspect_images' folder\n",
      "2. ‚úÖ Run all cells in order (1-7)\n",
      "3. ‚úÖ System automatically processes images with OCR\n",
      "4. ‚úÖ Vector database created for semantic search\n",
      "5. ‚úÖ Ready for forensic analysis\n",
      "\n",
      "üîß OCR MODEL OPTIONS:\n",
      "‚Ä¢ PaddleOCR: Best accuracy, 80+ languages, GPU support\n",
      "‚Ä¢ EasyOCR: Good balance, easy setup, 80+ languages\n",
      "‚Ä¢ Tesseract: Classic, reliable, widely supported\n",
      "\n",
      "üîç SEARCH CAPABILITIES:\n",
      "‚Ä¢ Semantic Search: \"bank account information\" finds related concepts\n",
      "‚Ä¢ Keyword Search: Exact/partial word matching in extracted text\n",
      "‚Ä¢ Combined Analysis: Both semantic + keyword for comprehensive results\n",
      "‚Ä¢ Batch Processing: Multiple queries for systematic investigation\n",
      "\n",
      "üéõÔ∏è KEY FUNCTIONS:\n",
      "forensic_system.search_by_text_query(\"bank account\")\n",
      "forensic_system.search_by_keywords([\"phone\", \"number\"])\n",
      "forensic_system.forensic_query_analysis(\"suspicious activity\")\n",
      "run_comprehensive_forensic_analysis(forensic_system)\n",
      "\n",
      "üìä ANALYSIS FEATURES:\n",
      "‚Ä¢ High-priority evidence detection (score > 0.5)\n",
      "‚Ä¢ Automatic categorization (financial, personal, security, illegal)\n",
      "‚Ä¢ Export results with images and metadata\n",
      "‚Ä¢ Query performance tracking\n",
      "‚Ä¢ Statistical reporting\n",
      "\n",
      "üîç FORENSIC QUERY EXAMPLES:\n",
      "‚Ä¢ \"bank account number\" - Financial information\n",
      "‚Ä¢ \"phone number\" - Contact information\n",
      "‚Ä¢ \"threat violence\" - Security concerns\n",
      "‚Ä¢ \"password\" - Security credentials\n",
      "‚Ä¢ \"suspicious activity\" - General investigation\n",
      "‚Ä¢ \"weapon gun knife\" - Dangerous items\n",
      "‚Ä¢ \"drugs illegal substances\" - Illegal content\n",
      "\n",
      "üìÅ OUTPUT STRUCTURE:\n",
      "forensic_results/\n",
      "‚îú‚îÄ‚îÄ forensic_analysis_20240614_143022/\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ high_priority/          # Critical findings\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ financial/              # Financial evidence\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ personal_info/          # Personal information\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ security/               # Security-related\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ illegal/                # Illegal content\n",
      "‚îî‚îÄ‚îÄ search_metadata.json       # Analysis metadata\n",
      "\n",
      "‚öôÔ∏è CONFIGURATION OPTIONS:\n",
      "‚Ä¢ OCR_CONFIDENCE_THRESHOLD: Minimum OCR confidence (0.6)\n",
      "‚Ä¢ SIMILARITY_THRESHOLD: Semantic search threshold (0.3)\n",
      "‚Ä¢ TEXT_MIN_LENGTH: Minimum text length to process (3)\n",
      "‚Ä¢ MAX_RESULTS_DISPLAY: Results shown per query (10)\n",
      "‚Ä¢ BATCH_SIZE: Processing batch size (4)\n",
      "\n",
      "üö® FORENSIC BEST PRACTICES:\n",
      "‚Ä¢ Start with comprehensive batch analysis\n",
      "‚Ä¢ Review high-priority findings first\n",
      "‚Ä¢ Cross-reference semantic + keyword results\n",
      "‚Ä¢ Export evidence with metadata for reports\n",
      "‚Ä¢ Document analysis parameters for court\n",
      "‚Ä¢ Use interactive search for targeted investigation\n",
      "\n",
      "üìà PERFORMANCE OPTIMIZATION:\n",
      "‚Ä¢ PaddleOCR + GPU: Best performance for large datasets\n",
      "‚Ä¢ Adjust confidence thresholds based on image quality\n",
      "‚Ä¢ Use batch processing for efficiency\n",
      "‚Ä¢ Monitor similarity scores for relevance tuning\n",
      "‚Ä¢ Regular database maintenance for speed\n",
      "\n",
      "üîç TROUBLESHOOTING:\n",
      "‚Ä¢ \"No text found\" ‚Üí Check OCR confidence threshold\n",
      "‚Ä¢ \"No search results\" ‚Üí Lower similarity threshold\n",
      "‚Ä¢ \"Slow processing\" ‚Üí Reduce batch size or use CPU\n",
      "‚Ä¢ \"Memory issues\" ‚Üí Process smaller batches\n",
      "‚Ä¢ \"Poor OCR accuracy\" ‚Üí Try different OCR model\n",
      "\n",
      "üìä EXPECTED PERFORMANCE:\n",
      "‚Ä¢ OCR Processing: 2-5 seconds per image (CPU)\n",
      "‚Ä¢ Semantic Search: 0.1-0.5 seconds per query\n",
      "‚Ä¢ Text Detection Rate: 60-90% depending on image quality\n",
      "‚Ä¢ Search Accuracy: High for clear, well-extracted text\n",
      "\n",
      "üéØ INVESTIGATION WORKFLOW:\n",
      "1. Process all suspect images with OCR\n",
      "2. Run comprehensive forensic analysis\n",
      "3. Review high-priority findings\n",
      "4. Conduct targeted searches based on case needs\n",
      "5. Export evidence with metadata\n",
      "6. Generate reports for legal proceedings\n",
      "\n",
      "üí° ADVANCED FEATURES:\n",
      "‚Ä¢ Custom query development for specific cases\n",
      "‚Ä¢ Integration with existing forensic workflows\n",
      "‚Ä¢ Batch export for evidence management\n",
      "‚Ä¢ Statistical analysis for case patterns\n",
      "‚Ä¢ Interactive search for real-time investigation\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
