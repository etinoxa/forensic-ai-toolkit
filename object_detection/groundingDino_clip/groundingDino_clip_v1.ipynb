{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-14T05:34:33.006191Z",
     "start_time": "2025-06-14T05:34:12.187316Z"
    }
   },
   "source": [
    "# Cell 1: Configuration and Setup\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Configuration Settings\n",
    "CONFIG = {\n",
    "    # Model Selection\n",
    "    'GROUNDING_DINO_MODEL': 'IDEA-Research/grounding-dino-base',\n",
    "    'CLIP_MODEL': 'ViT-B/32',\n",
    "    'DEVICE': 'auto',\n",
    "\n",
    "    # Paths\n",
    "    'SUSPECTS_GALLERY_PATH': '../../datasets/images/objects/raw',  # Input folder with suspect images\n",
    "    'RESULTS_OUTPUT_PATH': '../../datasets/images/objects/detections',      # Output folder for matched images\n",
    "    # Detection parameters\n",
    "    'GROUNDING_DINO_CONFIDENCE': 0.35,\n",
    "    'GROUNDING_DINO_BOX_THRESHOLD': 0.3,\n",
    "    'GROUNDING_DINO_TEXT_THRESHOLD': 0.25,\n",
    "\n",
    "    'CLIP_SIMILARITY_THRESHOLD': 0.25,\n",
    "    'CLIP_PATCH_SIZE': 224,\n",
    "    'CLIP_STRIDE': 112,\n",
    "    'CLIP_MAX_PATCHES': 16,\n",
    "\n",
    "    # Processing settings\n",
    "    'BATCH_SIZE': 4,\n",
    "    'MAX_RESULTS_DISPLAY': 10,\n",
    "    'FIGURE_SIZE': (15, 10),\n",
    "    'COMPARISON_MODE': True,  # Show results from both models\n",
    "}\n",
    "\n",
    "# Available models\n",
    "AVAILABLE_MODELS = {\n",
    "    'grounding_dino': {\n",
    "        'grounding-dino-tiny': 'IDEA-Research/grounding-dino-tiny',\n",
    "        'grounding-dino-base': 'IDEA-Research/grounding-dino-base'\n",
    "    },\n",
    "    'clip': {\n",
    "        'RN50': 'RN50',\n",
    "        'RN101': 'RN101',\n",
    "        'ViT-B/32': 'ViT-B/32',\n",
    "        'ViT-B/16': 'ViT-B/16',\n",
    "        'ViT-L/14': 'ViT-L/14'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ Configuration loaded successfully\")\n",
    "print(f\"📁 Suspects gallery: {CONFIG['SUSPECTS_GALLERY_PATH']}\")\n",
    "print(f\"📁 Results output: {CONFIG['RESULTS_OUTPUT_PATH']}\")\n",
    "print(f\"🎯 GroundingDINO: {CONFIG['GROUNDING_DINO_MODEL']}\")\n",
    "print(f\"🔍 CLIP: {CONFIG['CLIP_MODEL']}\")\n",
    "print(\"🚀 Combined model approach for comprehensive analysis\")\n",
    "\n",
    "# Cell 2: Install and Import Dependencies\n",
    "def install_dependencies():\n",
    "    \"\"\"Install all required dependencies\"\"\"\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    packages = [\n",
    "        \"torch\",\n",
    "        \"torchvision\",\n",
    "        \"transformers\",\n",
    "        \"git+https://github.com/openai/CLIP.git\",\n",
    "        \"ipywidgets\",\n",
    "        \"matplotlib\",\n",
    "        \"pillow\",\n",
    "        \"numpy\"\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        try:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"❌ Failed to install {package}: {e}\")\n",
    "\n",
    "    print(\"📦 Installation complete\")\n",
    "\n",
    "# Check and install dependencies\n",
    "try:\n",
    "    import transformers\n",
    "    import clip\n",
    "    print(\"✅ Dependencies already installed\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ Installing dependencies...\")\n",
    "    install_dependencies()\n",
    "    print(\"🔄 Please restart kernel and run this cell again\")\n",
    "\n",
    "# Import required libraries\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "    import clip\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "\n",
    "    print(\"✅ All dependencies imported successfully\")\n",
    "\n",
    "    # Device setup\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"🚀 CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        default_device = \"cuda\"\n",
    "    else:\n",
    "        print(\"🖥️ Using CPU mode\")\n",
    "        default_device = \"cpu\"\n",
    "\n",
    "    if CONFIG['DEVICE'] == 'auto':\n",
    "        CONFIG['DEVICE'] = default_device\n",
    "        print(f\"📍 Device: {CONFIG['DEVICE']}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"Please restart kernel after installation\")\n",
    "\n",
    "# Cell 3: Model Loading and Management\n",
    "class CombinedModels:\n",
    "    \"\"\"Combined GroundingDINO and CLIP models\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.grounding_dino_model = None\n",
    "        self.grounding_dino_processor = None\n",
    "        self.clip_model = None\n",
    "        self.clip_preprocess = None\n",
    "        self.device = CONFIG['DEVICE']\n",
    "\n",
    "    def load_grounding_dino(self, model_name=None):\n",
    "        \"\"\"Load GroundingDINO model\"\"\"\n",
    "        try:\n",
    "            if model_name is None:\n",
    "                model_name = CONFIG['GROUNDING_DINO_MODEL']\n",
    "\n",
    "            print(f\"📥 Loading GroundingDINO: {model_name}\")\n",
    "\n",
    "            self.grounding_dino_processor = AutoProcessor.from_pretrained(model_name)\n",
    "            self.grounding_dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(model_name)\n",
    "            self.grounding_dino_model = self.grounding_dino_model.to(self.device)\n",
    "            self.grounding_dino_model.eval()\n",
    "\n",
    "            print(\"✅ GroundingDINO loaded successfully\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading GroundingDINO: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_clip(self, model_name=None):\n",
    "        \"\"\"Load CLIP model\"\"\"\n",
    "        try:\n",
    "            if model_name is None:\n",
    "                model_name = CONFIG['CLIP_MODEL']\n",
    "\n",
    "            print(f\"📥 Loading CLIP: {model_name}\")\n",
    "\n",
    "            self.clip_model, self.clip_preprocess = clip.load(model_name, device=self.device)\n",
    "            self.clip_model.eval()\n",
    "\n",
    "            print(\"✅ CLIP loaded successfully\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading CLIP: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_both_models(self):\n",
    "        \"\"\"Load both models\"\"\"\n",
    "        print(\"🚀 Loading both models...\")\n",
    "\n",
    "        gdino_success = self.load_grounding_dino()\n",
    "        clip_success = self.load_clip()\n",
    "\n",
    "        if gdino_success and clip_success:\n",
    "            print(\"✅ Both models loaded successfully!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ Failed to load one or both models\")\n",
    "            return False\n",
    "\n",
    "    def search_with_grounding_dino(self, image_path, query):\n",
    "        \"\"\"Search using GroundingDINO\"\"\"\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "            # Format query\n",
    "            text_query = query.lower()\n",
    "            if not text_query.endswith('.'):\n",
    "                text_query += '.'\n",
    "\n",
    "            # Process inputs\n",
    "            inputs = self.grounding_dino_processor(images=image, text=text_query, return_tensors=\"pt\")\n",
    "            inputs = inputs.to(self.device)\n",
    "\n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                outputs = self.grounding_dino_model(**inputs)\n",
    "\n",
    "            # Post-process\n",
    "            results = self.grounding_dino_processor.post_process_grounded_object_detection(\n",
    "                outputs,\n",
    "                inputs.input_ids,\n",
    "                box_threshold=CONFIG['GROUNDING_DINO_BOX_THRESHOLD'],\n",
    "                text_threshold=CONFIG['GROUNDING_DINO_TEXT_THRESHOLD'],\n",
    "                target_sizes=[image.size[::-1]]\n",
    "            )\n",
    "\n",
    "            if results and len(results) > 0:\n",
    "                result = results[0]\n",
    "                if 'scores' in result and len(result['scores']) > 0:\n",
    "                    # Filter by confidence\n",
    "                    high_conf_mask = result['scores'] >= CONFIG['GROUNDING_DINO_CONFIDENCE']\n",
    "\n",
    "                    if high_conf_mask.any():\n",
    "                        return {\n",
    "                            'image_path': image_path,\n",
    "                            'image': image,\n",
    "                            'boxes': result['boxes'][high_conf_mask],\n",
    "                            'scores': result['scores'][high_conf_mask],\n",
    "                            'labels': [query] * len(result['scores'][high_conf_mask]),\n",
    "                            'query': query,\n",
    "                            'model': 'GroundingDINO'\n",
    "                        }\n",
    "\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"GroundingDINO error on {image_path.name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def search_with_clip(self, image_path, query):\n",
    "        \"\"\"Search using CLIP\"\"\"\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "            # Extract patches\n",
    "            patches, positions = self._extract_patches(image)\n",
    "\n",
    "            if not patches:\n",
    "                return None\n",
    "\n",
    "            # Compute similarities\n",
    "            similarities = self._compute_clip_similarity(query, patches)\n",
    "\n",
    "            if len(similarities) == 0:\n",
    "                return None\n",
    "\n",
    "            # Filter by threshold\n",
    "            high_sim_mask = similarities >= CONFIG['CLIP_SIMILARITY_THRESHOLD']\n",
    "\n",
    "            if not high_sim_mask.any():\n",
    "                return None\n",
    "\n",
    "            filtered_similarities = similarities[high_sim_mask]\n",
    "            filtered_positions = [positions[i] for i in range(len(positions)) if high_sim_mask[i]]\n",
    "\n",
    "            return {\n",
    "                'image_path': image_path,\n",
    "                'image': image,\n",
    "                'boxes': torch.tensor(filtered_positions, dtype=torch.float32),\n",
    "                'scores': torch.tensor(filtered_similarities, dtype=torch.float32),\n",
    "                'labels': [query] * len(filtered_similarities),\n",
    "                'query': query,\n",
    "                'model': 'CLIP'\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"CLIP error on {image_path.name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _extract_patches(self, image):\n",
    "        \"\"\"Extract patches for CLIP analysis\"\"\"\n",
    "        width, height = image.size\n",
    "        patch_size = CONFIG['CLIP_PATCH_SIZE']\n",
    "        stride = CONFIG['CLIP_STRIDE']\n",
    "        max_patches = CONFIG['CLIP_MAX_PATCHES']\n",
    "\n",
    "        patches = []\n",
    "        positions = []\n",
    "\n",
    "        x_positions = range(0, max(1, width - patch_size + 1), stride)\n",
    "        y_positions = range(0, max(1, height - patch_size + 1), stride)\n",
    "\n",
    "        for y in y_positions:\n",
    "            for x in x_positions:\n",
    "                x_end = min(x + patch_size, width)\n",
    "                y_end = min(y + patch_size, height)\n",
    "\n",
    "                patch = image.crop((x, y, x_end, y_end))\n",
    "\n",
    "                if patch.size != (patch_size, patch_size):\n",
    "                    patch = patch.resize((patch_size, patch_size), Image.LANCZOS)\n",
    "\n",
    "                patches.append(patch)\n",
    "                positions.append((x, y, x_end, y_end))\n",
    "\n",
    "                if len(patches) >= max_patches:\n",
    "                    break\n",
    "            if len(patches) >= max_patches:\n",
    "                break\n",
    "\n",
    "        return patches, positions\n",
    "\n",
    "    def _compute_clip_similarity(self, text_query, images):\n",
    "        \"\"\"Compute CLIP similarity\"\"\"\n",
    "        try:\n",
    "            image_inputs = torch.stack([self.clip_preprocess(img) for img in images]).to(self.device)\n",
    "            text_inputs = clip.tokenize([text_query]).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                image_features = self.clip_model.encode_image(image_inputs)\n",
    "                text_features = self.clip_model.encode_text(text_inputs)\n",
    "\n",
    "                image_features = F.normalize(image_features, dim=-1)\n",
    "                text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "                similarities = torch.matmul(image_features, text_features.T).squeeze(-1)\n",
    "\n",
    "            return similarities.cpu().numpy()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"CLIP similarity error: {e}\")\n",
    "            return np.array([])\n",
    "\n",
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories\"\"\"\n",
    "    os.makedirs(CONFIG['SUSPECTS_GALLERY_PATH'], exist_ok=True)\n",
    "    os.makedirs(CONFIG['RESULTS_OUTPUT_PATH'], exist_ok=True)\n",
    "    print(\"📁 Directories ready\")\n",
    "\n",
    "# Initialize\n",
    "setup_directories()\n",
    "combined_models = CombinedModels()\n",
    "models_loaded = combined_models.load_both_models()\n",
    "\n",
    "# Cell 4: Combined Search Functions\n",
    "def search_with_both_models(query, gallery_path, comparison_mode=True):\n",
    "    \"\"\"Search using both GroundingDINO and CLIP\"\"\"\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    start_time = time.time()\n",
    "    start_datetime = datetime.now()\n",
    "\n",
    "    print(f\"🔍 Combined search for: '{query}'\")\n",
    "    print(f\"⏰ Start: {start_datetime.strftime('%H:%M:%S')}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Get image files\n",
    "    gallery_path = Path(gallery_path)\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}\n",
    "    image_files = [f for f in gallery_path.iterdir() if f.suffix.lower() in image_extensions]\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"⚠️ No images found in {gallery_path}\")\n",
    "        return {}, {}\n",
    "\n",
    "    grounding_dino_results = []\n",
    "    clip_results = []\n",
    "    total_files = len(image_files)\n",
    "\n",
    "    print(f\"📸 Processing {total_files} images with both models...\")\n",
    "\n",
    "    for i, img_path in enumerate(image_files):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Progress: {i+1}/{total_files}\", end='\\r')\n",
    "\n",
    "        # Search with GroundingDINO\n",
    "        if combined_models.grounding_dino_model:\n",
    "            gdino_result = combined_models.search_with_grounding_dino(img_path, query)\n",
    "            if gdino_result:\n",
    "                grounding_dino_results.append(gdino_result)\n",
    "\n",
    "        # Search with CLIP\n",
    "        if combined_models.clip_model:\n",
    "            clip_result = combined_models.search_with_clip(img_path, query)\n",
    "            if clip_result:\n",
    "                clip_results.append(clip_result)\n",
    "\n",
    "        # Memory cleanup\n",
    "        if torch.cuda.is_available() and i % 20 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Calculate timing\n",
    "    end_time = time.time()\n",
    "    end_datetime = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    duration_str = f\"{int(duration//60)}m {duration%60:.1f}s\" if duration >= 60 else f\"{duration:.1f}s\"\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"📊 COMBINED SEARCH SUMMARY\")\n",
    "    print(f\"=\"*60)\n",
    "    print(f\"📸 Images processed: {total_files}\")\n",
    "    print(f\"🎯 GroundingDINO matches: {len(grounding_dino_results)}\")\n",
    "    print(f\"🔍 CLIP matches: {len(clip_results)}\")\n",
    "    print(f\"⏰ Start: {start_datetime.strftime('%H:%M:%S')}\")\n",
    "    print(f\"🏁 End: {end_datetime.strftime('%H:%M:%S')}\")\n",
    "    print(f\"⏱️ Duration: {duration_str}\")\n",
    "    print(f\"📈 Avg per image: {duration/total_files:.2f}s\")\n",
    "    print(f\"=\"*60)\n",
    "\n",
    "    return {\n",
    "        'grounding_dino': grounding_dino_results,\n",
    "        'clip': clip_results,\n",
    "        'query': query,\n",
    "        'total_images': total_files,\n",
    "        'duration': duration\n",
    "    }\n",
    "\n",
    "def compare_model_results(results_dict):\n",
    "    \"\"\"Compare results from both models\"\"\"\n",
    "    gdino_results = results_dict.get('grounding_dino', [])\n",
    "    clip_results = results_dict.get('clip', [])\n",
    "\n",
    "    print(f\"\\n📊 MODEL COMPARISON\")\n",
    "    print(f\"-\" * 40)\n",
    "    print(f\"🎯 GroundingDINO: {len(gdino_results)} matches\")\n",
    "    print(f\"🔍 CLIP: {len(clip_results)} matches\")\n",
    "\n",
    "    # Find images detected by both models\n",
    "    gdino_images = {result['image_path'].name for result in gdino_results}\n",
    "    clip_images = {result['image_path'].name for result in clip_results}\n",
    "\n",
    "    common_images = gdino_images & clip_images\n",
    "    gdino_only = gdino_images - clip_images\n",
    "    clip_only = clip_images - gdino_images\n",
    "\n",
    "    print(f\"🤝 Both models: {len(common_images)} images\")\n",
    "    print(f\"🎯 GroundingDINO only: {len(gdino_only)} images\")\n",
    "    print(f\"🔍 CLIP only: {len(clip_only)} images\")\n",
    "\n",
    "    if common_images:\n",
    "        print(f\"✅ High confidence matches (both models agree):\")\n",
    "        for img_name in sorted(list(common_images)[:5]):\n",
    "            print(f\"   • {img_name}\")\n",
    "\n",
    "    return {\n",
    "        'common': common_images,\n",
    "        'grounding_dino_only': gdino_only,\n",
    "        'clip_only': clip_only\n",
    "    }\n",
    "\n",
    "def copy_combined_results(results_dict, output_folder):\n",
    "    \"\"\"Copy results from both models to organized folders\"\"\"\n",
    "    if not results_dict:\n",
    "        return None\n",
    "\n",
    "    output_path = Path(output_folder)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Create timestamped folder\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    search_folder = output_path / f\"combined_search_{timestamp}\"\n",
    "    search_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    # Create subfolders for each model\n",
    "    gdino_folder = search_folder / \"grounding_dino\"\n",
    "    clip_folder = search_folder / \"clip\"\n",
    "    both_folder = search_folder / \"both_models\"\n",
    "\n",
    "    gdino_folder.mkdir(exist_ok=True)\n",
    "    clip_folder.mkdir(exist_ok=True)\n",
    "    both_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    gdino_results = results_dict.get('grounding_dino', [])\n",
    "    clip_results = results_dict.get('clip', [])\n",
    "\n",
    "    # Get image sets\n",
    "    gdino_images = {result['image_path'].name: result for result in gdino_results}\n",
    "    clip_images = {result['image_path'].name: result for result in clip_results}\n",
    "    common_images = set(gdino_images.keys()) & set(clip_images.keys())\n",
    "\n",
    "    copied_files = {'grounding_dino': [], 'clip': [], 'both': []}\n",
    "\n",
    "    # Copy GroundingDINO results\n",
    "    for i, result in enumerate(gdino_results):\n",
    "        try:\n",
    "            source_path = result['image_path']\n",
    "            max_score = float(result['scores'].max()) if len(result['scores']) > 0 else 0.0\n",
    "            filename = f\"{i+1:03d}_{source_path.stem}_gdino{max_score:.2f}{source_path.suffix}\"\n",
    "\n",
    "            if source_path.name in common_images:\n",
    "                dest_path = both_folder / filename\n",
    "            else:\n",
    "                dest_path = gdino_folder / filename\n",
    "\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            copied_files['grounding_dino'].append(dest_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying GroundingDINO result: {e}\")\n",
    "\n",
    "    # Copy CLIP results\n",
    "    for i, result in enumerate(clip_results):\n",
    "        try:\n",
    "            source_path = result['image_path']\n",
    "            max_score = float(result['scores'].max()) if len(result['scores']) > 0 else 0.0\n",
    "            filename = f\"{i+1:03d}_{source_path.stem}_clip{max_score:.2f}{source_path.suffix}\"\n",
    "\n",
    "            if source_path.name not in common_images:  # Only copy if not already in both folder\n",
    "                dest_path = clip_folder / filename\n",
    "                shutil.copy2(source_path, dest_path)\n",
    "                copied_files['clip'].append(dest_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying CLIP result: {e}\")\n",
    "\n",
    "    total_copied = len(copied_files['grounding_dino']) + len(copied_files['clip'])\n",
    "\n",
    "    print(f\"📋 Results saved to: {search_folder}\")\n",
    "    print(f\"   🎯 GroundingDINO: {len(copied_files['grounding_dino'])} files\")\n",
    "    print(f\"   🔍 CLIP: {len(copied_files['clip'])} files\")\n",
    "    print(f\"   🤝 Both models: {len(common_images)} files\")\n",
    "\n",
    "    return search_folder, copied_files\n",
    "\n",
    "# Cell 5: Interactive Combined Interface\n",
    "def create_combined_interface():\n",
    "    \"\"\"Create interface for combined model search\"\"\"\n",
    "\n",
    "    # Widgets\n",
    "    grounding_dino_selector = widgets.Dropdown(\n",
    "        options=[(f\"GroundingDINO {key}\", val) for key, val in AVAILABLE_MODELS['grounding_dino'].items()],\n",
    "        value=CONFIG['GROUNDING_DINO_MODEL'],\n",
    "        description='GroundingDINO:',\n",
    "        layout=widgets.Layout(width='350px')\n",
    "    )\n",
    "\n",
    "    clip_selector = widgets.Dropdown(\n",
    "        options=[(f\"CLIP {key}\", val) for key, val in AVAILABLE_MODELS['clip'].items()],\n",
    "        value=CONFIG['CLIP_MODEL'],\n",
    "        description='CLIP:',\n",
    "        layout=widgets.Layout(width='350px')\n",
    "    )\n",
    "\n",
    "    query_input = widgets.Text(\n",
    "        value='person with weapon',\n",
    "        placeholder='Enter search query...',\n",
    "        description='Query:',\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "\n",
    "    gdino_confidence_slider = widgets.FloatSlider(\n",
    "        value=CONFIG['GROUNDING_DINO_CONFIDENCE'],\n",
    "        min=0.1, max=0.9, step=0.05,\n",
    "        description='GroundingDINO Confidence:',\n",
    "        readout_format='.2f',\n",
    "        layout=widgets.Layout(width='300px')\n",
    "    )\n",
    "\n",
    "    clip_threshold_slider = widgets.FloatSlider(\n",
    "        value=CONFIG['CLIP_SIMILARITY_THRESHOLD'],\n",
    "        min=0.1, max=0.8, step=0.05,\n",
    "        description='CLIP Threshold:',\n",
    "        readout_format='.2f',\n",
    "        layout=widgets.Layout(width='300px')\n",
    "    )\n",
    "\n",
    "    comparison_mode_checkbox = widgets.Checkbox(\n",
    "        value=CONFIG['COMPARISON_MODE'],\n",
    "        description='Comparison Mode',\n",
    "        indent=False\n",
    "    )\n",
    "\n",
    "    search_button = widgets.Button(\n",
    "        description='🔍 Search Both',\n",
    "        button_style='primary',\n",
    "        layout=widgets.Layout(width='140px')\n",
    "    )\n",
    "\n",
    "    copy_button = widgets.Button(\n",
    "        description='📋 Copy Results',\n",
    "        button_style='success',\n",
    "        layout=widgets.Layout(width='140px'),\n",
    "        disabled=True\n",
    "    )\n",
    "\n",
    "    reload_button = widgets.Button(\n",
    "        description='🔄 Reload Models',\n",
    "        button_style='info',\n",
    "        layout=widgets.Layout(width='140px')\n",
    "    )\n",
    "\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    # State\n",
    "    search_results = {}\n",
    "\n",
    "    def on_reload_models(b):\n",
    "        with output_area:\n",
    "            print(\"🔄 Reloading models...\")\n",
    "            CONFIG['GROUNDING_DINO_MODEL'] = grounding_dino_selector.value\n",
    "            CONFIG['CLIP_MODEL'] = clip_selector.value\n",
    "\n",
    "            success = combined_models.load_both_models()\n",
    "            if success:\n",
    "                print(\"✅ Models reloaded successfully!\")\n",
    "            else:\n",
    "                print(\"❌ Failed to reload models\")\n",
    "\n",
    "    def on_search(b):\n",
    "        nonlocal search_results\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            if not combined_models.grounding_dino_model and not combined_models.clip_model:\n",
    "                print(\"❌ No models loaded\")\n",
    "                return\n",
    "\n",
    "            query = query_input.value.strip()\n",
    "            if not query:\n",
    "                print(\"⚠️ Enter a query\")\n",
    "                return\n",
    "\n",
    "            # Update config\n",
    "            CONFIG['GROUNDING_DINO_CONFIDENCE'] = gdino_confidence_slider.value\n",
    "            CONFIG['CLIP_SIMILARITY_THRESHOLD'] = clip_threshold_slider.value\n",
    "            CONFIG['COMPARISON_MODE'] = comparison_mode_checkbox.value\n",
    "\n",
    "            # Search\n",
    "            search_results = search_with_both_models(\n",
    "                query, CONFIG['SUSPECTS_GALLERY_PATH'], CONFIG['COMPARISON_MODE']\n",
    "            )\n",
    "\n",
    "            if search_results:\n",
    "                copy_button.disabled = False\n",
    "\n",
    "                # Show comparison\n",
    "                comparison = compare_model_results(search_results)\n",
    "\n",
    "                # Display results\n",
    "                display_combined_results(search_results)\n",
    "            else:\n",
    "                print(\"🔍 No matches found with either model\")\n",
    "                copy_button.disabled = True\n",
    "\n",
    "    def on_copy(b):\n",
    "        with output_area:\n",
    "            if search_results:\n",
    "                folder, files = copy_combined_results(search_results, CONFIG['RESULTS_OUTPUT_PATH'])\n",
    "                print(f\"✅ Combined results saved!\")\n",
    "            else:\n",
    "                print(\"⚠️ No results to copy\")\n",
    "\n",
    "    # Connect events\n",
    "    reload_button.on_click(on_reload_models)\n",
    "    search_button.on_click(on_search)\n",
    "    copy_button.on_click(on_copy)\n",
    "\n",
    "    # Layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>🚀 Combined GroundingDINO + CLIP Forensic Search</h3>\"),\n",
    "        widgets.HBox([grounding_dino_selector, clip_selector]),\n",
    "        query_input,\n",
    "        widgets.HBox([gdino_confidence_slider, clip_threshold_slider]),\n",
    "        widgets.HBox([comparison_mode_checkbox]),\n",
    "        widgets.HBox([search_button, copy_button, reload_button]),\n",
    "        widgets.HTML(\"<hr>\")\n",
    "    ])\n",
    "\n",
    "    return widgets.VBox([controls, output_area])\n",
    "\n",
    "def display_combined_results(results_dict):\n",
    "    \"\"\"Display results from both models side by side\"\"\"\n",
    "    gdino_results = results_dict.get('grounding_dino', [])\n",
    "    clip_results = results_dict.get('clip', [])\n",
    "\n",
    "    max_display = CONFIG['MAX_RESULTS_DISPLAY']\n",
    "    gdino_display = gdino_results[:max_display//2]\n",
    "    clip_display = clip_results[:max_display//2]\n",
    "\n",
    "    if not gdino_display and not clip_display:\n",
    "        return\n",
    "\n",
    "    # Create figure with subplots\n",
    "    total_results = len(gdino_display) + len(clip_display)\n",
    "    cols = 2\n",
    "    rows = max(len(gdino_display), len(clip_display))\n",
    "\n",
    "    if rows == 0:\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    # Plot GroundingDINO results\n",
    "    for i in range(rows):\n",
    "        ax = axes[i, 0]\n",
    "\n",
    "        if i < len(gdino_display):\n",
    "            result = gdino_display[i]\n",
    "            ax.imshow(result['image'])\n",
    "\n",
    "            # Draw bounding boxes\n",
    "            for box, score in zip(result['boxes'], result['scores']):\n",
    "                x1, y1, x2, y2 = box.tolist()\n",
    "                rect = patches.Rectangle(\n",
    "                    (x1, y1), x2 - x1, y2 - y1,\n",
    "                    linewidth=2, edgecolor='blue', facecolor='none'\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(x1, y1 - 5, f'{score:.2f}',\n",
    "                       color='blue', fontweight='bold', fontsize=9)\n",
    "\n",
    "            ax.set_title(f\"GroundingDINO: {result['image_path'].name}\\nMatches: {len(result['boxes'])}\",\n",
    "                        fontsize=10, color='blue')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No more\\nGroundingDINO\\nresults',\n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Plot CLIP results\n",
    "    for i in range(rows):\n",
    "        ax = axes[i, 1]\n",
    "\n",
    "        if i < len(clip_display):\n",
    "            result = clip_display[i]\n",
    "            ax.imshow(result['image'])\n",
    "\n",
    "            # Draw bounding boxes\n",
    "            for box, score in zip(result['boxes'], result['scores']):\n",
    "                x1, y1, x2, y2 = box.tolist()\n",
    "                rect = patches.Rectangle(\n",
    "                    (x1, y1), x2 - x1, y2 - y1,\n",
    "                    linewidth=2, edgecolor='red', facecolor='none'\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(x1, y1 - 5, f'{score:.2f}',\n",
    "                       color='red', fontweight='bold', fontsize=9)\n",
    "\n",
    "            ax.set_title(f\"CLIP: {result['image_path'].name}\\nMatches: {len(result['boxes'])}\",\n",
    "                        fontsize=10, color='red')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No more\\nCLIP\\nresults',\n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Combined Results: GroundingDINO (Blue) vs CLIP (Red)\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display interface\n",
    "interface = create_combined_interface()\n",
    "display(interface)\n",
    "\n",
    "# Cell 6: Batch Analysis with Both Models\n",
    "def run_combined_batch_analysis(custom_queries=None):\n",
    "    \"\"\"Run batch analysis with both models\"\"\"\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Default forensic queries\n",
    "    default_queries = [\n",
    "        \"person with weapon\",\n",
    "        \"weapon\",\n",
    "        \"gun\",\n",
    "        \"knife\",\n",
    "        \"suspicious person\",\n",
    "        \"person running\",\n",
    "        \"vehicle\",\n",
    "        \"mask\",\n",
    "        \"backpack\",\n",
    "        \"dark clothing\"\n",
    "    ]\n",
    "\n",
    "    queries = default_queries + (custom_queries or [])\n",
    "\n",
    "    print(f\"🚀 Starting combined batch analysis with {len(queries)} queries\")\n",
    "    batch_start = time.time()\n",
    "    batch_start_datetime = datetime.now()\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\n[{i}/{len(queries)}] Processing: '{query}'\")\n",
    "\n",
    "        query_start = time.time()\n",
    "        results = search_with_both_models(query, CONFIG['SUSPECTS_GALLERY_PATH'])\n",
    "        query_time = time.time() - query_start\n",
    "\n",
    "        # Analyze results\n",
    "        gdino_count = len(results.get('grounding_dino', []))\n",
    "        clip_count = len(results.get('clip', []))\n",
    "\n",
    "        all_results[query] = {\n",
    "            'results': results,\n",
    "            'grounding_dino_count': gdino_count,\n",
    "            'clip_count': clip_count,\n",
    "            'time': query_time\n",
    "        }\n",
    "\n",
    "        # Save results if any found\n",
    "        if gdino_count > 0 or clip_count > 0:\n",
    "            folder, files = copy_combined_results(results, CONFIG['RESULTS_OUTPUT_PATH'])\n",
    "            print(f\"📁 Saved to: {folder.name}\")\n",
    "\n",
    "    # Calculate totals\n",
    "    batch_time = time.time() - batch_start\n",
    "    batch_end_datetime = datetime.now()\n",
    "\n",
    "    total_gdino_matches = sum(data['grounding_dino_count'] for data in all_results.values())\n",
    "    total_clip_matches = sum(data['clip_count'] for data in all_results.values())\n",
    "\n",
    "    # Format duration\n",
    "    if batch_time >= 3600:\n",
    "        duration_str = f\"{int(batch_time//3600)}h {int((batch_time%3600)//60)}m {batch_time%60:.1f}s\"\n",
    "    elif batch_time >= 60:\n",
    "        duration_str = f\"{int(batch_time//60)}m {batch_time%60:.1f}s\"\n",
    "    else:\n",
    "        duration_str = f\"{batch_time:.1f}s\"\n",
    "\n",
    "    # Print comprehensive summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"📊 COMBINED BATCH ANALYSIS SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    print(f\"⏰ Start time: {batch_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"🏁 End time: {batch_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"⏱️ Total duration: {duration_str}\")\n",
    "    print(f\"📊 Queries processed: {len(queries)}\")\n",
    "    print(f\"📈 Average per query: {batch_time/len(queries):.1f}s\")\n",
    "\n",
    "    print(f\"\\n🎯 MODEL PERFORMANCE:\")\n",
    "    print(f\"   GroundingDINO total matches: {total_gdino_matches}\")\n",
    "    print(f\"   CLIP total matches: {total_clip_matches}\")\n",
    "\n",
    "    print(f\"\\n📋 DETAILED RESULTS:\")\n",
    "    print(f\"{'Query':<25} {'GroundingDINO':<12} {'CLIP':<8} {'Time':<8}\")\n",
    "    print(f\"-\" * 60)\n",
    "\n",
    "    for query, data in all_results.items():\n",
    "        gdino_status = \"✅\" if data['grounding_dino_count'] > 0 else \"⚪\"\n",
    "        clip_status = \"✅\" if data['clip_count'] > 0 else \"⚪\"\n",
    "\n",
    "        print(f\"{query[:24]:<25} {gdino_status} {data['grounding_dino_count']:<10} {clip_status} {data['clip_count']:<6} {data['time']:.1f}s\")\n",
    "\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def analyze_model_agreement(batch_results):\n",
    "    \"\"\"Analyze agreement between models across all queries\"\"\"\n",
    "    print(f\"\\n📊 MODEL AGREEMENT ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    agreement_stats = {\n",
    "        'both_found': 0,\n",
    "        'grounding_dino_only': 0,\n",
    "        'clip_only': 0,\n",
    "        'neither_found': 0\n",
    "    }\n",
    "\n",
    "    for query, data in batch_results.items():\n",
    "        gdino_count = data['grounding_dino_count']\n",
    "        clip_count = data['clip_count']\n",
    "\n",
    "        if gdino_count > 0 and clip_count > 0:\n",
    "            agreement_stats['both_found'] += 1\n",
    "        elif gdino_count > 0:\n",
    "            agreement_stats['grounding_dino_only'] += 1\n",
    "        elif clip_count > 0:\n",
    "            agreement_stats['clip_only'] += 1\n",
    "        else:\n",
    "            agreement_stats['neither_found'] += 1\n",
    "\n",
    "    total_queries = len(batch_results)\n",
    "\n",
    "    print(f\"🤝 Both models found matches: {agreement_stats['both_found']}/{total_queries} ({agreement_stats['both_found']/total_queries*100:.1f}%)\")\n",
    "    print(f\"🎯 GroundingDINO only: {agreement_stats['grounding_dino_only']}/{total_queries} ({agreement_stats['grounding_dino_only']/total_queries*100:.1f}%)\")\n",
    "    print(f\"🔍 CLIP only: {agreement_stats['clip_only']}/{total_queries} ({agreement_stats['clip_only']/total_queries*100:.1f}%)\")\n",
    "    print(f\"❌ Neither found matches: {agreement_stats['neither_found']}/{total_queries} ({agreement_stats['neither_found']/total_queries*100:.1f}%)\")\n",
    "\n",
    "    # Model effectiveness\n",
    "    gdino_effective = agreement_stats['both_found'] + agreement_stats['grounding_dino_only']\n",
    "    clip_effective = agreement_stats['both_found'] + agreement_stats['clip_only']\n",
    "\n",
    "    print(f\"\\n📈 MODEL EFFECTIVENESS:\")\n",
    "    print(f\"🎯 GroundingDINO effective queries: {gdino_effective}/{total_queries} ({gdino_effective/total_queries*100:.1f}%)\")\n",
    "    print(f\"🔍 CLIP effective queries: {clip_effective}/{total_queries} ({clip_effective/total_queries*100:.1f}%)\")\n",
    "\n",
    "    return agreement_stats\n",
    "\n",
    "def generate_forensic_report(batch_results, save_to_file=True):\n",
    "    \"\"\"Generate a comprehensive forensic analysis report\"\"\"\n",
    "    from datetime import datetime\n",
    "\n",
    "    report_content = []\n",
    "    report_content.append(\"=\"*80)\n",
    "    report_content.append(\"FORENSIC IMAGE ANALYSIS REPORT\")\n",
    "    report_content.append(\"Combined GroundingDINO + CLIP Analysis\")\n",
    "    report_content.append(\"=\"*80)\n",
    "    report_content.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report_content.append(\"\")\n",
    "\n",
    "    # Executive Summary\n",
    "    total_queries = len(batch_results)\n",
    "    total_gdino = sum(data['grounding_dino_count'] for data in batch_results.values())\n",
    "    total_clip = sum(data['clip_count'] for data in batch_results.values())\n",
    "    total_time = sum(data['time'] for data in batch_results.values())\n",
    "\n",
    "    report_content.append(\"EXECUTIVE SUMMARY\")\n",
    "    report_content.append(\"-\" * 20)\n",
    "    report_content.append(f\"Total queries processed: {total_queries}\")\n",
    "    report_content.append(f\"Total processing time: {total_time/60:.1f} minutes\")\n",
    "    report_content.append(f\"GroundingDINO total detections: {total_gdino}\")\n",
    "    report_content.append(f\"CLIP total detections: {total_clip}\")\n",
    "    report_content.append(f\"Combined unique findings: {total_gdino + total_clip}\")\n",
    "    report_content.append(\"\")\n",
    "\n",
    "    # Detailed Results\n",
    "    report_content.append(\"DETAILED FINDINGS\")\n",
    "    report_content.append(\"-\" * 20)\n",
    "\n",
    "    for query, data in batch_results.items():\n",
    "        report_content.append(f\"Query: '{query}'\")\n",
    "        report_content.append(f\"  GroundingDINO: {data['grounding_dino_count']} detections\")\n",
    "        report_content.append(f\"  CLIP: {data['clip_count']} detections\")\n",
    "        report_content.append(f\"  Processing time: {data['time']:.1f}s\")\n",
    "        report_content.append(\"\")\n",
    "\n",
    "    # Model Analysis\n",
    "    agreement_stats = analyze_model_agreement(batch_results)\n",
    "    report_content.append(\"MODEL AGREEMENT ANALYSIS\")\n",
    "    report_content.append(\"-\" * 25)\n",
    "    report_content.append(f\"Queries where both models found matches: {agreement_stats['both_found']}\")\n",
    "    report_content.append(f\"Queries where only GroundingDINO found matches: {agreement_stats['grounding_dino_only']}\")\n",
    "    report_content.append(f\"Queries where only CLIP found matches: {agreement_stats['clip_only']}\")\n",
    "    report_content.append(f\"Queries where neither found matches: {agreement_stats['neither_found']}\")\n",
    "    report_content.append(\"\")\n",
    "\n",
    "    # Recommendations\n",
    "    report_content.append(\"RECOMMENDATIONS\")\n",
    "    report_content.append(\"-\" * 15)\n",
    "    if agreement_stats['both_found'] > 0:\n",
    "        report_content.append(\"• High-confidence detections where both models agree should be prioritized for review\")\n",
    "    if agreement_stats['grounding_dino_only'] > agreement_stats['clip_only']:\n",
    "        report_content.append(\"• GroundingDINO showed superior performance for this dataset\")\n",
    "    elif agreement_stats['clip_only'] > agreement_stats['grounding_dino_only']:\n",
    "        report_content.append(\"• CLIP showed superior performance for this dataset\")\n",
    "    else:\n",
    "        report_content.append(\"• Both models showed comparable performance - use both for comprehensive analysis\")\n",
    "\n",
    "    report_content.append(\"• Review all detections manually for final verification\")\n",
    "    report_content.append(\"• Consider adjusting confidence thresholds based on results\")\n",
    "    report_content.append(\"\")\n",
    "    report_content.append(\"=\"*80)\n",
    "\n",
    "    report_text = \"\\n\".join(report_content)\n",
    "\n",
    "    # Save to file if requested\n",
    "    if save_to_file:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        report_path = Path(CONFIG['RESULTS_OUTPUT_PATH']) / f\"forensic_report_{timestamp}.txt\"\n",
    "\n",
    "        try:\n",
    "            with open(report_path, 'w') as f:\n",
    "                f.write(report_text)\n",
    "            print(f\"📄 Report saved to: {report_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error saving report: {e}\")\n",
    "\n",
    "    print(\"\\n\" + report_text)\n",
    "    return report_text\n",
    "\n",
    "# Cell 7: Usage Guide and Examples\n",
    "print(\"\"\"\n",
    "🎯 COMBINED GROUNDINGDINO + CLIP FORENSIC SEARCH\n",
    "==============================================\n",
    "\n",
    "🚀 FEATURES:\n",
    "• Dual-model approach for comprehensive detection\n",
    "• Side-by-side result comparison\n",
    "• Agreement analysis between models\n",
    "• Organized output with model-specific folders\n",
    "• Comprehensive forensic reporting\n",
    "\n",
    "📋 SETUP:\n",
    "1. Place images in './suspects_gallery' folder\n",
    "2. Run cells 1-5 in order\n",
    "3. Use interface above for interactive search\n",
    "4. Check both model results and agreements\n",
    "\n",
    "🔍 MODEL COMPARISON:\n",
    "• GroundingDINO: Better for complex language understanding\n",
    "• CLIP: Better for general object recognition\n",
    "• Combined: Maximum coverage and confidence validation\n",
    "\n",
    "🎛️ PARAMETERS:\n",
    "• GroundingDINO Confidence: 0.1-0.9 (default 0.35)\n",
    "• CLIP Similarity: 0.1-0.8 (default 0.25)\n",
    "• Comparison Mode: Shows both models side-by-side\n",
    "\n",
    "📊 OUTPUT STRUCTURE:\n",
    "search_results/\n",
    "├── combined_search_20240611_143022/\n",
    "│   ├── grounding_dino/          # GroundingDINO unique results\n",
    "│   ├── clip/                    # CLIP unique results\n",
    "│   ├── both_models/             # Images detected by both\n",
    "│   └── forensic_report_*.txt    # Analysis report\n",
    "\n",
    "🔍 RECOMMENDED QUERIES:\n",
    "• \"person with weapon\" / \"weapon\" / \"gun\" / \"knife\"\n",
    "• \"suspicious person\" / \"person running\"\n",
    "• \"vehicle\" / \"suspicious vehicle\"\n",
    "• \"mask\" / \"person wearing mask\"\n",
    "• \"backpack\" / \"bag\"\n",
    "\n",
    "💡 BEST PRACTICES:\n",
    "• Use both models for critical analysis\n",
    "• Higher confidence when both models agree\n",
    "• Review model-specific detections manually\n",
    "• Adjust thresholds based on dataset characteristics\n",
    "• Generate reports for documentation\n",
    "\n",
    "🚨 INTERPRETATION:\n",
    "• Blue boxes: GroundingDINO detections\n",
    "• Red boxes: CLIP detections\n",
    "• Both models agreeing = higher confidence\n",
    "• Model-specific detections may reveal different aspects\n",
    "\n",
    "# Example usage:\n",
    "# batch_results = run_combined_batch_analysis()\n",
    "# report = generate_forensic_report(batch_results)\n",
    "\"\"\")\n",
    "\n",
    "# Additional utility functions\n",
    "def quick_model_test(query=\"person with weapon\"):\n",
    "    \"\"\"Quick test of both models\"\"\"\n",
    "    print(f\"🧪 Quick test with query: '{query}'\")\n",
    "\n",
    "    if not models_loaded:\n",
    "        print(\"❌ Models not loaded properly\")\n",
    "        return\n",
    "\n",
    "    results = search_with_both_models(query, CONFIG['SUSPECTS_GALLERY_PATH'])\n",
    "\n",
    "    if results:\n",
    "        comparison = compare_model_results(results)\n",
    "        print(\"✅ Quick test completed - check results above\")\n",
    "        return results\n",
    "    else:\n",
    "        print(\"⚪ No results found in quick test\")\n",
    "        return None\n",
    "\n",
    "def model_performance_summary():\n",
    "    \"\"\"Show current model status and performance info\"\"\"\n",
    "    print(\"📊 MODEL STATUS SUMMARY\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"🎯 GroundingDINO: {'✅ Loaded' if combined_models.grounding_dino_model else '❌ Not loaded'}\")\n",
    "    print(f\"   Model: {CONFIG['GROUNDING_DINO_MODEL']}\")\n",
    "    print(f\"   Confidence threshold: {CONFIG['GROUNDING_DINO_CONFIDENCE']}\")\n",
    "\n",
    "    print(f\"🔍 CLIP: {'✅ Loaded' if combined_models.clip_model else '❌ Not loaded'}\")\n",
    "    print(f\"   Model: {CONFIG['CLIP_MODEL']}\")\n",
    "    print(f\"   Similarity threshold: {CONFIG['CLIP_SIMILARITY_THRESHOLD']}\")\n",
    "\n",
    "    print(f\"🖥️ Device: {CONFIG['DEVICE']}\")\n",
    "    print(f\"📁 Gallery: {CONFIG['SUSPECTS_GALLERY_PATH']}\")\n",
    "    print(f\"📋 Output: {CONFIG['RESULTS_OUTPUT_PATH']}\")\n",
    "\n",
    "# Show current status\n",
    "model_performance_summary()\n",
    "\n",
    "# Uncomment to run batch analysis:\n",
    "# batch_results = run_combined_batch_analysis()\n",
    "# analysis_report = generate_forensic_report(batch_results, save_to_file=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded successfully\n",
      "📁 Suspects gallery: ../../datasets/images/objects/raw\n",
      "📁 Results output: ../../datasets/images/objects/detections\n",
      "🎯 GroundingDINO: IDEA-Research/grounding-dino-base\n",
      "🔍 CLIP: ViT-B/32\n",
      "🚀 Combined model approach for comprehensive analysis\n",
      "✅ Dependencies already installed\n",
      "✅ All dependencies imported successfully\n",
      "🖥️ Using CPU mode\n",
      "📍 Device: cpu\n",
      "📁 Directories ready\n",
      "🚀 Loading both models...\n",
      "📥 Loading GroundingDINO: IDEA-Research/grounding-dino-base\n",
      "✅ GroundingDINO loaded successfully\n",
      "📥 Loading CLIP: ViT-B/32\n",
      "✅ CLIP loaded successfully\n",
      "✅ Both models loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h3>🚀 Combined GroundingDINO + CLIP Forensic Search</h3>'), HBox(chi…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58ebb03648da4f2f8dec5e308f89c0e8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 COMBINED GROUNDINGDINO + CLIP FORENSIC SEARCH\n",
      "==============================================\n",
      "\n",
      "🚀 FEATURES:\n",
      "• Dual-model approach for comprehensive detection\n",
      "• Side-by-side result comparison\n",
      "• Agreement analysis between models\n",
      "• Organized output with model-specific folders\n",
      "• Comprehensive forensic reporting\n",
      "\n",
      "📋 SETUP:\n",
      "1. Place images in './suspects_gallery' folder\n",
      "2. Run cells 1-5 in order\n",
      "3. Use interface above for interactive search\n",
      "4. Check both model results and agreements\n",
      "\n",
      "🔍 MODEL COMPARISON:\n",
      "• GroundingDINO: Better for complex language understanding\n",
      "• CLIP: Better for general object recognition\n",
      "• Combined: Maximum coverage and confidence validation\n",
      "\n",
      "🎛️ PARAMETERS:\n",
      "• GroundingDINO Confidence: 0.1-0.9 (default 0.35)\n",
      "• CLIP Similarity: 0.1-0.8 (default 0.25)\n",
      "• Comparison Mode: Shows both models side-by-side\n",
      "\n",
      "📊 OUTPUT STRUCTURE:\n",
      "search_results/\n",
      "├── combined_search_20240611_143022/\n",
      "│   ├── grounding_dino/          # GroundingDINO unique results\n",
      "│   ├── clip/                    # CLIP unique results\n",
      "│   ├── both_models/             # Images detected by both\n",
      "│   └── forensic_report_*.txt    # Analysis report\n",
      "\n",
      "🔍 RECOMMENDED QUERIES:\n",
      "• \"person with weapon\" / \"weapon\" / \"gun\" / \"knife\"\n",
      "• \"suspicious person\" / \"person running\"\n",
      "• \"vehicle\" / \"suspicious vehicle\"\n",
      "• \"mask\" / \"person wearing mask\"\n",
      "• \"backpack\" / \"bag\"\n",
      "\n",
      "💡 BEST PRACTICES:\n",
      "• Use both models for critical analysis\n",
      "• Higher confidence when both models agree\n",
      "• Review model-specific detections manually\n",
      "• Adjust thresholds based on dataset characteristics\n",
      "• Generate reports for documentation\n",
      "\n",
      "🚨 INTERPRETATION:\n",
      "• Blue boxes: GroundingDINO detections\n",
      "• Red boxes: CLIP detections\n",
      "• Both models agreeing = higher confidence\n",
      "• Model-specific detections may reveal different aspects\n",
      "\n",
      "# Example usage:\n",
      "# batch_results = run_combined_batch_analysis()\n",
      "# report = generate_forensic_report(batch_results)\n",
      "\n",
      "📊 MODEL STATUS SUMMARY\n",
      "------------------------------\n",
      "🎯 GroundingDINO: ✅ Loaded\n",
      "   Model: IDEA-Research/grounding-dino-base\n",
      "   Confidence threshold: 0.35\n",
      "🔍 CLIP: ✅ Loaded\n",
      "   Model: ViT-B/32\n",
      "   Similarity threshold: 0.25\n",
      "🖥️ Device: cpu\n",
      "📁 Gallery: ../../datasets/images/objects/raw\n",
      "📋 Output: ../../datasets/images/objects/detections\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
