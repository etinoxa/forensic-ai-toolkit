{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-12T04:16:13.285251Z",
     "start_time": "2025-06-12T04:15:19.342004Z"
    }
   },
   "source": [
    "# Cell 1: Configuration and Setup\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Configuration Settings\n",
    "CONFIG = {\n",
    "    # Model Selection (Hugging Face OWL-ViT)\n",
    "    'MODEL_NAME': 'google/owlvit-base-patch32',  # Options: owlvit-base-patch32, owlvit-base-patch16, owlvit-large-patch14\n",
    "    'DEVICE': 'auto',  # 'auto', 'cpu', 'cuda'\n",
    "\n",
    "    # Paths\n",
    "    'SUSPECTS_GALLERY_PATH': '../../datasets/images/objects/raw',  # Input folder with suspect images\n",
    "    'RESULTS_OUTPUT_PATH': '../../datasets/images/objects/detections',      # Output folder for matched images\n",
    "\n",
    "    # Detection parameters\n",
    "    'CONFIDENCE_THRESHOLD': 0.1,   # Lower threshold for OWL-ViT (typically uses lower values)\n",
    "    'DETECTION_THRESHOLD': 0.1,    # Detection threshold for OWL-ViT\n",
    "    'NMS_THRESHOLD': 0.3,          # Non-maximum suppression threshold\n",
    "\n",
    "    # Processing settings\n",
    "    'BATCH_SIZE': 4,               # Default batch size for processing\n",
    "    'MAX_RESULTS_DISPLAY': 10,     # Maximum results to display at once\n",
    "    'FIGURE_SIZE': (12, 8),        # Size of result visualization\n",
    "}\n",
    "\n",
    "# Available OWL-ViT model options\n",
    "AVAILABLE_MODELS = {\n",
    "    'owlvit-base-patch32': {\n",
    "        'name': 'OWL-ViT Base Patch32',\n",
    "        'model_id': 'google/owlvit-base-patch32',\n",
    "        'description': 'Fastest OWL-ViT model - good balance of speed and accuracy',\n",
    "        'performance': 'Base performance, fastest inference'\n",
    "    },\n",
    "    'owlvit-base-patch16': {\n",
    "        'name': 'OWL-ViT Base Patch16',\n",
    "        'model_id': 'google/owlvit-base-patch16',\n",
    "        'description': 'Higher resolution features - better accuracy, slower',\n",
    "        'performance': 'Better accuracy, moderate speed'\n",
    "    },\n",
    "    'owlvit-large-patch14': {\n",
    "        'name': 'OWL-ViT Large Patch14',\n",
    "        'model_id': 'google/owlvit-large-patch14',\n",
    "        'description': 'Best accuracy - largest model, slowest inference',\n",
    "        'performance': 'Best accuracy, slowest speed'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ Configuration loaded successfully\")\n",
    "print(f\"📁 Suspects gallery: {CONFIG['SUSPECTS_GALLERY_PATH']}\")\n",
    "print(f\"📁 Results output: {CONFIG['RESULTS_OUTPUT_PATH']}\")\n",
    "print(f\"🦉 Selected model: {CONFIG['MODEL_NAME']}\")\n",
    "\n",
    "# Cell 2: Install and Import Dependencies\n",
    "# Run this cell first to install required packages\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"✅ Transformers already installed\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ Installing required packages...\")\n",
    "    !pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
    "    !pip install transformers\n",
    "    !pip install ipywidgets\n",
    "    !pip install Pillow\n",
    "    !pip install matplotlib\n",
    "    !pip install opencv-python\n",
    "    print(\"📦 Installation complete\")\n",
    "\n",
    "# Import required libraries\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    print(\"✅ All dependencies imported successfully\")\n",
    "\n",
    "    # Check PyTorch device compatibility\n",
    "    print(f\"🔧 PyTorch version: {torch.__version__}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"🚀 CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        default_device = \"cuda\"\n",
    "    else:\n",
    "        print(\"🖥️ Using CPU mode (CUDA not available)\")\n",
    "        default_device = \"cpu\"\n",
    "\n",
    "    # Update config with detected device\n",
    "    if CONFIG['DEVICE'] == 'auto':\n",
    "        CONFIG['DEVICE'] = default_device\n",
    "        print(f\"📍 Auto-detected device: {CONFIG['DEVICE']}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"🔧 Troubleshooting steps:\")\n",
    "    print(\"1. Restart kernel and run this cell again\")\n",
    "    print(\"2. Check if all packages installed correctly\")\n",
    "\n",
    "# Cell 3: Initialize Model and Directories\n",
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories if they don't exist\"\"\"\n",
    "    os.makedirs(CONFIG['SUSPECTS_GALLERY_PATH'], exist_ok=True)\n",
    "    os.makedirs(CONFIG['RESULTS_OUTPUT_PATH'], exist_ok=True)\n",
    "    print(f\"📁 Created directories: {CONFIG['SUSPECTS_GALLERY_PATH']}, {CONFIG['RESULTS_OUTPUT_PATH']}\")\n",
    "\n",
    "def load_owlvit_model(model_name=None):\n",
    "    \"\"\"Load OWL-ViT model from Hugging Face\"\"\"\n",
    "    try:\n",
    "        # Use provided model name or default from config\n",
    "        if model_name is None:\n",
    "            model_name = CONFIG['MODEL_NAME']\n",
    "\n",
    "        device = CONFIG['DEVICE']\n",
    "        print(f\"📥 Loading OWL-ViT model: {model_name}\")\n",
    "        print(f\"🖥️ Target device: {device}\")\n",
    "\n",
    "        # Load processor and model\n",
    "        print(\"⏳ Loading processor...\")\n",
    "        processor = OwlViTProcessor.from_pretrained(model_name)\n",
    "\n",
    "        print(\"⏳ Loading model weights...\")\n",
    "        model = OwlViTForObjectDetection.from_pretrained(model_name)\n",
    "\n",
    "        # Move to device\n",
    "        print(f\"📍 Moving model to {device}...\")\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        print(f\"✅ Model loaded successfully!\")\n",
    "        print(f\"   🦉 Model: {model_name}\")\n",
    "        print(f\"   🖥️ Device: {device}\")\n",
    "\n",
    "        return model, processor, device, model_name\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model: {e}\")\n",
    "        print(\"\\n🔧 Troubleshooting steps:\")\n",
    "        print(\"1. Check internet connection (models download from Hugging Face)\")\n",
    "        print(\"2. Verify model name is correct\")\n",
    "        print(\"3. Try switching to 'owlvit-base-patch32' for faster loading\")\n",
    "        print(\"4. Restart kernel if memory issues occur\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def switch_model(model_key):\n",
    "    \"\"\"Switch to a different OWL-ViT model variant\"\"\"\n",
    "    if model_key in AVAILABLE_MODELS:\n",
    "        CONFIG['MODEL_NAME'] = AVAILABLE_MODELS[model_key]['model_id']\n",
    "        print(f\"🔄 Switched to: {AVAILABLE_MODELS[model_key]['name']}\")\n",
    "        return load_owlvit_model()\n",
    "    else:\n",
    "        print(f\"❌ Unknown model: {model_key}\")\n",
    "        print(f\"Available models: {list(AVAILABLE_MODELS.keys())}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Initialize\n",
    "setup_directories()\n",
    "model, processor, device, model_name = load_owlvit_model()\n",
    "\n",
    "# Cell 4: Core Search Functions\n",
    "def process_image_batch(image_paths, model, processor, query, device, batch_size=4):\n",
    "    \"\"\"Process a batch of images efficiently using OWL-ViT\"\"\"\n",
    "    batch_results = []\n",
    "\n",
    "    # Process images one by one to avoid memory issues\n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        try:\n",
    "            # Progress indicator\n",
    "            if i % 5 == 0:\n",
    "                print(f\"Processing {i+1}/{len(image_paths)}: {img_path.name[:30]}...\", end='\\r')\n",
    "\n",
    "            # Load image\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            # Prepare text queries for OWL-ViT (can handle multiple queries)\n",
    "            # OWL-ViT expects a list of text queries\n",
    "            text_queries = [query.strip()]\n",
    "\n",
    "            # Process inputs\n",
    "            inputs = processor(text=text_queries, images=image, return_tensors=\"pt\")\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Post-process results\n",
    "            # OWL-ViT outputs: logits, bbox predictions\n",
    "            target_sizes = torch.Tensor([image.size[::-1]])  # (height, width)\n",
    "            results = processor.post_process_object_detection(\n",
    "                outputs=outputs,\n",
    "                target_sizes=target_sizes,\n",
    "                threshold=CONFIG['DETECTION_THRESHOLD']\n",
    "            )\n",
    "\n",
    "            # Filter by confidence threshold\n",
    "            if results and len(results) > 0:\n",
    "                result = results[0]  # First (and only) image in batch\n",
    "\n",
    "                if 'scores' in result and len(result['scores']) > 0:\n",
    "                    # Filter by confidence\n",
    "                    high_conf_mask = result['scores'] >= CONFIG['CONFIDENCE_THRESHOLD']\n",
    "\n",
    "                    if high_conf_mask.any():\n",
    "                        filtered_boxes = result['boxes'][high_conf_mask]\n",
    "                        filtered_scores = result['scores'][high_conf_mask]\n",
    "                        filtered_labels = result['labels'][high_conf_mask]\n",
    "\n",
    "                        # Convert labels to text (OWL-ViT returns label indices)\n",
    "                        filtered_labels_text = [text_queries[label] for label in filtered_labels]\n",
    "\n",
    "                        batch_results.append({\n",
    "                            'image_path': img_path,\n",
    "                            'image': image,\n",
    "                            'boxes': filtered_boxes,\n",
    "                            'confidence_scores': filtered_scores,\n",
    "                            'labels': filtered_labels_text,\n",
    "                            'query': query\n",
    "                        })\n",
    "\n",
    "            # Clear memory after each image\n",
    "            del inputs, outputs, results, image\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print error but continue processing\n",
    "            print(f\"\\n⚠️ Error processing {img_path.name}: {str(e)[:50]}...\")\n",
    "            continue\n",
    "\n",
    "        # Small break every 10 images to prevent system overload\n",
    "        if i % 10 == 0 and i > 0:\n",
    "            import time\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    return batch_results\n",
    "\n",
    "def search_images_with_query(query, model, processor, device, model_name, gallery_path, batch_size=4):\n",
    "    \"\"\"\n",
    "    Search for objects in images using natural language query with OWL-ViT\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    gallery_path = Path(gallery_path)\n",
    "\n",
    "    if not gallery_path.exists():\n",
    "        print(f\"❌ Gallery path {gallery_path} does not exist\")\n",
    "        return results\n",
    "\n",
    "    if not model or not processor:\n",
    "        print(\"❌ Model or processor not loaded. Please check model initialization.\")\n",
    "        return results\n",
    "\n",
    "    # Supported image extensions\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}\n",
    "    image_files = [f for f in gallery_path.iterdir()\n",
    "                  if f.suffix.lower() in image_extensions]\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"⚠️ No images found in {gallery_path}\")\n",
    "        return results\n",
    "\n",
    "    total_files = len(image_files)\n",
    "\n",
    "    print(f\"🔍 Processing {total_files} images for: '{query}'\")\n",
    "    print(f\"🖥️ Device: {device} | Processing one by one for stability\")\n",
    "    print(f\"🦉 Model: {model_name}\")\n",
    "\n",
    "    # Process images one by one with progress tracking\n",
    "    try:\n",
    "        print(\"⏳ Starting image processing...\")\n",
    "        results = process_image_batch(image_files, model, processor, query, device, batch_size)\n",
    "\n",
    "        # Show final progress\n",
    "        matches_found = len(results)\n",
    "        print(f\"\\n📊 Final: {total_files}/{total_files} processed | {matches_found} matches found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Processing error: {e}\")\n",
    "        print(\"💡 Try reducing confidence threshold or switching to owlvit-base-patch32\")\n",
    "\n",
    "    # Final summary\n",
    "    print(f\"✅ Complete: {len(results)} images with matches found\")\n",
    "    return results\n",
    "\n",
    "def copy_results_to_folder(results, output_folder):\n",
    "    \"\"\"Copy matched images to results folder\"\"\"\n",
    "    output_path = Path(output_folder)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Create subfolder with timestamp\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    search_folder = output_path / f\"search_{timestamp}\"\n",
    "    search_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    copied_files = []\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        try:\n",
    "            source_path = result['image_path']\n",
    "            # Create descriptive filename\n",
    "            max_conf = float(result['confidence_scores'].max()) if len(result['confidence_scores']) > 0 else 0.0\n",
    "            filename = f\"{i+1:03d}_{source_path.stem}_conf{max_conf:.2f}{source_path.suffix}\"\n",
    "            dest_path = search_folder / filename\n",
    "\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            copied_files.append(dest_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error copying {source_path}: {e}\")\n",
    "\n",
    "    print(f\"📋 Copied {len(copied_files)} files to {search_folder}\")\n",
    "    return search_folder, copied_files\n",
    "\n",
    "# Cell 5: Interactive Query Interface\n",
    "def create_search_interface():\n",
    "    \"\"\"Create interactive search interface for forensic analysts\"\"\"\n",
    "\n",
    "    # Model selection dropdown\n",
    "    model_options = [(f\"{info['name']} - {info['description']}\", key)\n",
    "                    for key, info in AVAILABLE_MODELS.items()]\n",
    "\n",
    "    model_selector = widgets.Dropdown(\n",
    "        options=model_options,\n",
    "        value='owlvit-base-patch32',\n",
    "        description='Model:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='500px')\n",
    "    )\n",
    "\n",
    "    # Detection threshold slider (OWL-ViT specific)\n",
    "    detection_threshold_slider = widgets.FloatSlider(\n",
    "        value=CONFIG['DETECTION_THRESHOLD'],\n",
    "        min=0.01,\n",
    "        max=0.5,\n",
    "        step=0.01,\n",
    "        description='Detection Threshold:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    # Input widgets\n",
    "    query_input = widgets.Text(\n",
    "        value='person with weapon',\n",
    "        placeholder='Enter your search query (e.g., \"person with weapon\", \"suspicious vehicle\")',\n",
    "        description='Query:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='500px')\n",
    "    )\n",
    "\n",
    "    confidence_slider = widgets.FloatSlider(\n",
    "        value=CONFIG['CONFIDENCE_THRESHOLD'],\n",
    "        min=0.01,\n",
    "        max=0.5,\n",
    "        step=0.01,\n",
    "        description='Min Confidence:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    search_button = widgets.Button(\n",
    "        description='🔍 Search Gallery',\n",
    "        button_style='primary',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "\n",
    "    copy_button = widgets.Button(\n",
    "        description='📋 Copy Results',\n",
    "        button_style='success',\n",
    "        layout=widgets.Layout(width='150px'),\n",
    "        disabled=True\n",
    "    )\n",
    "\n",
    "    clear_button = widgets.Button(\n",
    "        description='🗑️ Clear Results',\n",
    "        button_style='warning',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "\n",
    "    switch_model_button = widgets.Button(\n",
    "        description='🔄 Switch Model',\n",
    "        button_style='info',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "\n",
    "    # Output area\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    # Store results and current model\n",
    "    search_results = []\n",
    "    current_model = model\n",
    "    current_processor = processor\n",
    "    current_device = device\n",
    "    current_model_name = model_name\n",
    "\n",
    "    def on_model_switch_clicked(b):\n",
    "        nonlocal current_model, current_processor, current_device, current_model_name\n",
    "        with output_area:\n",
    "            selected_model = model_selector.value\n",
    "            print(f\"🔄 Switching to: {AVAILABLE_MODELS[selected_model]['name']}\")\n",
    "            new_model, new_processor, new_device, new_model_name = switch_model(selected_model)\n",
    "            if new_model and new_processor:\n",
    "                current_model = new_model\n",
    "                current_processor = new_processor\n",
    "                current_device = new_device\n",
    "                current_model_name = new_model_name\n",
    "                print(\"✅ Model switched successfully!\")\n",
    "            else:\n",
    "                print(\"❌ Failed to switch model\")\n",
    "\n",
    "    def on_search_clicked(b):\n",
    "        nonlocal search_results\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            if not current_model or not current_processor:\n",
    "                print(\"❌ Model not loaded. Please switch to a valid model first.\")\n",
    "                return\n",
    "\n",
    "            query = query_input.value.strip()\n",
    "            if not query:\n",
    "                print(\"⚠️ Please enter a search query\")\n",
    "                return\n",
    "\n",
    "            # Update configuration\n",
    "            CONFIG['CONFIDENCE_THRESHOLD'] = confidence_slider.value\n",
    "            CONFIG['DETECTION_THRESHOLD'] = detection_threshold_slider.value\n",
    "\n",
    "            print(f\"🚀 Starting OWL-ViT search: '{query}'\")\n",
    "            print(f\"📊 Confidence: {CONFIG['CONFIDENCE_THRESHOLD']:.2f} | Detection: {CONFIG['DETECTION_THRESHOLD']:.2f}\")\n",
    "            print(f\"💡 Processing images individually for stability\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # Perform search\n",
    "            search_results = search_images_with_query(\n",
    "                query, current_model, current_processor, current_device, current_model_name,\n",
    "                CONFIG['SUSPECTS_GALLERY_PATH'], CONFIG['BATCH_SIZE']\n",
    "            )\n",
    "\n",
    "            if search_results:\n",
    "                copy_button.disabled = False\n",
    "                display_results(search_results[:CONFIG['MAX_RESULTS_DISPLAY']])\n",
    "\n",
    "                if len(search_results) > CONFIG['MAX_RESULTS_DISPLAY']:\n",
    "                    print(f\"\\n📝 Showing first {CONFIG['MAX_RESULTS_DISPLAY']} results out of {len(search_results)} total matches\")\n",
    "            else:\n",
    "                print(\"🔍 No matches found for your query\")\n",
    "                print(\"💡 Try lowering the confidence threshold or using different search terms\")\n",
    "                copy_button.disabled = True\n",
    "\n",
    "    def on_copy_clicked(b):\n",
    "        with output_area:\n",
    "            if search_results:\n",
    "                print(\"\\n📋 Copying results to output folder...\")\n",
    "                folder, files = copy_results_to_folder(search_results, CONFIG['RESULTS_OUTPUT_PATH'])\n",
    "                print(f\"✅ Results saved to: {folder}\")\n",
    "            else:\n",
    "                print(\"⚠️ No results to copy\")\n",
    "\n",
    "    def on_clear_clicked(b):\n",
    "        nonlocal search_results\n",
    "        search_results = []\n",
    "        copy_button.disabled = True\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            print(\"🗑️ Results cleared\")\n",
    "\n",
    "    # Connect button events\n",
    "    switch_model_button.on_click(on_model_switch_clicked)\n",
    "    search_button.on_click(on_search_clicked)\n",
    "    copy_button.on_click(on_copy_clicked)\n",
    "    clear_button.on_click(on_clear_clicked)\n",
    "\n",
    "    # Layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>🦉 Forensic Image Search Interface (OWL-ViT)</h3>\"),\n",
    "        model_selector,\n",
    "        query_input,\n",
    "        widgets.HBox([confidence_slider, detection_threshold_slider]),\n",
    "        widgets.HBox([search_button, copy_button, clear_button, switch_model_button]),\n",
    "        widgets.HTML(\"<hr>\")\n",
    "    ])\n",
    "\n",
    "    return widgets.VBox([controls, output_area])\n",
    "\n",
    "def display_results(results):\n",
    "    \"\"\"Display search results with bounding boxes\"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "\n",
    "    cols = 2\n",
    "    rows = (len(results) + cols - 1) // cols\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=CONFIG['FIGURE_SIZE'])\n",
    "    if rows == 1:\n",
    "        axes = [axes] if cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        ax = axes[i] if len(results) > 1 else axes\n",
    "\n",
    "        # Display image\n",
    "        image = result['image']\n",
    "        ax.imshow(image)\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        w, h = image.size\n",
    "        boxes = result['boxes']\n",
    "        confidences = result['confidence_scores']\n",
    "\n",
    "        for box, conf in zip(boxes, confidences):\n",
    "            # Convert from [x1, y1, x2, y2] to matplotlib rectangle\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "\n",
    "            # Create rectangle\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), x2 - x1, y2 - y1,\n",
    "                linewidth=2, edgecolor='red', facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Add confidence text\n",
    "            ax.text(x1, y1 - 5, f'{conf:.2f}',\n",
    "                   color='red', fontweight='bold', fontsize=10)\n",
    "\n",
    "        ax.set_title(f\"{result['image_path'].name}\\nMatches: {len(boxes)}\", fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for j in range(len(results), len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display the interface\n",
    "interface = create_search_interface()\n",
    "display(interface)\n",
    "\n",
    "# Cell 6: Batch Processing Functions (Enhanced)\n",
    "def batch_search_multiple_queries(queries_list, model, processor, device, model_name, gallery_path, output_base_path, batch_size=4):\n",
    "    \"\"\"\n",
    "    Process multiple queries in batch for comprehensive analysis using OWL-ViT\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "\n",
    "    print(f\"🚀 Starting batch analysis with {len(queries_list)} queries\")\n",
    "    print(f\"📁 Gallery: {gallery_path}\")\n",
    "    print(f\"🦉 Model: {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i, query in enumerate(queries_list, 1):\n",
    "        print(f\"\\n[{i}/{len(queries_list)}] Query: '{query}'\")\n",
    "\n",
    "        results = search_images_with_query(query, model, processor, device, model_name, gallery_path, batch_size)\n",
    "\n",
    "        if results:\n",
    "            # Create query-specific output folder\n",
    "            query_folder = Path(output_base_path) / f\"query_{query.replace(' ', '_').replace('/', '_')}\"\n",
    "            folder, files = copy_results_to_folder(results, query_folder)\n",
    "            all_results[query] = {\n",
    "                'results': results,\n",
    "                'output_folder': folder,\n",
    "                'file_count': len(files),\n",
    "                'match_count': len(results)\n",
    "            }\n",
    "            print(f\"📁 Saved {len(files)} files to: {folder.name}\")\n",
    "        else:\n",
    "            all_results[query] = {\n",
    "                'results': [],\n",
    "                'output_folder': None,\n",
    "                'file_count': 0,\n",
    "                'match_count': 0\n",
    "            }\n",
    "            print(\"⚪ No matches found\")\n",
    "\n",
    "    # Summary report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📊 BATCH SEARCH SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    total_matches = 0\n",
    "    total_files = 0\n",
    "\n",
    "    for query, data in all_results.items():\n",
    "        matches = data['match_count']\n",
    "        files = data['file_count']\n",
    "        total_matches += matches\n",
    "        total_files += files\n",
    "\n",
    "        status = \"✅\" if matches > 0 else \"⚪\"\n",
    "        print(f\"{status} '{query}': {matches} images, {files} files saved\")\n",
    "\n",
    "    print(f\"\\n🎯 TOTAL: {total_matches} matched images, {total_files} files copied\")\n",
    "    return all_results\n",
    "\n",
    "# Enhanced batch processing with common forensic queries\n",
    "def run_forensic_batch_analysis(custom_queries=None, batch_size=4, model_to_use=None):\n",
    "    \"\"\"Run comprehensive forensic analysis with predefined and custom queries using OWL-ViT\"\"\"\n",
    "\n",
    "    # Use provided model or current global model\n",
    "    if model_to_use:\n",
    "        current_model, current_processor, current_device, current_model_name = model_to_use\n",
    "    else:\n",
    "        current_model, current_processor, current_device, current_model_name = model, processor, device, model_name\n",
    "\n",
    "    # Default forensic queries optimized for OWL-ViT\n",
    "    default_queries = [\n",
    "        \"person with weapon\",\n",
    "        \"person holding gun\",\n",
    "        \"person with knife\",\n",
    "        \"suspicious vehicle\",\n",
    "        \"person wearing mask\",\n",
    "        \"person running\",\n",
    "        \"backpack\",\n",
    "        \"group of people\",\n",
    "        \"person with phone\",\n",
    "        \"person in dark clothing\",\n",
    "        \"weapon\",\n",
    "        \"gun\",\n",
    "        \"knife\"\n",
    "    ]\n",
    "\n",
    "    # Combine with custom queries if provided\n",
    "    if custom_queries:\n",
    "        queries = default_queries + custom_queries\n",
    "        print(f\"📋 Using {len(default_queries)} default + {len(custom_queries)} custom queries\")\n",
    "    else:\n",
    "        queries = default_queries\n",
    "        print(f\"📋 Using {len(default_queries)} default forensic queries\")\n",
    "\n",
    "    if current_model and current_processor:\n",
    "        print(\"🔍 Starting comprehensive OWL-ViT forensic analysis...\")\n",
    "        batch_results = batch_search_multiple_queries(\n",
    "            queries,\n",
    "            current_model,\n",
    "            current_processor,\n",
    "            current_device,\n",
    "            current_model_name,\n",
    "            CONFIG['SUSPECTS_GALLERY_PATH'],\n",
    "            CONFIG['RESULTS_OUTPUT_PATH'],\n",
    "            batch_size\n",
    "        )\n",
    "        return batch_results\n",
    "    else:\n",
    "        print(\"❌ Model or processor not loaded. Cannot run batch analysis.\")\n",
    "        return None\n",
    "\n",
    "# Quick test with reduced output\n",
    "def quick_forensic_search(query=\"person with weapon\", batch_size=4, model_to_use=None):\n",
    "    \"\"\"Quick single query search for testing with OWL-ViT\"\"\"\n",
    "    if model_to_use:\n",
    "        current_model, current_processor, current_device, current_model_name = model_to_use\n",
    "    else:\n",
    "        current_model, current_processor, current_device, current_model_name = model, processor, device, model_name\n",
    "\n",
    "    if not current_model or not current_processor:\n",
    "        print(\"❌ Model or processor not loaded\")\n",
    "        return None\n",
    "\n",
    "    print(f\"🔍 Quick OWL-ViT search: '{query}'\")\n",
    "    print(f\"🦉 Using model: {current_model_name}\")\n",
    "\n",
    "    results = search_images_with_query(query, current_model, current_processor, current_device, current_model_name,\n",
    "                                     CONFIG['SUSPECTS_GALLERY_PATH'], batch_size)\n",
    "\n",
    "    if results:\n",
    "        print(f\"📋 Found {len(results)} matches - ready for detailed analysis\")\n",
    "        return results\n",
    "    else:\n",
    "        print(\"⚪ No matches found\")\n",
    "        return []\n",
    "\n",
    "# Model comparison utilities\n",
    "def list_available_models():\n",
    "    \"\"\"Display available OWL-ViT models with their descriptions\"\"\"\n",
    "    print(\"🦉 Available OWL-ViT Models:\")\n",
    "    print(\"-\" * 50)\n",
    "    for key, info in AVAILABLE_MODELS.items():\n",
    "        print(f\"🔹 {info['name']} ({key})\")\n",
    "        print(f\"   📊 {info['performance']}\")\n",
    "        print(f\"   📝 {info['description']}\")\n",
    "        print()\n",
    "\n",
    "# Display available models\n",
    "list_available_models()\n",
    "\n",
    "# Uncomment to run batch analysis\n",
    "# batch_results = run_forensic_batch_analysis(batch_size=4)\n",
    "\n",
    "# Uncomment for quick test\n",
    "# quick_results = quick_forensic_search(\"person holding gun\", batch_size=4)\n",
    "\n",
    "# Cell 7: Usage Instructions and Tips\n",
    "print(\"\"\"\n",
    "🎯 FORENSIC IMAGE SEARCH SYSTEM - OWL-ViT VERSION\n",
    "===============================================\n",
    "\n",
    "🆕 OWL-ViT FEATURES:\n",
    "• 🦉 Open-World Localization with Vision Transformers\n",
    "• 🎯 Zero-shot object detection without training data\n",
    "• 📊 Multiple model sizes for different speed/accuracy needs\n",
    "• 🔍 Excellent performance on diverse object categories\n",
    "\n",
    "📋 SETUP CHECKLIST:\n",
    "1. ✅ Place suspect images in the './suspects_gallery' folder\n",
    "2. ✅ Run all cells in order (1-6)\n",
    "3. ✅ OWL-ViT models download automatically on first use\n",
    "\n",
    "🦉 AVAILABLE OWL-ViT MODELS:\n",
    "• Patch32: Fastest, good for quick analysis\n",
    "• Patch16: Better accuracy, moderate speed\n",
    "• Large Patch14: Best accuracy, slowest (recommended for critical analysis)\n",
    "\n",
    "🔧 DEVICE COMPATIBILITY:\n",
    "• System automatically detects GPU/CPU availability\n",
    "• Models work on both CPU and GPU\n",
    "• CPU mode: Slower but works on all systems\n",
    "• GPU mode: Faster with CUDA support\n",
    "\n",
    "🔍 SEARCH TIPS FOR OWL-ViT:\n",
    "• Use simple, clear terms: \"weapon\", \"gun\", \"knife\", \"person\"\n",
    "• OWL-ViT works well with single objects and people\n",
    "• Try both specific and general terms: \"weapon\" vs \"gun\"\n",
    "• Lower confidence thresholds (0.05-0.15) often work better\n",
    "• Common forensic queries:\n",
    "  - \"weapon\" / \"gun\" / \"knife\"\n",
    "  - \"person\" / \"person with weapon\"\n",
    "  - \"vehicle\" / \"car\" / \"suspicious vehicle\"\n",
    "  - \"mask\" / \"person wearing mask\"\n",
    "  - \"backpack\" / \"bag\"\n",
    "\n",
    "⚙️ INTERFACE FEATURES:\n",
    "• Model selector for switching between OWL-ViT variants\n",
    "• Dual thresholds: Detection threshold + Confidence threshold\n",
    "• Real-time search with progress tracking\n",
    "• Copy results to organized folders\n",
    "• Clear visual results with bounding boxes\n",
    "\n",
    "🎛️ THRESHOLD TUNING:\n",
    "• Detection Threshold (0.01-0.5): Controls initial detection sensitivity\n",
    "• Confidence Threshold (0.01-0.5): Filters final results\n",
    "• Start with low values (0.05-0.1) and adjust based on results\n",
    "• OWL-ViT typically uses lower thresholds than other models\n",
    "\n",
    "📁 OUTPUT STRUCTURE:\n",
    "search_results/\n",
    "├── search_20240611_143022/\n",
    "│   ├── 001_suspect1_conf0.12.jpg\n",
    "│   ├── 002_suspect5_conf0.18.jpg\n",
    "│   └── ...\n",
    "\n",
    "🚨 TROUBLESHOOTING:\n",
    "• \"Model loading failed\" → Check internet connection (first download)\n",
    "• \"No matches found\" → Try lowering detection/confidence thresholds\n",
    "• \"Low confidence scores\" → Normal for OWL-ViT, try 0.05-0.15 range\n",
    "• Slow performance → Use Patch32 model or reduce image count\n",
    "• Memory issues → Restart kernel, use CPU mode\n",
    "\n",
    "🔄 MODEL COMPARISON:\n",
    "• Patch32: Fast processing, good for real-time analysis\n",
    "• Patch16: Balanced performance, recommended for most forensic work\n",
    "• Large Patch14: Maximum accuracy for critical evidence analysis\n",
    "\n",
    "⭐ OWL-ViT ADVANTAGES:\n",
    "• Excellent zero-shot performance on novel objects\n",
    "• No need for training on specific datasets\n",
    "• Strong performance on people and common objects\n",
    "• Good generalization to forensic scenarios\n",
    "• Multiple resolution options for different needs\n",
    "\n",
    "💡 FORENSIC BEST PRACTICES:\n",
    "• Start with broad terms like \"person\", \"weapon\", \"vehicle\"\n",
    "• Use multiple queries for comprehensive analysis\n",
    "• Review low-confidence detections manually\n",
    "• Cross-reference results with different models\n",
    "• Document threshold settings for evidence reports\n",
    "\n",
    "🎯 PERFORMANCE EXPECTATIONS:\n",
    "• OWL-ViT excels at detecting people and common objects\n",
    "• May require lower confidence thresholds than other models\n",
    "• Performs well on weapons, vehicles, and suspicious activities\n",
    "• Best results with clear, well-lit images\n",
    "• Good performance even on partially occluded objects\n",
    "\n",
    "For batch processing of multiple queries, use the functions in Cell 6.\n",
    "For comparing different models, use the model switching interface.\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded successfully\n",
      "📁 Suspects gallery: ../../datasets/images/objects/raw\n",
      "📁 Results output: ../../datasets/images/objects/detections\n",
      "🦉 Selected model: google/owlvit-base-patch32\n",
      "✅ Transformers already installed\n",
      "✅ All dependencies imported successfully\n",
      "🔧 PyTorch version: 2.7.1+cpu\n",
      "🖥️ Using CPU mode (CUDA not available)\n",
      "📍 Auto-detected device: cpu\n",
      "📁 Created directories: ../../datasets/images/objects/raw, ../../datasets/images/objects/detections\n",
      "📥 Loading OWL-ViT model: google/owlvit-base-patch32\n",
      "🖥️ Target device: cpu\n",
      "⏳ Loading processor...\n",
      "⏳ Loading model weights...\n",
      "📍 Moving model to cpu...\n",
      "✅ Model loaded successfully!\n",
      "   🦉 Model: google/owlvit-base-patch32\n",
      "   🖥️ Device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h3>🦉 Forensic Image Search Interface (OWL-ViT)</h3>'), Dropdown(des…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "633c6888003e4289820ed444195af207"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦉 Available OWL-ViT Models:\n",
      "--------------------------------------------------\n",
      "🔹 OWL-ViT Base Patch32 (owlvit-base-patch32)\n",
      "   📊 Base performance, fastest inference\n",
      "   📝 Fastest OWL-ViT model - good balance of speed and accuracy\n",
      "\n",
      "🔹 OWL-ViT Base Patch16 (owlvit-base-patch16)\n",
      "   📊 Better accuracy, moderate speed\n",
      "   📝 Higher resolution features - better accuracy, slower\n",
      "\n",
      "🔹 OWL-ViT Large Patch14 (owlvit-large-patch14)\n",
      "   📊 Best accuracy, slowest speed\n",
      "   📝 Best accuracy - largest model, slowest inference\n",
      "\n",
      "\n",
      "🎯 FORENSIC IMAGE SEARCH SYSTEM - OWL-ViT VERSION\n",
      "===============================================\n",
      "\n",
      "🆕 OWL-ViT FEATURES:\n",
      "• 🦉 Open-World Localization with Vision Transformers\n",
      "• 🎯 Zero-shot object detection without training data\n",
      "• 📊 Multiple model sizes for different speed/accuracy needs\n",
      "• 🔍 Excellent performance on diverse object categories\n",
      "\n",
      "📋 SETUP CHECKLIST:\n",
      "1. ✅ Place suspect images in the './suspects_gallery' folder\n",
      "2. ✅ Run all cells in order (1-6)\n",
      "3. ✅ OWL-ViT models download automatically on first use\n",
      "\n",
      "🦉 AVAILABLE OWL-ViT MODELS:\n",
      "• Patch32: Fastest, good for quick analysis\n",
      "• Patch16: Better accuracy, moderate speed\n",
      "• Large Patch14: Best accuracy, slowest (recommended for critical analysis)\n",
      "\n",
      "🔧 DEVICE COMPATIBILITY:\n",
      "• System automatically detects GPU/CPU availability\n",
      "• Models work on both CPU and GPU\n",
      "• CPU mode: Slower but works on all systems\n",
      "• GPU mode: Faster with CUDA support\n",
      "\n",
      "🔍 SEARCH TIPS FOR OWL-ViT:\n",
      "• Use simple, clear terms: \"weapon\", \"gun\", \"knife\", \"person\"\n",
      "• OWL-ViT works well with single objects and people\n",
      "• Try both specific and general terms: \"weapon\" vs \"gun\"\n",
      "• Lower confidence thresholds (0.05-0.15) often work better\n",
      "• Common forensic queries:\n",
      "  - \"weapon\" / \"gun\" / \"knife\"\n",
      "  - \"person\" / \"person with weapon\"\n",
      "  - \"vehicle\" / \"car\" / \"suspicious vehicle\"\n",
      "  - \"mask\" / \"person wearing mask\"\n",
      "  - \"backpack\" / \"bag\"\n",
      "\n",
      "⚙️ INTERFACE FEATURES:\n",
      "• Model selector for switching between OWL-ViT variants\n",
      "• Dual thresholds: Detection threshold + Confidence threshold\n",
      "• Real-time search with progress tracking\n",
      "• Copy results to organized folders\n",
      "• Clear visual results with bounding boxes\n",
      "\n",
      "🎛️ THRESHOLD TUNING:\n",
      "• Detection Threshold (0.01-0.5): Controls initial detection sensitivity\n",
      "• Confidence Threshold (0.01-0.5): Filters final results\n",
      "• Start with low values (0.05-0.1) and adjust based on results\n",
      "• OWL-ViT typically uses lower thresholds than other models\n",
      "\n",
      "📁 OUTPUT STRUCTURE:\n",
      "search_results/\n",
      "├── search_20240611_143022/\n",
      "│   ├── 001_suspect1_conf0.12.jpg\n",
      "│   ├── 002_suspect5_conf0.18.jpg\n",
      "│   └── ...\n",
      "\n",
      "🚨 TROUBLESHOOTING:\n",
      "• \"Model loading failed\" → Check internet connection (first download)\n",
      "• \"No matches found\" → Try lowering detection/confidence thresholds\n",
      "• \"Low confidence scores\" → Normal for OWL-ViT, try 0.05-0.15 range\n",
      "• Slow performance → Use Patch32 model or reduce image count\n",
      "• Memory issues → Restart kernel, use CPU mode\n",
      "\n",
      "🔄 MODEL COMPARISON:\n",
      "• Patch32: Fast processing, good for real-time analysis\n",
      "• Patch16: Balanced performance, recommended for most forensic work\n",
      "• Large Patch14: Maximum accuracy for critical evidence analysis\n",
      "\n",
      "⭐ OWL-ViT ADVANTAGES:\n",
      "• Excellent zero-shot performance on novel objects\n",
      "• No need for training on specific datasets\n",
      "• Strong performance on people and common objects\n",
      "• Good generalization to forensic scenarios\n",
      "• Multiple resolution options for different needs\n",
      "\n",
      "💡 FORENSIC BEST PRACTICES:\n",
      "• Start with broad terms like \"person\", \"weapon\", \"vehicle\"\n",
      "• Use multiple queries for comprehensive analysis\n",
      "• Review low-confidence detections manually\n",
      "• Cross-reference results with different models\n",
      "• Document threshold settings for evidence reports\n",
      "\n",
      "🎯 PERFORMANCE EXPECTATIONS:\n",
      "• OWL-ViT excels at detecting people and common objects\n",
      "• May require lower confidence thresholds than other models\n",
      "• Performs well on weapons, vehicles, and suspicious activities\n",
      "• Best results with clear, well-lit images\n",
      "• Good performance even on partially occluded objects\n",
      "\n",
      "For batch processing of multiple queries, use the functions in Cell 6.\n",
      "For comparing different models, use the model switching interface.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
