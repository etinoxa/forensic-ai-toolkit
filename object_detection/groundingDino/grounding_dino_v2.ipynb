{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 1: Configuration and Setup\n",
    "import torch\n",
    "from IPython.display import display\n",
    "\n",
    "# Configuration Settings\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'SUSPECTS_GALLERY_PATH': '../../datasets/images/objects/raw',  # Input folder with suspect images\n",
    "    'RESULTS_OUTPUT_PATH': '../../datasets/images/objects/detections',      # Output folder for matched images\n",
    "    'MODEL_CHECKPOINT': '../GroundingDINO/weights/groundingdino_swint_ogc.pth',  # GroundingDINO model path\n",
    "    'MODEL_CONFIG': '../GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py',       # Model config file\n",
    "\n",
    "    # Detection parameters\n",
    "    'CONFIDENCE_THRESHOLD': 0.35,  # Minimum confidence for detections\n",
    "    'BOX_THRESHOLD': 0.3,          # Box threshold for NMS\n",
    "    'TEXT_THRESHOLD': 0.25,        # Text similarity threshold\n",
    "\n",
    "    # Display settings\n",
    "    'MAX_RESULTS_DISPLAY': 10,     # Maximum results to display at once\n",
    "    'FIGURE_SIZE': (12, 8),        # Size of result visualization\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully\")\n",
    "print(f\"üìÅ Suspects gallery: {CONFIG['SUSPECTS_GALLERY_PATH']}\")\n",
    "print(f\"üìÅ Results output: {CONFIG['RESULTS_OUTPUT_PATH']}\")\n",
    "\n",
    "# Cell 2: Install and Import Dependencies\n",
    "# Run this cell first to install required packages\n",
    "try:\n",
    "    import groundingdino\n",
    "    print(\"‚úÖ GroundingDINO already installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Installing GroundingDINO and dependencies...\")\n",
    "    !pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu  # CPU-only version\n",
    "    !pip install groundingdino-py\n",
    "    !pip install supervision\n",
    "    !pip install transformers\n",
    "    !pip install ipywidgets\n",
    "    print(\"üì¶ Installation complete - using CPU-optimized PyTorch\")\n",
    "\n",
    "# Additional imports after installation\n",
    "try:\n",
    "    from groundingdino.models import build_model\n",
    "    from groundingdino.util.slconfig import SLConfig\n",
    "    from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "    from groundingdino.util.inference import annotate, load_image, predict\n",
    "    import supervision as sv\n",
    "    print(\"‚úÖ All dependencies imported successfully\")\n",
    "\n",
    "    # Check PyTorch device compatibility\n",
    "    print(f\"üîß PyTorch version: {torch.__version__}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üöÄ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"üñ•Ô∏è Using CPU mode (CUDA not available)\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"1. Restart kernel and run this cell again\")\n",
    "    print(\"2. Check if all packages installed correctly\")\n",
    "    print(\"3. Try installing dependencies manually:\")\n",
    "    print(\"   !pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\")\n",
    "    print(\"   !pip install groundingdino-py supervision transformers\")\n",
    "\n",
    "# Cell 3: Initialize Model and Directories\n",
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories if they don't exist\"\"\"\n",
    "    os.makedirs(CONFIG['SUSPECTS_GALLERY_PATH'], exist_ok=True)\n",
    "    os.makedirs(CONFIG['RESULTS_OUTPUT_PATH'], exist_ok=True)\n",
    "    print(f\"üìÅ Created directories: {CONFIG['SUSPECTS_GALLERY_PATH']}, {CONFIG['RESULTS_OUTPUT_PATH']}\")\n",
    "\n",
    "def check_device_compatibility():\n",
    "    \"\"\"Check and configure device compatibility\"\"\"\n",
    "    print(\"üîß Checking device compatibility...\")\n",
    "\n",
    "    # Check PyTorch installation\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "    # Check CUDA availability\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "    if cuda_available:\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "        device = 'cuda'\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è CUDA not available - using CPU mode\")\n",
    "        print(\"Note: CPU inference will be slower but should work fine\")\n",
    "        device = 'cpu'\n",
    "\n",
    "    return device\n",
    "\n",
    "def load_groundingdino_model():\n",
    "    \"\"\"Load GroundingDINO model with proper device handling\"\"\"\n",
    "    try:\n",
    "        # Check device compatibility first\n",
    "        device = check_device_compatibility()\n",
    "\n",
    "        # Load model configuration\n",
    "        args = SLConfig.fromfile(CONFIG['MODEL_CONFIG'])\n",
    "        args.device = device\n",
    "        model = build_model(args)\n",
    "\n",
    "        # Load checkpoint with proper device mapping\n",
    "        print(f\"üì• Loading model checkpoint...\")\n",
    "        checkpoint = torch.load(CONFIG['MODEL_CHECKPOINT'], map_location=device)\n",
    "        model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
    "\n",
    "        # Move model to device and set to eval mode\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        # Store device info for later use\n",
    "        model.device = device\n",
    "\n",
    "        print(f\"‚úÖ Model loaded successfully on {device}\")\n",
    "        return model\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå Model files not found: {e}\")\n",
    "        print(\"Please ensure model files are available:\")\n",
    "        print(f\"  - {CONFIG['MODEL_CONFIG']}\")\n",
    "        print(f\"  - {CONFIG['MODEL_CHECKPOINT']}\")\n",
    "        print(\"\\nTo download GroundingDINO model files:\")\n",
    "        print(\"1. Visit: https://github.com/IDEA-Research/GroundingDINO\")\n",
    "        print(\"2. Download the pre-trained weights\")\n",
    "        print(\"3. Update the CONFIG paths accordingly\")\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        print(\"\\nTroubleshooting steps:\")\n",
    "        print(\"1. Check if all dependencies are installed\")\n",
    "        print(\"2. Verify model file paths are correct\")\n",
    "        print(\"3. Ensure sufficient memory available\")\n",
    "        return None\n",
    "\n",
    "# Initialize\n",
    "setup_directories()\n",
    "model = load_groundingdino_model()\n",
    "\n",
    "# Cell 4: Core Search Functions\n",
    "def process_image_batch(image_batch, model, query, device):\n",
    "    \"\"\"Process a batch of images efficiently\"\"\"\n",
    "    batch_results = []\n",
    "\n",
    "    for img_path, image_source, image in image_batch:\n",
    "        try:\n",
    "            # Ensure image tensor is on the same device as model\n",
    "            if hasattr(image, 'to'):\n",
    "                image = image.to(device)\n",
    "\n",
    "            # Run inference with device handling\n",
    "            with torch.no_grad():  # Save memory during inference\n",
    "                boxes, logits, phrases = predict(\n",
    "                    model=model,\n",
    "                    image=image,\n",
    "                    caption=query,\n",
    "                    box_threshold=CONFIG['BOX_THRESHOLD'],\n",
    "                    text_threshold=CONFIG['TEXT_THRESHOLD'],\n",
    "                    device=device\n",
    "                )\n",
    "\n",
    "            # Filter by confidence threshold\n",
    "            if len(logits) > 0:\n",
    "                high_conf_indices = logits > CONFIG['CONFIDENCE_THRESHOLD']\n",
    "\n",
    "                if high_conf_indices.any():\n",
    "                    filtered_boxes = boxes[high_conf_indices]\n",
    "                    filtered_logits = logits[high_conf_indices]\n",
    "                    filtered_phrases = [phrases[i] for i in range(len(phrases)) if high_conf_indices[i]]\n",
    "\n",
    "                    batch_results.append({\n",
    "                        'image_path': img_path,\n",
    "                        'image_source': image_source,\n",
    "                        'boxes': filtered_boxes,\n",
    "                        'confidence_scores': filtered_logits,\n",
    "                        'phrases': filtered_phrases,\n",
    "                        'query': query\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            # Silently skip problematic images\n",
    "            continue\n",
    "\n",
    "    return batch_results\n",
    "\n",
    "def search_images_with_query(query, model, gallery_path, batch_size=8):\n",
    "    \"\"\"\n",
    "    Search for objects in images using natural language query with batch processing\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    gallery_path = Path(gallery_path)\n",
    "\n",
    "    if not gallery_path.exists():\n",
    "        print(f\"‚ùå Gallery path {gallery_path} does not exist\")\n",
    "        return results\n",
    "\n",
    "    if not model:\n",
    "        print(\"‚ùå Model not loaded. Please check model initialization.\")\n",
    "        return results\n",
    "\n",
    "    # Supported image extensions\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}\n",
    "    image_files = [f for f in gallery_path.iterdir()\n",
    "                  if f.suffix.lower() in image_extensions]\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"‚ö†Ô∏è No images found in {gallery_path}\")\n",
    "        return results\n",
    "\n",
    "    device = getattr(model, 'device', 'cpu')\n",
    "    total_files = len(image_files)\n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    print(f\"üîç Processing {total_files} images for: '{query}'\")\n",
    "    print(f\"üñ•Ô∏è Device: {device} | Batch size: {batch_size}\")\n",
    "\n",
    "    # Process images in batches\n",
    "    for i in range(0, total_files, batch_size):\n",
    "        batch_files = image_files[i:i + batch_size]\n",
    "        batch_data = []\n",
    "\n",
    "        # Load batch of images\n",
    "        for img_path in batch_files:\n",
    "            try:\n",
    "                image_source, image = load_image(str(img_path))\n",
    "                batch_data.append((img_path, image_source, image))\n",
    "                processed_count += 1\n",
    "            except Exception:\n",
    "                error_count += 1\n",
    "                continue\n",
    "\n",
    "        # Process the batch\n",
    "        if batch_data:\n",
    "            batch_results = process_image_batch(batch_data, model, query, device)\n",
    "            results.extend(batch_results)\n",
    "\n",
    "        # Progress update\n",
    "        progress = min(i + batch_size, total_files)\n",
    "        matches_so_far = len(results)\n",
    "        print(f\"üìä Progress: {progress}/{total_files} | Matches: {matches_so_far}\")\n",
    "\n",
    "        # Clear cache periodically\n",
    "        if torch.cuda.is_available() and i % (batch_size * 3) == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Final summary\n",
    "    print(f\"\\n‚úÖ Complete: {processed_count} processed, {error_count} errors, {len(results)} matches\")\n",
    "    return results\n",
    "\n",
    "def copy_results_to_folder(results, output_folder):\n",
    "    \"\"\"Copy matched images to results folder\"\"\"\n",
    "    output_path = Path(output_folder)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Create subfolder with timestamp\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    search_folder = output_path / f\"search_{timestamp}\"\n",
    "    search_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    copied_files = []\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        try:\n",
    "            source_path = result['image_path']\n",
    "            # Create descriptive filename\n",
    "            filename = f\"{i+1:03d}_{source_path.stem}_conf{result['confidence_scores'].max():.2f}{source_path.suffix}\"\n",
    "            dest_path = search_folder / filename\n",
    "\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            copied_files.append(dest_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error copying {source_path}: {e}\")\n",
    "\n",
    "    print(f\"üìã Copied {len(copied_files)} files to {search_folder}\")\n",
    "    return search_folder, copied_files\n",
    "\n",
    "# Cell 5: Interactive Query Interface\n",
    "def create_search_interface():\n",
    "    \"\"\"Create interactive search interface for forensic analysts\"\"\"\n",
    "\n",
    "    batch_size_slider = widgets.IntSlider(\n",
    "        value=8,\n",
    "        min=1,\n",
    "        max=16,\n",
    "        step=1,\n",
    "        description='Batch Size:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    # Input widgets\n",
    "    query_input = widgets.Text(\n",
    "        value='person with red shirt',\n",
    "        placeholder='Enter your search query (e.g., \"person with weapon\", \"suspicious vehicle\")',\n",
    "        description='Query:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='500px')\n",
    "    )\n",
    "\n",
    "    confidence_slider = widgets.FloatSlider(\n",
    "        value=CONFIG['CONFIDENCE_THRESHOLD'],\n",
    "        min=0.1,\n",
    "        max=0.9,\n",
    "        step=0.05,\n",
    "        description='Min Confidence:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    search_button = widgets.Button(\n",
    "        description='üîç Search Gallery',\n",
    "        button_style='primary',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "\n",
    "    copy_button = widgets.Button(\n",
    "        description='üìã Copy Results',\n",
    "        button_style='success',\n",
    "        layout=widgets.Layout(width='150px'),\n",
    "        disabled=True\n",
    "    )\n",
    "\n",
    "    clear_button = widgets.Button(\n",
    "        description='üóëÔ∏è Clear Results',\n",
    "        button_style='warning',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "\n",
    "    # Output area\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    # Store results for copying\n",
    "    search_results = []\n",
    "\n",
    "    def on_search_clicked(b):\n",
    "        nonlocal search_results\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            if not model:\n",
    "                print(\"‚ùå Model not loaded. Please run the model initialization cell first.\")\n",
    "                return\n",
    "\n",
    "            query = query_input.value.strip()\n",
    "            if not query:\n",
    "                print(\"‚ö†Ô∏è Please enter a search query\")\n",
    "                return\n",
    "\n",
    "            # Update configuration\n",
    "            CONFIG['CONFIDENCE_THRESHOLD'] = confidence_slider.value\n",
    "            batch_size = batch_size_slider.value\n",
    "\n",
    "            print(f\"üöÄ Starting search: '{query}' | Confidence: {CONFIG['CONFIDENCE_THRESHOLD']:.2f} | Batch: {batch_size}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # Perform search\n",
    "            search_results = search_images_with_query(\n",
    "                query, model, CONFIG['SUSPECTS_GALLERY_PATH'], batch_size\n",
    "            )\n",
    "\n",
    "            if search_results:\n",
    "                copy_button.disabled = False\n",
    "                display_results(search_results[:CONFIG['MAX_RESULTS_DISPLAY']])\n",
    "\n",
    "                if len(search_results) > CONFIG['MAX_RESULTS_DISPLAY']:\n",
    "                    print(f\"\\nüìù Showing first {CONFIG['MAX_RESULTS_DISPLAY']} results out of {len(search_results)} total matches\")\n",
    "            else:\n",
    "                print(\"üîç No matches found for your query\")\n",
    "                copy_button.disabled = True\n",
    "\n",
    "    def on_copy_clicked(b):\n",
    "        with output_area:\n",
    "            if search_results:\n",
    "                print(\"\\nüìã Copying results to output folder...\")\n",
    "                folder, files = copy_results_to_folder(search_results, CONFIG['RESULTS_OUTPUT_PATH'])\n",
    "                print(f\"‚úÖ Results saved to: {folder}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No results to copy\")\n",
    "\n",
    "    def on_clear_clicked(b):\n",
    "        nonlocal search_results\n",
    "        search_results = []\n",
    "        copy_button.disabled = True\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            print(\"üóëÔ∏è Results cleared\")\n",
    "\n",
    "    # Connect button events\n",
    "    search_button.on_click(on_search_clicked)\n",
    "    copy_button.on_click(on_copy_clicked)\n",
    "    clear_button.on_click(on_clear_clicked)\n",
    "\n",
    "    # Layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>üîç Forensic Image Search Interface</h3>\"),\n",
    "        query_input,\n",
    "        widgets.HBox([confidence_slider, batch_size_slider]),\n",
    "        widgets.HBox([search_button, copy_button, clear_button]),\n",
    "        widgets.HTML(\"<hr>\")\n",
    "    ])\n",
    "\n",
    "    return widgets.VBox([controls, output_area])\n",
    "\n",
    "def display_results(results):\n",
    "    \"\"\"Display search results with bounding boxes\"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "\n",
    "    cols = 2\n",
    "    rows = (len(results) + cols - 1) // cols\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=CONFIG['FIGURE_SIZE'])\n",
    "    if rows == 1:\n",
    "        axes = [axes] if cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        ax = axes[i] if len(results) > 1 else axes\n",
    "\n",
    "        # Display image\n",
    "        image = np.array(result['image_source'])\n",
    "        ax.imshow(image)\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        h, w = image.shape[:2]\n",
    "        boxes = result['boxes']\n",
    "        confidences = result['confidence_scores']\n",
    "\n",
    "        for box, conf in zip(boxes, confidences):\n",
    "            # Convert normalized coordinates to pixel coordinates\n",
    "            x1, y1, x2, y2 = box\n",
    "            x1, x2 = x1 * w, x2 * w\n",
    "            y1, y2 = y1 * h, y2 * h\n",
    "\n",
    "            # Create rectangle\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), x2 - x1, y2 - y1,\n",
    "                linewidth=2, edgecolor='red', facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Add confidence text\n",
    "            ax.text(x1, y1 - 5, f'{conf:.2f}',\n",
    "                   color='red', fontweight='bold', fontsize=10)\n",
    "\n",
    "        ax.set_title(f\"{result['image_path'].name}\\nMatches: {len(boxes)}\", fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for j in range(len(results), len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display the interface\n",
    "interface = create_search_interface()\n",
    "display(interface)\n",
    "\n",
    "# Cell 6: Batch Processing Functions (Enhanced)\n",
    "def batch_search_multiple_queries(queries_list, model, gallery_path, output_base_path, batch_size=8):\n",
    "    \"\"\"\n",
    "    Process multiple queries in batch for comprehensive analysis\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "\n",
    "    print(f\"üöÄ Starting batch analysis with {len(queries_list)} queries\")\n",
    "    print(f\"üìÅ Gallery: {gallery_path}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i, query in enumerate(queries_list, 1):\n",
    "        print(f\"\\n[{i}/{len(queries_list)}] Query: '{query}'\")\n",
    "\n",
    "        results = search_images_with_query(query, model, gallery_path, batch_size)\n",
    "\n",
    "        if results:\n",
    "            # Create query-specific output folder\n",
    "            query_folder = Path(output_base_path) / f\"query_{query.replace(' ', '_').replace('/', '_')}\"\n",
    "            folder, files = copy_results_to_folder(results, query_folder)\n",
    "            all_results[query] = {\n",
    "                'results': results,\n",
    "                'output_folder': folder,\n",
    "                'file_count': len(files),\n",
    "                'match_count': len(results)\n",
    "            }\n",
    "            print(f\"üìÅ Saved {len(files)} files to: {folder.name}\")\n",
    "        else:\n",
    "            all_results[query] = {\n",
    "                'results': [],\n",
    "                'output_folder': None,\n",
    "                'file_count': 0,\n",
    "                'match_count': 0\n",
    "            }\n",
    "            print(\"‚ö™ No matches found\")\n",
    "\n",
    "    # Summary report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä BATCH SEARCH SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    total_matches = 0\n",
    "    total_files = 0\n",
    "\n",
    "    for query, data in all_results.items():\n",
    "        matches = data['match_count']\n",
    "        files = data['file_count']\n",
    "        total_matches += matches\n",
    "        total_files += files\n",
    "\n",
    "        status = \"‚úÖ\" if matches > 0 else \"‚ö™\"\n",
    "        print(f\"{status} '{query}': {matches} images, {files} files saved\")\n",
    "\n",
    "    print(f\"\\nüéØ TOTAL: {total_matches} matched images, {total_files} files copied\")\n",
    "    return all_results\n",
    "\n",
    "# Enhanced batch processing with common forensic queries\n",
    "def run_forensic_batch_analysis(custom_queries=None, batch_size=8):\n",
    "    \"\"\"Run comprehensive forensic analysis with predefined and custom queries\"\"\"\n",
    "\n",
    "    # Default forensic queries\n",
    "    default_queries = [\n",
    "        \"person with weapon\",\n",
    "        \"person holding gun\",\n",
    "        \"person with knife\",\n",
    "        \"suspicious vehicle\",\n",
    "        \"person wearing mask\",\n",
    "        \"person running\",\n",
    "        \"bag or backpack\",\n",
    "        \"group of people\",\n",
    "        \"person with phone\",\n",
    "        \"person in dark clothing\"\n",
    "    ]\n",
    "\n",
    "    # Combine with custom queries if provided\n",
    "    if custom_queries:\n",
    "        queries = default_queries + custom_queries\n",
    "        print(f\"üìã Using {len(default_queries)} default + {len(custom_queries)} custom queries\")\n",
    "    else:\n",
    "        queries = default_queries\n",
    "        print(f\"üìã Using {len(default_queries)} default forensic queries\")\n",
    "\n",
    "    if model:\n",
    "        print(\"üîç Starting comprehensive forensic analysis...\")\n",
    "        batch_results = batch_search_multiple_queries(\n",
    "            queries,\n",
    "            model,\n",
    "            CONFIG['SUSPECTS_GALLERY_PATH'],\n",
    "            CONFIG['RESULTS_OUTPUT_PATH'],\n",
    "            batch_size\n",
    "        )\n",
    "        return batch_results\n",
    "    else:\n",
    "        print(\"‚ùå Model not loaded. Cannot run batch analysis.\")\n",
    "        return None\n",
    "\n",
    "# Quick test with reduced output\n",
    "def quick_forensic_search(query=\"person with weapon\", batch_size=8):\n",
    "    \"\"\"Quick single query search for testing\"\"\"\n",
    "    if not model:\n",
    "        print(\"‚ùå Model not loaded\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üîç Quick search: '{query}'\")\n",
    "    results = search_images_with_query(query, model, CONFIG['SUSPECTS_GALLERY_PATH'], batch_size)\n",
    "\n",
    "    if results:\n",
    "        print(f\"üìã Found {len(results)} matches - ready for detailed analysis\")\n",
    "        return results\n",
    "    else:\n",
    "        print(\"‚ö™ No matches found\")\n",
    "        return []\n",
    "\n",
    "# Uncomment to run batch analysis\n",
    "# batch_results = run_forensic_batch_analysis(batch_size=8)\n",
    "\n",
    "# Uncomment for quick test\n",
    "# quick_results = quick_forensic_search(\"person holding gun\", batch_size=8)\n",
    "\n",
    "# Cell 7: Usage Instructions and Tips\n",
    "print(\"\"\"\n",
    "üéØ FORENSIC IMAGE SEARCH SYSTEM - USAGE GUIDE\n",
    "============================================\n",
    "\n",
    "üìã SETUP CHECKLIST:\n",
    "1. ‚úÖ Place suspect images in the './suspects_gallery' folder\n",
    "2. ‚úÖ Ensure GroundingDINO model files are available\n",
    "3. ‚úÖ Run all cells in order (1-6)\n",
    "\n",
    "üîß CUDA/CPU COMPATIBILITY:\n",
    "‚Ä¢ System automatically detects GPU/CPU availability\n",
    "‚Ä¢ CPU mode: Slower but works on all systems\n",
    "‚Ä¢ GPU mode: Faster but requires CUDA-compatible PyTorch\n",
    "‚Ä¢ If getting CUDA errors, the system will fallback to CPU mode\n",
    "\n",
    "üîç SEARCH TIPS:\n",
    "‚Ä¢ Use specific, descriptive queries: \"person with red jacket\" instead of just \"person\"\n",
    "‚Ä¢ Try multiple variations: \"weapon\", \"gun\", \"knife\", \"suspicious object\"\n",
    "‚Ä¢ Adjust confidence threshold based on your needs (lower = more results, higher = more precise)\n",
    "‚Ä¢ Common forensic queries:\n",
    "  - \"person with weapon\"\n",
    "  - \"suspicious vehicle\"\n",
    "  - \"person wearing mask\"\n",
    "  - \"bag or backpack\"\n",
    "  - \"person running\"\n",
    "  - \"group of people\"\n",
    "\n",
    "‚öôÔ∏è CONFIGURATION:\n",
    "‚Ä¢ Modify CONFIG dictionary in Cell 1 to adjust paths and parameters\n",
    "‚Ä¢ Confidence threshold: 0.35 (recommended for forensic work)\n",
    "‚Ä¢ Results are automatically timestamped and organized\n",
    "\n",
    "üìÅ OUTPUT STRUCTURE:\n",
    "search_results/\n",
    "‚îú‚îÄ‚îÄ search_20240611_143022/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 001_suspect1_conf0.87.jpg\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 002_suspect5_conf0.72.jpg\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "\n",
    "üö® TROUBLESHOOTING:\n",
    "‚Ä¢ \"CUDA not enabled\" ‚Üí System will auto-switch to CPU mode\n",
    "‚Ä¢ \"Model not loaded\" ‚Üí Check model file paths in CONFIG or download model files\n",
    "‚Ä¢ \"No images found\" ‚Üí Verify images are in suspects_gallery folder\n",
    "‚Ä¢ \"Out of memory\" ‚Üí Reduce batch size or use CPU mode\n",
    "‚Ä¢ Low confidence scores ‚Üí Try different query phrasing or lower threshold\n",
    "\n",
    "üì• MODEL FILES:\n",
    "If model files are missing, download from:\n",
    "‚Ä¢ GroundingDINO GitHub: https://github.com/IDEA-Research/GroundingDINO\n",
    "‚Ä¢ Pre-trained weights and config files needed\n",
    "\n",
    "For batch processing of multiple queries, use the functions in Cell 6.\n",
    "\"\"\")"
   ],
   "id": "25d2058e9081d6b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T03:10:43.565663Z",
     "start_time": "2025-06-12T03:10:23.911038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 1: Configuration and Setup\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Configuration Settings\n",
    "CONFIG = {\n",
    "    # Model Selection (Hugging Face)\n",
    "    'MODEL_NAME': 'IDEA-Research/grounding-dino-base',  # Options: grounding-dino-tiny, grounding-dino-base\n",
    "    'DEVICE': 'auto',  # 'auto', 'cpu', 'cuda'\n",
    "\n",
    "    # Paths\n",
    "    'SUSPECTS_GALLERY_PATH': '../../datasets/images/objects/raw',  # Input folder with suspect images\n",
    "    'RESULTS_OUTPUT_PATH': '../../datasets/images/objects/detections',      # Output folder for matched images\n",
    "\n",
    "    # Detection parameters\n",
    "    'CONFIDENCE_THRESHOLD': 0.35,  # Minimum confidence for detections\n",
    "    'BOX_THRESHOLD': 0.35,         # Box threshold for detections\n",
    "    'TEXT_THRESHOLD': 0.25,        # Text similarity threshold\n",
    "\n",
    "    # Processing settings\n",
    "    'BATCH_SIZE': 8,               # Default batch size for processing\n",
    "    'MAX_RESULTS_DISPLAY': 10,     # Maximum results to display at once\n",
    "    'FIGURE_SIZE': (12, 8),        # Size of result visualization\n",
    "}\n",
    "\n",
    "# Available model options\n",
    "AVAILABLE_MODELS = {\n",
    "    'grounding-dino-tiny': {\n",
    "        'name': 'GroundingDINO-Tiny',\n",
    "        'model_id': 'IDEA-Research/grounding-dino-tiny',\n",
    "        'description': 'Fastest, smallest model - good for quick testing',\n",
    "        'performance': 'Lower accuracy, fastest speed'\n",
    "    },\n",
    "    'grounding-dino-base': {\n",
    "        'name': 'GroundingDINO-Base',\n",
    "        'model_id': 'IDEA-Research/grounding-dino-base',\n",
    "        'description': 'Best balance of speed and accuracy - recommended',\n",
    "        'performance': '52.5 AP on COCO, good speed'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully\")\n",
    "print(f\"üìÅ Suspects gallery: {CONFIG['SUSPECTS_GALLERY_PATH']}\")\n",
    "print(f\"üìÅ Results output: {CONFIG['RESULTS_OUTPUT_PATH']}\")\n",
    "print(f\"ü§ñ Selected model: {CONFIG['MODEL_NAME']}\")\n",
    "\n",
    "# Cell 2: Install and Import Dependencies\n",
    "# Run this cell first to install required packages\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"‚úÖ Transformers already installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Installing required packages...\")\n",
    "    !pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
    "    !pip install transformers\n",
    "    !pip install ipywidgets\n",
    "    !pip install Pillow\n",
    "    !pip install matplotlib\n",
    "    !pip install opencv-python\n",
    "    print(\"üì¶ Installation complete\")\n",
    "\n",
    "# Import required libraries\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    print(\"‚úÖ All dependencies imported successfully\")\n",
    "\n",
    "    # Check PyTorch device compatibility\n",
    "    print(f\"üîß PyTorch version: {torch.__version__}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üöÄ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        default_device = \"cuda\"\n",
    "    else:\n",
    "        print(\"üñ•Ô∏è Using CPU mode (CUDA not available)\")\n",
    "        default_device = \"cpu\"\n",
    "\n",
    "    # Update config with detected device\n",
    "    if CONFIG['DEVICE'] == 'auto':\n",
    "        CONFIG['DEVICE'] = default_device\n",
    "        print(f\"üìç Auto-detected device: {CONFIG['DEVICE']}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"1. Restart kernel and run this cell again\")\n",
    "    print(\"2. Check if all packages installed correctly\")\n",
    "\n",
    "# Cell 3: Initialize Model and Directories\n",
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories if they don't exist\"\"\"\n",
    "    os.makedirs(CONFIG['SUSPECTS_GALLERY_PATH'], exist_ok=True)\n",
    "    os.makedirs(CONFIG['RESULTS_OUTPUT_PATH'], exist_ok=True)\n",
    "    print(f\"üìÅ Created directories: {CONFIG['SUSPECTS_GALLERY_PATH']}, {CONFIG['RESULTS_OUTPUT_PATH']}\")\n",
    "\n",
    "def load_grounding_dino_model(model_name=None):\n",
    "    \"\"\"Load GroundingDINO model from Hugging Face\"\"\"\n",
    "    try:\n",
    "        # Use provided model name or default from config\n",
    "        if model_name is None:\n",
    "            model_name = CONFIG['MODEL_NAME']\n",
    "\n",
    "        device = CONFIG['DEVICE']\n",
    "        print(f\"üì• Loading model: {model_name}\")\n",
    "        print(f\"üñ•Ô∏è Target device: {device}\")\n",
    "\n",
    "        # Load processor and model\n",
    "        print(\"‚è≥ Loading processor...\")\n",
    "        processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "        print(\"‚è≥ Loading model weights...\")\n",
    "        model = AutoModelForZeroShotObjectDetection.from_pretrained(model_name)\n",
    "\n",
    "        # Move to device\n",
    "        print(f\"üìç Moving model to {device}...\")\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        print(f\"‚úÖ Model loaded successfully!\")\n",
    "        print(f\"   üìä Model: {model_name}\")\n",
    "        print(f\"   üñ•Ô∏è Device: {device}\")\n",
    "\n",
    "        return model, processor, device, model_name\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        print(\"\\nüîß Troubleshooting steps:\")\n",
    "        print(\"1. Check internet connection (models download from Hugging Face)\")\n",
    "        print(\"2. Verify model name is correct\")\n",
    "        print(\"3. Try switching to 'grounding-dino-tiny' for faster loading\")\n",
    "        print(\"4. Restart kernel if memory issues occur\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def switch_model(model_key):\n",
    "    \"\"\"Switch to a different model variant\"\"\"\n",
    "    if model_key in AVAILABLE_MODELS:\n",
    "        CONFIG['MODEL_NAME'] = AVAILABLE_MODELS[model_key]['model_id']\n",
    "        print(f\"üîÑ Switched to: {AVAILABLE_MODELS[model_key]['name']}\")\n",
    "        return load_grounding_dino_model()\n",
    "    else:\n",
    "        print(f\"‚ùå Unknown model: {model_key}\")\n",
    "        print(f\"Available models: {list(AVAILABLE_MODELS.keys())}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Initialize\n",
    "setup_directories()\n",
    "model, processor, device, model_name = load_grounding_dino_model()\n",
    "\n",
    "# Cell 4: Core Search Functions\n",
    "def process_image_batch(image_paths, model, processor, query, device, batch_size=4):\n",
    "    \"\"\"Process a batch of images efficiently using Hugging Face models\"\"\"\n",
    "    batch_results = []\n",
    "\n",
    "    # Process images one by one to avoid memory issues\n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        try:\n",
    "            # Progress indicator\n",
    "            if i % 5 == 0:\n",
    "                print(f\"Processing {i+1}/{len(image_paths)}: {img_path.name[:30]}...\", end='\\r')\n",
    "\n",
    "            # Load image\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            # Prepare text query (ensure proper format)\n",
    "            text_query = query.lower()\n",
    "            if not text_query.endswith('.'):\n",
    "                text_query += '.'\n",
    "\n",
    "            # Process inputs\n",
    "            inputs = processor(images=image, text=text_query, return_tensors=\"pt\")\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Post-process results with updated parameter name\n",
    "            results = processor.post_process_grounded_object_detection(\n",
    "                outputs,\n",
    "                inputs.input_ids,\n",
    "                box_threshold=CONFIG['BOX_THRESHOLD'],\n",
    "                text_threshold=CONFIG['TEXT_THRESHOLD'],\n",
    "                target_sizes=[image.size[::-1]]  # (height, width)\n",
    "            )\n",
    "\n",
    "            # Filter by confidence threshold\n",
    "            if results and len(results) > 0:\n",
    "                result = results[0]  # First (and only) image in batch\n",
    "\n",
    "                if 'scores' in result and len(result['scores']) > 0:\n",
    "                    # Filter by confidence\n",
    "                    high_conf_mask = result['scores'] >= CONFIG['CONFIDENCE_THRESHOLD']\n",
    "\n",
    "                    if high_conf_mask.any():\n",
    "                        filtered_boxes = result['boxes'][high_conf_mask]\n",
    "                        filtered_scores = result['scores'][high_conf_mask]\n",
    "\n",
    "                        # Use text_labels if available, otherwise fallback to labels\n",
    "                        if 'text_labels' in result:\n",
    "                            filtered_labels = [result['text_labels'][i] for i in range(len(result['text_labels'])) if high_conf_mask[i]]\n",
    "                        else:\n",
    "                            filtered_labels = [result['labels'][i] for i in range(len(result['labels'])) if high_conf_mask[i]]\n",
    "\n",
    "                        batch_results.append({\n",
    "                            'image_path': img_path,\n",
    "                            'image': image,\n",
    "                            'boxes': filtered_boxes,\n",
    "                            'confidence_scores': filtered_scores,\n",
    "                            'labels': filtered_labels,\n",
    "                            'query': query\n",
    "                        })\n",
    "\n",
    "            # Clear memory after each image\n",
    "            del inputs, outputs, results, image\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print error but continue processing\n",
    "            print(f\"\\n‚ö†Ô∏è Error processing {img_path.name}: {str(e)[:50]}...\")\n",
    "            continue\n",
    "\n",
    "        # Small break every 10 images to prevent system overload\n",
    "        if i % 10 == 0 and i > 0:\n",
    "            import time\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    return batch_results\n",
    "\n",
    "def search_images_with_query(query, model, processor, device, model_name, gallery_path, batch_size=4):\n",
    "    \"\"\"\n",
    "    Search for objects in images using natural language query with Hugging Face models\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    gallery_path = Path(gallery_path)\n",
    "\n",
    "    if not gallery_path.exists():\n",
    "        print(f\"‚ùå Gallery path {gallery_path} does not exist\")\n",
    "        return results\n",
    "\n",
    "    if not model or not processor:\n",
    "        print(\"‚ùå Model or processor not loaded. Please check model initialization.\")\n",
    "        return results\n",
    "\n",
    "    # Supported image extensions\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}\n",
    "    image_files = [f for f in gallery_path.iterdir()\n",
    "                  if f.suffix.lower() in image_extensions]\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"‚ö†Ô∏è No images found in {gallery_path}\")\n",
    "        return results\n",
    "\n",
    "    total_files = len(image_files)\n",
    "\n",
    "    print(f\"üîç Processing {total_files} images for: '{query}'\")\n",
    "    print(f\"üñ•Ô∏è Device: {device} | Processing one by one for stability\")\n",
    "    print(f\"ü§ñ Model: {model_name}\")\n",
    "\n",
    "    # Process images one by one with progress tracking\n",
    "    try:\n",
    "        print(\"‚è≥ Starting image processing...\")\n",
    "        results = process_image_batch(image_files, model, processor, query, device, batch_size)\n",
    "\n",
    "        # Show final progress\n",
    "        matches_found = len(results)\n",
    "        print(f\"\\nüìä Final: {total_files}/{total_files} processed | {matches_found} matches found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Processing error: {e}\")\n",
    "        print(\"üí° Try reducing batch size or switching to grounding-dino-tiny\")\n",
    "\n",
    "    # Final summary\n",
    "    print(f\"‚úÖ Complete: {len(results)} images with matches found\")\n",
    "    return results\n",
    "\n",
    "def copy_results_to_folder(results, output_folder):\n",
    "    \"\"\"Copy matched images to results folder\"\"\"\n",
    "    output_path = Path(output_folder)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Create subfolder with timestamp\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    search_folder = output_path / f\"search_{timestamp}\"\n",
    "    search_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    copied_files = []\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        try:\n",
    "            source_path = result['image_path']\n",
    "            # Create descriptive filename\n",
    "            max_conf = float(result['confidence_scores'].max()) if len(result['confidence_scores']) > 0 else 0.0\n",
    "            filename = f\"{i+1:03d}_{source_path.stem}_conf{max_conf:.2f}{source_path.suffix}\"\n",
    "            dest_path = search_folder / filename\n",
    "\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            copied_files.append(dest_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error copying {source_path}: {e}\")\n",
    "\n",
    "    print(f\"üìã Copied {len(copied_files)} files to {search_folder}\")\n",
    "    return search_folder, copied_files\n",
    "\n",
    "# Cell 5: Interactive Query Interface\n",
    "def create_search_interface():\n",
    "    \"\"\"Create interactive search interface for forensic analysts\"\"\"\n",
    "\n",
    "    # Model selection dropdown\n",
    "    model_options = [(f\"{info['name']} - {info['description']}\", key)\n",
    "                    for key, info in AVAILABLE_MODELS.items()]\n",
    "\n",
    "    model_selector = widgets.Dropdown(\n",
    "        options=model_options,\n",
    "        value='grounding-dino-base',\n",
    "        description='Model:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='500px')\n",
    "    )\n",
    "\n",
    "    # Batch size slider\n",
    "    batch_size_slider = widgets.IntSlider(\n",
    "        value=4,  # Reduced default for stability\n",
    "        min=1,\n",
    "        max=8,    # Reduced max to prevent memory issues\n",
    "        step=1,\n",
    "        description='Batch Size:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    # Input widgets\n",
    "    query_input = widgets.Text(\n",
    "        value='person with weapon',\n",
    "        placeholder='Enter your search query (e.g., \"person with weapon\", \"suspicious vehicle\")',\n",
    "        description='Query:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='500px')\n",
    "    )\n",
    "\n",
    "    confidence_slider = widgets.FloatSlider(\n",
    "        value=CONFIG['CONFIDENCE_THRESHOLD'],\n",
    "        min=0.1,\n",
    "        max=0.9,\n",
    "        step=0.05,\n",
    "        description='Min Confidence:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    search_button = widgets.Button(\n",
    "        description='üîç Search Gallery',\n",
    "        button_style='primary',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "\n",
    "    copy_button = widgets.Button(\n",
    "        description='üìã Copy Results',\n",
    "        button_style='success',\n",
    "        layout=widgets.Layout(width='150px'),\n",
    "        disabled=True\n",
    "    )\n",
    "\n",
    "    clear_button = widgets.Button(\n",
    "        description='üóëÔ∏è Clear Results',\n",
    "        button_style='warning',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "\n",
    "    switch_model_button = widgets.Button(\n",
    "        description='üîÑ Switch Model',\n",
    "        button_style='info',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "\n",
    "    # Output area\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    # Store results and current model\n",
    "    search_results = []\n",
    "    current_model = model\n",
    "    current_processor = processor\n",
    "    current_device = device\n",
    "    current_model_name = model_name\n",
    "\n",
    "    def on_model_switch_clicked(b):\n",
    "        nonlocal current_model, current_processor, current_device, current_model_name\n",
    "        with output_area:\n",
    "            selected_model = model_selector.value\n",
    "            print(f\"üîÑ Switching to: {AVAILABLE_MODELS[selected_model]['name']}\")\n",
    "            new_model, new_processor, new_device, new_model_name = switch_model(selected_model)\n",
    "            if new_model and new_processor:\n",
    "                current_model = new_model\n",
    "                current_processor = new_processor\n",
    "                current_device = new_device\n",
    "                current_model_name = new_model_name\n",
    "                print(\"‚úÖ Model switched successfully!\")\n",
    "            else:\n",
    "                print(\"‚ùå Failed to switch model\")\n",
    "\n",
    "    def on_search_clicked(b):\n",
    "        nonlocal search_results\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            if not current_model or not current_processor:\n",
    "                print(\"‚ùå Model not loaded. Please switch to a valid model first.\")\n",
    "                return\n",
    "\n",
    "            query = query_input.value.strip()\n",
    "            if not query:\n",
    "                print(\"‚ö†Ô∏è Please enter a search query\")\n",
    "                return\n",
    "\n",
    "            # Update configuration\n",
    "            CONFIG['CONFIDENCE_THRESHOLD'] = confidence_slider.value\n",
    "            batch_size = batch_size_slider.value\n",
    "\n",
    "            print(f\"üöÄ Starting search: '{query}'\")\n",
    "            print(f\"üìä Confidence: {CONFIG['CONFIDENCE_THRESHOLD']:.2f}\")\n",
    "            print(f\"üí° Processing images individually for stability\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # Perform search\n",
    "            search_results = search_images_with_query(\n",
    "                query, current_model, current_processor, current_device, current_model_name,\n",
    "                CONFIG['SUSPECTS_GALLERY_PATH'], batch_size\n",
    "            )\n",
    "\n",
    "            if search_results:\n",
    "                copy_button.disabled = False\n",
    "                display_results(search_results[:CONFIG['MAX_RESULTS_DISPLAY']])\n",
    "\n",
    "                if len(search_results) > CONFIG['MAX_RESULTS_DISPLAY']:\n",
    "                    print(f\"\\nüìù Showing first {CONFIG['MAX_RESULTS_DISPLAY']} results out of {len(search_results)} total matches\")\n",
    "            else:\n",
    "                print(\"üîç No matches found for your query\")\n",
    "                copy_button.disabled = True\n",
    "\n",
    "    def on_copy_clicked(b):\n",
    "        with output_area:\n",
    "            if search_results:\n",
    "                print(\"\\nüìã Copying results to output folder...\")\n",
    "                folder, files = copy_results_to_folder(search_results, CONFIG['RESULTS_OUTPUT_PATH'])\n",
    "                print(f\"‚úÖ Results saved to: {folder}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No results to copy\")\n",
    "\n",
    "    def on_clear_clicked(b):\n",
    "        nonlocal search_results\n",
    "        search_results = []\n",
    "        copy_button.disabled = True\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            print(\"üóëÔ∏è Results cleared\")\n",
    "\n",
    "    # Connect button events\n",
    "    switch_model_button.on_click(on_model_switch_clicked)\n",
    "    search_button.on_click(on_search_clicked)\n",
    "    copy_button.on_click(on_copy_clicked)\n",
    "    clear_button.on_click(on_clear_clicked)\n",
    "\n",
    "    # Layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>üîç Forensic Image Search Interface (Hugging Face)</h3>\"),\n",
    "        model_selector,\n",
    "        query_input,\n",
    "        widgets.HBox([confidence_slider, batch_size_slider]),\n",
    "        widgets.HBox([search_button, copy_button, clear_button, switch_model_button]),\n",
    "        widgets.HTML(\"<hr>\")\n",
    "    ])\n",
    "\n",
    "    return widgets.VBox([controls, output_area])\n",
    "\n",
    "def display_results(results):\n",
    "    \"\"\"Display search results with bounding boxes\"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "\n",
    "    cols = 2\n",
    "    rows = (len(results) + cols - 1) // cols\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=CONFIG['FIGURE_SIZE'])\n",
    "    if rows == 1:\n",
    "        axes = [axes] if cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        ax = axes[i] if len(results) > 1 else axes\n",
    "\n",
    "        # Display image\n",
    "        image = result['image']\n",
    "        ax.imshow(image)\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        w, h = image.size\n",
    "        boxes = result['boxes']\n",
    "        confidences = result['confidence_scores']\n",
    "\n",
    "        for box, conf in zip(boxes, confidences):\n",
    "            # Convert from [x1, y1, x2, y2] to matplotlib rectangle\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "\n",
    "            # Create rectangle\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), x2 - x1, y2 - y1,\n",
    "                linewidth=2, edgecolor='red', facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Add confidence text\n",
    "            ax.text(x1, y1 - 5, f'{conf:.2f}',\n",
    "                   color='red', fontweight='bold', fontsize=10)\n",
    "\n",
    "        ax.set_title(f\"{result['image_path'].name}\\nMatches: {len(boxes)}\", fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for j in range(len(results), len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display the interface\n",
    "interface = create_search_interface()\n",
    "display(interface)\n",
    "\n",
    "# Cell 6: Batch Processing Functions (Enhanced)\n",
    "def batch_search_multiple_queries(queries_list, model, processor, device, model_name, gallery_path, output_base_path, batch_size=8):\n",
    "    \"\"\"\n",
    "    Process multiple queries in batch for comprehensive analysis\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "\n",
    "    print(f\"üöÄ Starting batch analysis with {len(queries_list)} queries\")\n",
    "    print(f\"üìÅ Gallery: {gallery_path}\")\n",
    "    print(f\"ü§ñ Model: {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i, query in enumerate(queries_list, 1):\n",
    "        print(f\"\\n[{i}/{len(queries_list)}] Query: '{query}'\")\n",
    "\n",
    "        results = search_images_with_query(query, model, processor, device, model_name, gallery_path, batch_size)\n",
    "\n",
    "        if results:\n",
    "            # Create query-specific output folder\n",
    "            query_folder = Path(output_base_path) / f\"query_{query.replace(' ', '_').replace('/', '_')}\"\n",
    "            folder, files = copy_results_to_folder(results, query_folder)\n",
    "            all_results[query] = {\n",
    "                'results': results,\n",
    "                'output_folder': folder,\n",
    "                'file_count': len(files),\n",
    "                'match_count': len(results)\n",
    "            }\n",
    "            print(f\"üìÅ Saved {len(files)} files to: {folder.name}\")\n",
    "        else:\n",
    "            all_results[query] = {\n",
    "                'results': [],\n",
    "                'output_folder': None,\n",
    "                'file_count': 0,\n",
    "                'match_count': 0\n",
    "            }\n",
    "            print(\"‚ö™ No matches found\")\n",
    "\n",
    "    # Summary report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä BATCH SEARCH SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    total_matches = 0\n",
    "    total_files = 0\n",
    "\n",
    "    for query, data in all_results.items():\n",
    "        matches = data['match_count']\n",
    "        files = data['file_count']\n",
    "        total_matches += matches\n",
    "        total_files += files\n",
    "\n",
    "        status = \"‚úÖ\" if matches > 0 else \"‚ö™\"\n",
    "        print(f\"{status} '{query}': {matches} images, {files} files saved\")\n",
    "\n",
    "    print(f\"\\nüéØ TOTAL: {total_matches} matched images, {total_files} files copied\")\n",
    "    return all_results\n",
    "\n",
    "# Enhanced batch processing with common forensic queries\n",
    "def run_forensic_batch_analysis(custom_queries=None, batch_size=8, model_to_use=None):\n",
    "    \"\"\"Run comprehensive forensic analysis with predefined and custom queries\"\"\"\n",
    "\n",
    "    # Use provided model or current global model\n",
    "    if model_to_use:\n",
    "        current_model, current_processor, current_device, current_model_name = model_to_use\n",
    "    else:\n",
    "        current_model, current_processor, current_device, current_model_name = model, processor, device, model_name\n",
    "\n",
    "    # Default forensic queries\n",
    "    default_queries = [\n",
    "        \"person with weapon\",\n",
    "        \"person holding gun\",\n",
    "        \"person with knife\",\n",
    "        \"suspicious vehicle\",\n",
    "        \"person wearing mask\",\n",
    "        \"person running\",\n",
    "        \"bag or backpack\",\n",
    "        \"group of people\",\n",
    "        \"person with phone\",\n",
    "        \"person in dark clothing\"\n",
    "    ]\n",
    "\n",
    "    # Combine with custom queries if provided\n",
    "    if custom_queries:\n",
    "        queries = default_queries + custom_queries\n",
    "        print(f\"üìã Using {len(default_queries)} default + {len(custom_queries)} custom queries\")\n",
    "    else:\n",
    "        queries = default_queries\n",
    "        print(f\"üìã Using {len(default_queries)} default forensic queries\")\n",
    "\n",
    "    if current_model and current_processor:\n",
    "        print(\"üîç Starting comprehensive forensic analysis...\")\n",
    "        batch_results = batch_search_multiple_queries(\n",
    "            queries,\n",
    "            current_model,\n",
    "            current_processor,\n",
    "            current_device,\n",
    "            current_model_name,\n",
    "            CONFIG['SUSPECTS_GALLERY_PATH'],\n",
    "            CONFIG['RESULTS_OUTPUT_PATH'],\n",
    "            batch_size\n",
    "        )\n",
    "        return batch_results\n",
    "    else:\n",
    "        print(\"‚ùå Model or processor not loaded. Cannot run batch analysis.\")\n",
    "        return None\n",
    "\n",
    "# Quick test with reduced output\n",
    "def quick_forensic_search(query=\"person with weapon\", batch_size=8, model_to_use=None):\n",
    "    \"\"\"Quick single query search for testing\"\"\"\n",
    "    if model_to_use:\n",
    "        current_model, current_processor, current_device, current_model_name = model_to_use\n",
    "    else:\n",
    "        current_model, current_processor, current_device, current_model_name = model, processor, device, model_name\n",
    "\n",
    "    if not current_model or not current_processor:\n",
    "        print(\"‚ùå Model or processor not loaded\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üîç Quick search: '{query}'\")\n",
    "    print(f\"ü§ñ Using model: {current_model_name}\")\n",
    "\n",
    "    results = search_images_with_query(query, current_model, current_processor, current_device, current_model_name,\n",
    "                                     CONFIG['SUSPECTS_GALLERY_PATH'], batch_size)\n",
    "\n",
    "    if results:\n",
    "        print(f\"üìã Found {len(results)} matches - ready for detailed analysis\")\n",
    "        return results\n",
    "    else:\n",
    "        print(\"‚ö™ No matches found\")\n",
    "        return []\n",
    "\n",
    "# Model switching utilities\n",
    "def list_available_models():\n",
    "    \"\"\"Display available models with their descriptions\"\"\"\n",
    "    print(\"ü§ñ Available GroundingDINO Models:\")\n",
    "    print(\"-\" * 50)\n",
    "    for key, info in AVAILABLE_MODELS.items():\n",
    "        print(f\"üîπ {info['name']} ({key})\")\n",
    "        print(f\"   üìä {info['performance']}\")\n",
    "        print(f\"   üìù {info['description']}\")\n",
    "        print()\n",
    "\n",
    "# Display available models\n",
    "list_available_models()\n",
    "\n",
    "# Uncomment to run batch analysis\n",
    "# batch_results = run_forensic_batch_analysis(batch_size=8)\n",
    "\n",
    "# Uncomment for quick test\n",
    "# quick_results = quick_forensic_search(\"person holding gun\", batch_size=8)\n",
    "\n",
    "# Cell 7: Usage Instructions and Tips\n",
    "print(\"\"\"\n",
    "üéØ FORENSIC IMAGE SEARCH SYSTEM - HUGGING FACE VERSION\n",
    "====================================================\n",
    "\n",
    "üÜï NEW FEATURES:\n",
    "‚Ä¢ ü§ñ Automatic model downloads from Hugging Face\n",
    "‚Ä¢ üîÑ Easy model switching between Tiny and Base variants\n",
    "‚Ä¢ üì¶ No manual model file downloads required\n",
    "‚Ä¢ üöÄ Improved performance and reliability\n",
    "\n",
    "üìã SETUP CHECKLIST:\n",
    "1. ‚úÖ Place suspect images in the './suspects_gallery' folder\n",
    "2. ‚úÖ Run all cells in order (1-6)\n",
    "3. ‚úÖ Models download automatically on first use\n",
    "\n",
    "ü§ñ AVAILABLE MODELS:\n",
    "‚Ä¢ GroundingDINO-Tiny: Fastest, good for quick testing\n",
    "‚Ä¢ GroundingDINO-Base: Best balance (recommended)\n",
    "‚Ä¢ Both models run locally after download\n",
    "\n",
    "üîß DEVICE COMPATIBILITY:\n",
    "‚Ä¢ System automatically detects GPU/CPU availability\n",
    "‚Ä¢ Models work on both CPU and GPU\n",
    "‚Ä¢ CPU mode: Slower but works on all systems\n",
    "‚Ä¢ GPU mode: Faster with CUDA support\n",
    "\n",
    "üîç SEARCH TIPS:\n",
    "‚Ä¢ Use specific, descriptive queries: \"person with red jacket\"\n",
    "‚Ä¢ Try multiple variations: \"weapon\", \"gun\", \"knife\", \"suspicious object\"\n",
    "‚Ä¢ Adjust confidence threshold (lower = more results, higher = more precise)\n",
    "‚Ä¢ Common forensic queries:\n",
    "  - \"person with weapon\"\n",
    "  - \"suspicious vehicle\"\n",
    "  - \"person wearing mask\"\n",
    "  - \"bag or backpack\"\n",
    "  - \"person running\"\n",
    "\n",
    "‚öôÔ∏è INTERFACE FEATURES:\n",
    "‚Ä¢ Model selector dropdown for easy switching\n",
    "‚Ä¢ Confidence and batch size sliders\n",
    "‚Ä¢ Real-time search with progress tracking\n",
    "‚Ä¢ Copy results to organized folders\n",
    "‚Ä¢ Clear visual results with bounding boxes\n",
    "\n",
    "üìÅ OUTPUT STRUCTURE:\n",
    "search_results/\n",
    "‚îú‚îÄ‚îÄ search_20240611_143022/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 001_suspect1_conf0.87.jpg\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 002_suspect5_conf0.72.jpg\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "\n",
    "üö® TROUBLESHOOTING:\n",
    "‚Ä¢ \"Model loading failed\" ‚Üí Check internet connection (first download)\n",
    "‚Ä¢ \"No images found\" ‚Üí Verify images are in suspects_gallery folder\n",
    "‚Ä¢ \"Out of memory\" ‚Üí Reduce batch size or use Tiny model\n",
    "‚Ä¢ Slow performance ‚Üí Try GPU if available, or reduce batch size\n",
    "\n",
    "üîÑ MODEL SWITCHING:\n",
    "‚Ä¢ Use the dropdown menu to select different models\n",
    "‚Ä¢ Click \"Switch Model\" to change models\n",
    "‚Ä¢ Tiny model: Faster, lower accuracy\n",
    "‚Ä¢ Base model: Better accuracy, slightly slower\n",
    "\n",
    "For batch processing of multiple queries, use the functions in Cell 6.\n",
    "\"\"\")"
   ],
   "id": "7868d05480023371",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully\n",
      "üìÅ Suspects gallery: ../../datasets/images/objects/raw\n",
      "üìÅ Results output: ../../datasets/images/objects/detections\n",
      "ü§ñ Selected model: IDEA-Research/grounding-dino-base\n",
      "‚úÖ Transformers already installed\n",
      "‚úÖ All dependencies imported successfully\n",
      "üîß PyTorch version: 2.7.1+cpu\n",
      "üñ•Ô∏è Using CPU mode (CUDA not available)\n",
      "üìç Auto-detected device: cpu\n",
      "üìÅ Created directories: ../../datasets/images/objects/raw, ../../datasets/images/objects/detections\n",
      "üì• Loading model: IDEA-Research/grounding-dino-base\n",
      "üñ•Ô∏è Target device: cpu\n",
      "‚è≥ Loading processor...\n",
      "‚è≥ Loading model weights...\n",
      "üìç Moving model to cpu...\n",
      "‚úÖ Model loaded successfully!\n",
      "   üìä Model: IDEA-Research/grounding-dino-base\n",
      "   üñ•Ô∏è Device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h3>üîç Forensic Image Search Interface (Hugging Face)</h3>'), Dropdow‚Ä¶"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d08d199e9ae4c06b9227faeedde4dfd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Available GroundingDINO Models:\n",
      "--------------------------------------------------\n",
      "üîπ GroundingDINO-Tiny (grounding-dino-tiny)\n",
      "   üìä Lower accuracy, fastest speed\n",
      "   üìù Fastest, smallest model - good for quick testing\n",
      "\n",
      "üîπ GroundingDINO-Base (grounding-dino-base)\n",
      "   üìä 52.5 AP on COCO, good speed\n",
      "   üìù Best balance of speed and accuracy - recommended\n",
      "\n",
      "\n",
      "üéØ FORENSIC IMAGE SEARCH SYSTEM - HUGGING FACE VERSION\n",
      "====================================================\n",
      "\n",
      "üÜï NEW FEATURES:\n",
      "‚Ä¢ ü§ñ Automatic model downloads from Hugging Face\n",
      "‚Ä¢ üîÑ Easy model switching between Tiny and Base variants\n",
      "‚Ä¢ üì¶ No manual model file downloads required\n",
      "‚Ä¢ üöÄ Improved performance and reliability\n",
      "\n",
      "üìã SETUP CHECKLIST:\n",
      "1. ‚úÖ Place suspect images in the './suspects_gallery' folder\n",
      "2. ‚úÖ Run all cells in order (1-6)\n",
      "3. ‚úÖ Models download automatically on first use\n",
      "\n",
      "ü§ñ AVAILABLE MODELS:\n",
      "‚Ä¢ GroundingDINO-Tiny: Fastest, good for quick testing\n",
      "‚Ä¢ GroundingDINO-Base: Best balance (recommended)\n",
      "‚Ä¢ Both models run locally after download\n",
      "\n",
      "üîß DEVICE COMPATIBILITY:\n",
      "‚Ä¢ System automatically detects GPU/CPU availability\n",
      "‚Ä¢ Models work on both CPU and GPU\n",
      "‚Ä¢ CPU mode: Slower but works on all systems\n",
      "‚Ä¢ GPU mode: Faster with CUDA support\n",
      "\n",
      "üîç SEARCH TIPS:\n",
      "‚Ä¢ Use specific, descriptive queries: \"person with red jacket\"\n",
      "‚Ä¢ Try multiple variations: \"weapon\", \"gun\", \"knife\", \"suspicious object\"\n",
      "‚Ä¢ Adjust confidence threshold (lower = more results, higher = more precise)\n",
      "‚Ä¢ Common forensic queries:\n",
      "  - \"person with weapon\"\n",
      "  - \"suspicious vehicle\"\n",
      "  - \"person wearing mask\"\n",
      "  - \"bag or backpack\"\n",
      "  - \"person running\"\n",
      "\n",
      "‚öôÔ∏è INTERFACE FEATURES:\n",
      "‚Ä¢ Model selector dropdown for easy switching\n",
      "‚Ä¢ Confidence and batch size sliders\n",
      "‚Ä¢ Real-time search with progress tracking\n",
      "‚Ä¢ Copy results to organized folders\n",
      "‚Ä¢ Clear visual results with bounding boxes\n",
      "\n",
      "üìÅ OUTPUT STRUCTURE:\n",
      "search_results/\n",
      "‚îú‚îÄ‚îÄ search_20240611_143022/\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ 001_suspect1_conf0.87.jpg\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ 002_suspect5_conf0.72.jpg\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
      "\n",
      "üö® TROUBLESHOOTING:\n",
      "‚Ä¢ \"Model loading failed\" ‚Üí Check internet connection (first download)\n",
      "‚Ä¢ \"No images found\" ‚Üí Verify images are in suspects_gallery folder\n",
      "‚Ä¢ \"Out of memory\" ‚Üí Reduce batch size or use Tiny model\n",
      "‚Ä¢ Slow performance ‚Üí Try GPU if available, or reduce batch size\n",
      "\n",
      "üîÑ MODEL SWITCHING:\n",
      "‚Ä¢ Use the dropdown menu to select different models\n",
      "‚Ä¢ Click \"Switch Model\" to change models\n",
      "‚Ä¢ Tiny model: Faster, lower accuracy\n",
      "‚Ä¢ Base model: Better accuracy, slightly slower\n",
      "\n",
      "For batch processing of multiple queries, use the functions in Cell 6.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
